# app/tasks.py

import logging
from datetime import datetime
import pytz
import time 
from flask import current_app
from apscheduler.schedulers.background import BackgroundScheduler
from sqlalchemy import text  # â˜… è¿½åŠ 

from . import db
from .models import Article
from .wp_client import post_to_wp  # çµ±ä¸€ã•ã‚ŒãŸ WordPress æŠ•ç¨¿é–¢æ•°
from sqlalchemy.orm import selectinload

# âœ… GSCã‚¯ãƒªãƒƒã‚¯ãƒ»è¡¨ç¤ºå›æ•°ã®æ¯æ—¥æ›´æ–°ã‚¸ãƒ§ãƒ–ç”¨
from app.google_client import update_all_gsc_sites

# æ—¢å­˜ import ã®ä¸‹ã‚ãŸã‚Šã«è¿½åŠ 
from concurrent.futures import ThreadPoolExecutor
from .models import (Site, Keyword, ExternalSEOJob,
                     BlogType, ExternalBlogAccount, ExternalArticleSchedule)

# app/tasks.py ï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã® BlogType ãªã©ã®ä¸‹ã‚ãŸã‚Šï¼‰
from app.services.blog_signup.livedoor_signup import signup as livedoor_signup
# æ—¢å­˜ import ç¾¤ã®ä¸‹ã«è¿½åŠ 
from app.external_seo_generator import generate_and_schedule_external_articles

# === å†…éƒ¨SEO è‡ªå‹•åŒ– ã§ä½¿ã† import ===
from app.models import InternalSeoRun
from app.utils.db_retry import with_db_retry
from app.services.internal_seo.indexer import sync_site_content_index
from app.services.internal_seo.link_graph import build_link_graph_for_site
from app.services.internal_seo.planner import plan_links_for_site
from app.services.internal_seo.applier import apply_actions_for_site
import os
from math import inf

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# APScheduler ï¼‹ ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãª APScheduler ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆ__init__.py ã§ start ã•ã‚Œã¦ã„ã¾ã™ï¼‰
scheduler = BackgroundScheduler(timezone="UTC")
executor = ThreadPoolExecutor(max_workers=3, thread_name_prefix="extseo")

# --------------------------------------------------------------------------- #
# 1) WordPress è‡ªå‹•æŠ•ç¨¿ã‚¸ãƒ§ãƒ–
# --------------------------------------------------------------------------- #
def _auto_post_job(app):
    with app.app_context():
        start = time.time()
        current_app.logger.info("Running auto_post_job")
        now = datetime.now(pytz.utc)

        try:
            pending = (
                db.session.query(Article)
                .filter(Article.status == "done", Article.scheduled_at <= now,Article.source != "external",)
                .options(selectinload(Article.site))
                .order_by(Article.scheduled_at.asc())
                .limit(20)
                .all()
            )

            for art in pending:
                if not art.site:
                    current_app.logger.warning(f"è¨˜äº‹ {art.id} ã®æŠ•ç¨¿å…ˆã‚µã‚¤ãƒˆæœªè¨­å®š")
                    continue

                try:
                    site = db.session.query(Site).get(art.site_id)
                    current_app.logger.info(f"æŠ•ç¨¿å‡¦ç†é–‹å§‹: Article {art.id}, User ID: {art.user_id}, Site ID: {art.site_id}")
                    url = post_to_wp(site, art)
                    art.posted_at = now
                    art.status = "posted"
                    art.posted_url = url 
                    db.session.commit()
                    current_app.logger.info(f"Auto-posted Article {art.id} -> {url}")

                except Exception as e:
                    db.session.rollback()
                    current_app.logger.error(f"åˆå›æŠ•ç¨¿å¤±æ•—: Article {art.id}, User ID: {art.user_id}, Site ID: {art.site_id}, ã‚¨ãƒ©ãƒ¼: {e}")
                    art.status = "error"  # æŠ•ç¨¿å¤±æ•—æ™‚ã«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ "error" ã«å¤‰æ›´
                    db.session.commit()  # ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã¨ã—ã¦ä¿å­˜

        finally:
            db.session.close()
            end = time.time()
            current_app.logger.info(f"âœ… [AutoPost] è‡ªå‹•æŠ•ç¨¿ã‚¸ãƒ§ãƒ–çµ‚äº†ï¼ˆæ‰€è¦æ™‚é–“: {end - start:.1f}ç§’ï¼‰")

# --------------------------------------------------------------------------- #
# 2) GSC ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯æ—¥æ›´æ–°
# --------------------------------------------------------------------------- #
def _gsc_metrics_job(app):
    """
    âœ… GSCã‚¯ãƒªãƒƒã‚¯ãƒ»è¡¨ç¤ºå›æ•°ã®æ¯æ—¥æ›´æ–°ã‚¸ãƒ§ãƒ–
    """
    with app.app_context():
        current_app.logger.info("ğŸ”„ GSCãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°ã‚¸ãƒ§ãƒ–ã‚’é–‹å§‹ã—ã¾ã™")
        try:
            update_all_gsc_sites()
            current_app.logger.info("âœ… GSCãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°å®Œäº†")
        except Exception as e:
            current_app.logger.error(f"âŒ GSCãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°å¤±æ•—: {str(e)}")

# --------------------------------------------------------------------------- #
# 3) GSC é€£æºã‚µã‚¤ãƒˆå‘ã‘ 1000 è¨˜äº‹ãƒ«ãƒ¼ãƒ—ç”Ÿæˆ
# --------------------------------------------------------------------------- #
def gsc_loop_generate(site):
    """
    ğŸ” GSCã‹ã‚‰ã®ã‚¯ã‚¨ãƒªã§1000è¨˜äº‹æœªæº€ãªã‚‰é€šå¸¸è¨˜äº‹ãƒ•ãƒ­ãƒ¼ã§ç”Ÿæˆã™ã‚‹ï¼ˆä¿®æ­£æ¸ˆï¼‰
    - æ–°è¦ã‚¯ã‚¨ãƒªã‚’ç™»éŒ²
    - æ—¢å­˜ã®æœªç”Ÿæˆï¼ˆpending/errorï¼‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚ã™ã¹ã¦ enqueue
    """
    from app import db
    from app.google_client import fetch_search_queries_for_site
    from app.models import Keyword, PromptTemplate
    from app.article_generator import enqueue_generation
    from flask import current_app

    if not site.gsc_connected:
        current_app.logger.info(f"[GSC LOOP] ã‚¹ã‚­ãƒƒãƒ—ï¼šæœªæ¥ç¶šã‚µã‚¤ãƒˆ {site.name}")
        return

    total_keywords = Keyword.query.filter_by(site_id=site.id).count()
    if total_keywords >= 1000:
        current_app.logger.info(f"[GSC LOOP] {site.name} ã¯æ—¢ã«1000è¨˜äº‹ã«åˆ°é”æ¸ˆã¿")
        return

    # âœ… GSCã‚¯ã‚¨ãƒªã‚’å–å¾—
    try:
        queries = fetch_search_queries_for_site(site, days=28)
    except Exception as e:
        current_app.logger.warning(f"[GSC LOOP] ã‚¯ã‚¨ãƒªå–å¾—å¤±æ•— - {site.url}: {e}")
        return

    # âœ… æ–°è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦è¿½åŠ 
    existing_keywords = set(
        k.keyword for k in Keyword.query.filter_by(site_id=site.id).all()
    )
    new_keywords = [q for q in queries if q not in existing_keywords]

    for kw in new_keywords:
        db.session.add(Keyword(
            keyword=kw,
            site_id=site.id,
            user_id=site.user_id,
            source='gsc',
            status='pending',
            used=False
        ))

    if new_keywords:
        current_app.logger.info(f"[GSC LOOP] {site.name} ã«æ–°è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {len(new_keywords)} ä»¶ç™»éŒ²")

    db.session.commit()

    # âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ï¼ˆãªã‘ã‚Œã°ç©ºï¼‰
    prompt = PromptTemplate.query.filter_by(user_id=site.user_id).order_by(PromptTemplate.id.desc()).first()
    title_prompt = prompt.title_pt if prompt else ""
    body_prompt  = prompt.body_pt  if prompt else ""

    # âœ… ä¿®æ­£ï¼šæœªç”Ÿæˆï¼ˆpending ã¾ãŸã¯ errorï¼‰ã‚’ã™ã¹ã¦ã‚­ãƒ¥ãƒ¼ã«æµã™
    from sqlalchemy import or_

    ungenerated_keywords = (
        Keyword.query
        .filter(
            Keyword.site_id == site.id,
            Keyword.source == "gsc",
            Keyword.status.in_(["pending", "error"])
        )
        .order_by(Keyword.id.asc())
        .all()
    )

    if not ungenerated_keywords:
        current_app.logger.info(f"[GSC LOOP] {site.name} ã«ç”Ÿæˆå¾…ã¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã—")
        return

    BATCH = 40
    for i in range(0, len(ungenerated_keywords), BATCH):
        batch_keywords = ungenerated_keywords[i:i+BATCH]
        keyword_strings = [k.keyword for k in batch_keywords]

        enqueue_generation(
            user_id      = site.user_id,
            site_id      = site.id,
            keywords     = keyword_strings,
            title_prompt = title_prompt,
            body_prompt  = body_prompt,
            format       = "html",
            self_review  = False,
        )

        # âœ… ä¿®æ­£ï¼šã‚­ãƒ¥ãƒ¼æŠ•å…¥æ¸ˆã¿ã¨ã—ã¦ status ã‚’æ›´æ–°
        for k in batch_keywords:
            k.status = "queued"

        db.session.commit()

        current_app.logger.info(
            f"[GSC LOOP] {site.name} â†’ batch {i//BATCH+1}: {len(batch_keywords)} ä»¶ã‚­ãƒ¥ãƒ¼æŠ•å…¥"
        )


def _gsc_generation_job(app):
    """
    âœ… GSCè¨˜äº‹è‡ªå‹•ç”Ÿæˆã‚¸ãƒ§ãƒ–
    """
    with app.app_context():
        from app.models import Site

        current_app.logger.info("ğŸ“ GSCè¨˜äº‹ç”Ÿæˆã‚¸ãƒ§ãƒ–ã‚’é–‹å§‹ã—ã¾ã™")
        sites = Site.query.filter_by(gsc_connected=True).all()

        for site in sites:
            try:
                gsc_loop_generate(site)
            except Exception as e:
                current_app.logger.warning(f"[GSCè‡ªå‹•ç”Ÿæˆ] å¤±æ•— - {site.url}: {e}")

        current_app.logger.info("âœ… GSCè¨˜äº‹ç”Ÿæˆã‚¸ãƒ§ãƒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")

# app/tasks.py ã©ã“ã§ã‚‚ OK ã§ã™ãŒ _run_external_seo_job ã®ç›´å‰ã‚ãŸã‚ŠãŒèª­ã¿ã‚„ã™ã„
def _run_livedoor_signup(app, site_id: int) -> None:
    """
    1) livedoor_signup.signup() ã‚’å‘¼ã³å‡ºã— ExternalBlogAccount ã‚’æ–°è¦ä½œæˆ
       - æˆåŠŸæ™‚ã¯ ExternalSEOJob ã‚’ step='generate' ã§ä½œæˆ / æ›´æ–°
       - å¤±æ•—æ™‚ã¯ status='error' ã®ã‚¸ãƒ§ãƒ–ã‚’æ®‹ã™
    2) API ã‚­ãƒ¼ãªã©è©³ç´°ãƒ­ã‚°ã¯ ExternalBlogAccount ã®ã‚«ãƒ©ãƒ ã«ä¿å­˜æ¸ˆã¿
    """
    with app.app_context():
        try:
            # â‘  Site ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ã—ã¦å­˜åœ¨ç¢ºèª
            site = Site.query.get(site_id)
            if not site:
                raise ValueError(f"Site id={site_id} not found")

            # â‘¡ livedoor ã‚¢ã‚«ã‚¦ãƒ³ãƒˆè‡ªå‹•ç™»éŒ²
            acc = livedoor_signup(site)   # â† Site ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™
            current_app.logger.info(
                "[LD-Signup] success: site=%s account_id=%s", site_id, acc.id
            )

            # â‘¢ ã‚¸ãƒ§ãƒ–è¡Œã‚’ running â†’ generate ã¸
            job = ExternalSEOJob(
                site_id     = site_id,
                blog_type   = BlogType.LIVEDOOR,
                status      = "running",
                step        = "generate",
                article_cnt = 0,
                message     = "signup OK",
            )
            db.session.add(job)
            db.session.commit()

            # TODO: enqueue_generation(job.id) ãªã©ã§è¨˜äº‹ç”Ÿæˆã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥ã™ã‚‹å ´åˆã¯ã“ã“

        except Exception as e:
            # â‘£ å¤±æ•—æ™‚ï¼šã‚¨ãƒ©ãƒ¼ã‚¸ãƒ§ãƒ–ã‚’æ®‹ã™ & ãƒ­ã‚°
            current_app.logger.exception("[LD-Signup] failed: %s", e)

            job = ExternalSEOJob(
                site_id   = site_id,
                blog_type = BlogType.LIVEDOOR,
                status    = "error",
                step      = "error",
                message   = str(e),
            )
            db.session.add(job)
            db.session.commit()

            
def enqueue_livedoor_signup(site_id: int):
    """
    å¤–éƒ¨SEOé–‹å§‹ãƒœã‚¿ãƒ³ â†’ ã“ã®é–¢æ•°ã‚’å‘¼ã¶
    """
    app = current_app._get_current_object()
    executor.submit(_run_livedoor_signup, app, site_id)


def _run_generate_and_schedule(app, user_id: int, site_id: int, blog_account_id: int,
                               count: int = 100, per_day: int = 10, start_day_jst=None):
    """
    å¤–éƒ¨SEO 100æœ¬ç”Ÿæˆï¼‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚’â€œã‚¢ãƒ—ãƒªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã§â€å®Ÿè¡Œã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼æœ¬ä½“
    """
    with app.app_context():
        try:
            created = generate_and_schedule_external_articles(
                user_id=user_id,
                site_id=site_id,
                blog_account_id=blog_account_id,
                count=count,
                per_day=per_day,
                start_day_jst=start_day_jst,
            )
            current_app.logger.info(
                "[external-seo] generate+schedule done: site=%s acct=%s created=%s",
                site_id, blog_account_id, created
            )
        except Exception as e:
            current_app.logger.exception("[external-seo] generate+schedule failed: %s", e)


def enqueue_generate_and_schedule(user_id: int, site_id: int, blog_account_id: int,
                                  count: int = 100, per_day: int = 10, start_day_jst=None):
    """
    ãƒ«ãƒ¼ãƒˆã‹ã‚‰å‘¼ã¶è»½é‡é–¢æ•°ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã§éåŒæœŸå®Ÿè¡Œï¼‰
    """
    app = current_app._get_current_object()
    executor.submit(
        _run_generate_and_schedule, app,
        user_id, site_id, blog_account_id, count, per_day, start_day_jst
    )


# --------------------------------------------------------------------------- #
# 4) å¤–éƒ¨SEO â‘  ã‚­ãƒ¥ãƒ¼ä½œæˆã‚¸ãƒ§ãƒ–
# --------------------------------------------------------------------------- #



# --------------------------------------------------------------------------- #
# 5) å¤–éƒ¨SEO â‘¡ æŠ•ç¨¿ã‚¸ãƒ§ãƒ–
# --------------------------------------------------------------------------- #

def _finalize_external_job(job_id: int):
    """åŒä¸€ã‚µã‚¤ãƒˆã®â€œã“ã®ã‚¸ãƒ§ãƒ–ä»¥é™ã«ä½œã‚‰ã‚ŒãŸâ€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å®Œäº†ã‚’é›†è¨ˆã—ã¦ã€å…¨éƒ¨ posted ãªã‚‰ success ã«ã™ã‚‹"""
    job = ExternalSEOJob.query.get(job_id)
    if not job:
        return

    # ã“ã®ã‚¸ãƒ§ãƒ–ä½œæˆä»¥é™ã«ç”Ÿæˆã•ã‚ŒãŸã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã ã‘ã‚’å¯¾è±¡ã«ã™ã‚‹ï¼ˆåŒä¸€ã‚µã‚¤ãƒˆã®åˆ¥ãƒãƒƒãƒã¨æ··ã–ã‚‰ãªã„ãŸã‚ï¼‰
    q_base = (ExternalArticleSchedule.query
              .join(ExternalBlogAccount,
                    ExternalArticleSchedule.blog_account_id == ExternalBlogAccount.id)
              .filter(ExternalBlogAccount.site_id == job.site_id,
                      ExternalArticleSchedule.created_at >= job.created_at))

    total = q_base.count()
    posted = q_base.filter(ExternalArticleSchedule.status == "posted").count()

    # total ãŒ 0 ã®é–“ã¯åˆ¤å®šã—ãªã„
    if total and posted >= total:
        job.step = "done"
        job.status = "success"
        db.session.commit()
        

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¤–éƒ¨ãƒ–ãƒ­ã‚°æŠ•ç¨¿ã‚¸ãƒ§ãƒ–ï¼ˆå¤–éƒ¨SEOãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å³å¯†ç´ä»˜ã‘ç‰ˆï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _run_external_post_job(app, schedule_id: int | None = None):
    """
    å¤–éƒ¨SEOè¨˜äº‹ã‚’å¤–éƒ¨ãƒ–ãƒ­ã‚°ã«æŠ•ç¨¿ã™ã‚‹ã‚¸ãƒ§ãƒ–
    - ExternalArticleSchedule ã® pending ã‚’å¯¾è±¡
    - ExternalBlogAccount ãŒ LIVEDOOR ã®ã¿å¯¾å¿œ
    - âœ… Article ã¯ sched.article_id ã§ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå–å¾—ï¼ˆkeyword ç…§åˆã¯å»ƒæ­¢ï¼‰
    - äºŒé‡æŠ•ç¨¿é˜²æ­¢ï¼šArticle.posted_url ãŒç©ºã®ã‚‚ã®ã®ã¿
    """
    from datetime import datetime
    from flask import current_app
    from sqlalchemy import or_
    from app import db
    from app.models import (
        ExternalArticleSchedule,
        ExternalBlogAccount,
        ExternalSEOJob,
        BlogType,
        Article,
    )
    from app.services.blog_post.livedoor_post import post_livedoor_article

    with app.app_context():
        now = datetime.utcnow()
        current_app.logger.info(f"[ExtPost] Job start at {now}")

        # â† ã“ã“ã‚’ rows_q ã«ã—ã¦ã€ãã®å¾Œã‚‚ rows_q ã‚’ä½¿ã†
        rows_q = (
            db.session.query(ExternalArticleSchedule, ExternalBlogAccount, Article)
            .join(ExternalBlogAccount, ExternalArticleSchedule.blog_account_id == ExternalBlogAccount.id)
            .join(Article, ExternalArticleSchedule.article_id == Article.id)
            .filter(
                ExternalArticleSchedule.status == "pending",
                ExternalArticleSchedule.scheduled_date <= now,
                Article.source == "external",
                or_(Article.posted_url == None, Article.posted_url == ""),  # noqa: E711
                Article.status == "done",
            )
        )

        if schedule_id is not None:
            rows_q = rows_q.filter(ExternalArticleSchedule.id == schedule_id)

        rows = (
            rows_q
            .order_by(ExternalArticleSchedule.scheduled_date.asc())
            .limit(1 if schedule_id is not None else 10)
            .all()
        )

        current_app.logger.info(f"[ExtPost] Found {len(rows)} pending schedules")
        if not rows:
            return

        for sched, acct, art in rows:
            try:
                # ãƒ–ãƒ­ã‚°ç¨®åˆ¥ãƒã‚§ãƒƒã‚¯
                if acct.blog_type != BlogType.LIVEDOOR:
                    sched.status = "error"
                    sched.message = f"æœªå¯¾å¿œãƒ–ãƒ­ã‚°ã‚¿ã‚¤ãƒ—: {acct.blog_type}"
                    db.session.commit()
                    current_app.logger.warning(
                        f"[ExtPost] Unsupported blog type {acct.blog_type} (sched_id={sched.id})"
                    )
                    continue

                # è¿½åŠ ã®å®‰å…¨ãƒã‚§ãƒƒã‚¯ï¼šè¨˜äº‹ã®ã‚µã‚¤ãƒˆã¨ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ã‚µã‚¤ãƒˆãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹
                if art.site_id != acct.site_id:
                    sched.status = "error"
                    sched.message = f"site mismatch: article.site_id={art.site_id} / account.site_id={acct.site_id}"
                    db.session.commit()
                    current_app.logger.error(
                        f"[ExtPost] Site mismatch (sched_id={sched.id}, art_id={art.id})"
                    )
                    continue

                current_app.logger.info(
                    f"[ExtPost] Posting article_id={art.id} (kw='{art.keyword}') to Livedoor..."
                )
                res = post_livedoor_article(acct, art.title, art.body)

                if res.get("ok"):
                    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ›´æ–°
                    sched.status = "posted"
                    sched.posted_url = res.get("url")
                    sched.posted_at = res.get("posted_at")

                    # è¨˜äº‹å´ã‚‚ posted
                    art.status = "posted"
                    if res.get("url"):
                        art.posted_url = res["url"]
                    art.posted_at = res.get("posted_at") or now

                    # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æŠ•ç¨¿ã‚«ã‚¦ãƒ³ãƒˆ
                    acct.posted_cnt = (acct.posted_cnt or 0) + 1

                    db.session.commit()
                    current_app.logger.info(f"[ExtPost] Success: {res.get('url')}")

                    # ã‚¸ãƒ§ãƒ–å®Œäº†åˆ¤å®šï¼ˆæœ€æ–°ã‚¸ãƒ§ãƒ–ã®ã¿ï¼‰
                    latest_job = (
                        ExternalSEOJob.query.filter_by(site_id=acct.site_id)
                        .order_by(ExternalSEOJob.id.desc())
                        .first()
                    )
                    if latest_job:
                        _finalize_external_job(latest_job.id)

                else:
                    # æŠ•ç¨¿å¤±æ•—
                    err = res.get("error") or "unknown error"
                    sched.status = "error"
                    sched.message = err
                    db.session.commit()
                    current_app.logger.error(f"[ExtPost] Failed: {err}")

            except Exception as e:
                current_app.logger.exception(f"[ExtPost] Exception during posting: {e}")
                sched.status = "error"
                sched.message = str(e)
                db.session.commit()




# --------------------------------------------------------------------------- #
# 6) å¤–éƒ¨SEO ãƒãƒƒãƒç›£è¦–ã‚¸ãƒ§ãƒ–ï¼ˆ100 æœ¬å®Œäº†ã”ã¨ã«æ¬¡ãƒãƒƒãƒï¼‰
# --------------------------------------------------------------------------- #


def init_scheduler(app):
    """
    Flask ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«å‘¼ã³å‡ºã—ã¦:
      1) APScheduler ã«è‡ªå‹•æŠ•ç¨¿ã‚¸ãƒ§ãƒ–ã‚’ç™»éŒ²
      2) 3åˆ†é–“éš”ã§ _auto_post_job ã‚’å®Ÿè¡Œã™ã‚‹ã‚ˆã†ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
      - GSCè¨˜äº‹ç”Ÿæˆã‚¸ãƒ§ãƒ–ï¼š10åˆ†é–“éš”ï¼ˆâ†ã“ã“ä¿®æ­£ï¼‰
    """
    scheduler.add_job(
        func=_auto_post_job,
        trigger="interval",
        minutes=3,
        args=[app],
        id="auto_post_job",
        replace_existing=True,
        max_instances=5
    )

    # âœ… GSCã‚¯ãƒªãƒƒã‚¯ãƒ»è¡¨ç¤ºå›æ•°ã‚’æ¯æ—¥0æ™‚ã«è‡ªå‹•æ›´æ–°ã™ã‚‹ã‚¸ãƒ§ãƒ–
    scheduler.add_job(
        func=_gsc_metrics_job,
        trigger="cron",
        hour=0,
        minute=0,
        args=[app],
        id="gsc_metrics_job",
        replace_existing=True,
        max_instances=1
    )

    # âœ… GSCè¨˜äº‹ç”Ÿæˆã‚¸ãƒ§ãƒ–
    scheduler.add_job(
        func=_gsc_generation_job,
        trigger="interval",
        minutes=20,
        args=[app],
        id="gsc_generation_job",
        replace_existing=True,
        max_instances=1
    )

    # âœ… å¤–éƒ¨ãƒ–ãƒ­ã‚°æŠ•ç¨¿ã‚¸ãƒ§ãƒ–ï¼ˆ10åˆ†ãŠãï¼‰
    scheduler.add_job(
        func=_run_external_post_job,
        trigger="interval",
        minutes=10,
        args=[app],
        id="external_post_job",
        replace_existing=True,
        max_instances=1
    )

    # âœ… å†…éƒ¨SEO ãƒŠã‚¤ãƒˆãƒªãƒ¼å®Ÿè¡Œï¼ˆç’°å¢ƒå¤‰æ•°ã§ON/OFFå¯èƒ½ï¼‰
    #   - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æ¯æ—¥ 18:15 UTC = JST 03:15
    if os.getenv("INTERNAL_SEO_ENABLED", "1") == "1":
        utc_hour = int(os.getenv("INTERNAL_SEO_UTC_HOUR", "18"))
        utc_min  = int(os.getenv("INTERNAL_SEO_UTC_MIN",  "15"))
        scheduler.add_job(
            func=_internal_seo_nightly_job,
            trigger="cron",
            hour=utc_hour,
            minute=utc_min,
            args=[app],
            id="internal_seo_job",
            replace_existing=True,
            max_instances=1,
        )
        app.logger.info(f"Scheduler started: internal_seo_job daily at {utc_hour:02d}:{utc_min:02d} UTC")
    else:
        app.logger.info("Scheduler skipped: internal_seo_job (INTERNAL_SEO_ENABLED!=1)")

    # âœ… å†…éƒ¨SEO ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆã‚­ãƒ¥ãƒ¼æ¶ˆåŒ–ï¼‰â€»æ¯åˆ†
    if os.getenv("INTERNAL_SEO_WORKER_ENABLED", "1") == "1":
        scheduler.add_job(
            func=_internal_seo_worker_tick,
            trigger="interval",
            minutes=int(os.getenv("INTERNAL_SEO_WORKER_INTERVAL_MIN", "1")),
            args=[app],
            id="internal_seo_worker_tick",
            replace_existing=True,
            max_instances=1,
        )
        app.logger.info("Scheduler started: internal_seo_worker_tick every minute")
    else:
        app.logger.info("Scheduler skipped: internal_seo_worker_tick (INTERNAL_SEO_WORKER_ENABLED!=1)")    
 


    scheduler.start()
    app.logger.info("Scheduler started: auto_post_job every 3 minutes")
    app.logger.info("Scheduler started: gsc_metrics_job daily at 0:00")
    app.logger.info("Scheduler started: gsc_generation_job every 20 minutes")
    app.logger.info("Scheduler started: external_post_job every 10 minutes")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å†…éƒ¨SEO è‡ªå‹•åŒ–ã‚¸ãƒ§ãƒ–
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@with_db_retry(max_retries=3, backoff=1.8)
def _internal_seo_run_one(site_id: int,
                          pages: int,
                          per_page: int,
                          min_score: float,
                          max_k: int,
                          limit_sources: int,
                          limit_posts: int,
                          incremental: bool,
                          job_kind: str = "scheduler"):
    """
    1ã‚µã‚¤ãƒˆåˆ†ã®å†…éƒ¨SEOãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã€InternalSeoRun ã«è¨˜éŒ²ã™ã‚‹ã€‚
    CLIã®å®Ÿè£…ã¨åŒã˜ã‚¹ãƒ†ãƒƒãƒ—/åŒã˜çµ±è¨ˆã‚­ãƒ¼ã§ä¿å­˜ã™ã‚‹ã€‚
    """
    # ãƒ©ãƒ³ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆï¼ˆrunningï¼‰
    run = InternalSeoRun(
        site_id=site_id,
        job_kind=job_kind,
        status="running",
        started_at=datetime.utcnow(),
        stats={},
    )
    db.session.add(run)
    db.session.commit()

    t0 = time.perf_counter()
    try:
        # Indexer
        current_app.logger.info(f"[Indexer] site={site_id} incremental={incremental} pages={pages} per_page={per_page}")
        stats_idx = sync_site_content_index(site_id, per_page=per_page, max_pages=pages, incremental=incremental)
        current_app.logger.info(f"[Indexer] -> {stats_idx}")

        # LinkGraph
        current_app.logger.info(f"[LinkGraph] site={site_id} max_k={max_k} min_score={min_score}")
        stats_graph = build_link_graph_for_site(site_id, max_targets_per_source=max_k, min_score=min_score)
        current_app.logger.info(f"[LinkGraph] -> {stats_graph}")

        # Planner
        current_app.logger.info(f"[Planner] site={site_id} limit_sources={limit_sources} max_candidates={max_k} min_score={min_score}")
        stats_plan = plan_links_for_site(site_id, limit_sources=limit_sources, mode_swap_check=True,
                                         min_score=min_score, max_candidates=max_k)
        current_app.logger.info(f"[Planner] -> {stats_plan}")

        # Applier
        current_app.logger.info(f"[Applier] site={site_id} limit_posts={limit_posts}")
        res_apply = apply_actions_for_site(site_id, limit_posts=limit_posts, dry_run=False)
        current_app.logger.info(f"[Applier] -> {res_apply}")

        # æˆåŠŸã§ç¢ºå®š
        run.status = "success"
        run.ended_at = datetime.utcnow()
        run.duration_ms = int((time.perf_counter() - t0) * 1000)
        run.stats = {
            "indexer": stats_idx,
            "link_graph": stats_graph,
            "planner": stats_plan,
            "applier": res_apply,
            "params": {
                "incremental": incremental,
                "pages": pages,
                "per_page": per_page,
                "min_score": min_score,
                "max_k": max_k,
                "limit_sources": limit_sources,
                "limit_posts": limit_posts,
                "job_kind": job_kind,
            },
        }
        db.session.commit()
    except Exception as e:
        db.session.rollback()
        run.status = "error"
        run.ended_at = datetime.utcnow()
        run.duration_ms = int((time.perf_counter() - t0) * 1000)
        run.stats = (run.stats or {})
        run.stats["error"] = {"type": e.__class__.__name__, "message": str(e)}
        db.session.add(run)
        db.session.commit()
        current_app.logger.exception(f"[internal-seo] failed for site {site_id}: {e}")

@with_db_retry(max_retries=3, backoff=1.8)
def _internal_seo_worker_tick(app):
    """
    internal_seo_job_queue ã‹ã‚‰ 'queued' ã‚’å®‰å…¨ã«å–ã‚Šå‡ºã—ã€
    åŒæ™‚å®Ÿè¡Œä¸Šé™ã‚’å®ˆã‚Šã¤ã¤ _internal_seo_run_one ã‚’å›ã™ã€‚
    """
    with app.app_context():
        # åŒæ™‚å®Ÿè¡Œä¸Šé™ï¼ˆENV ã§èª¿æ•´ï¼‰
        max_parallel = int(os.getenv("INTERNAL_SEO_WORKER_PARALLELISM", "3"))
        # ã„ã¾èµ°ã£ã¦ã„ã‚‹ run æ•°
        running_cnt = db.session.execute(text("""
            SELECT COUNT(*) FROM internal_seo_runs WHERE status='running'
        """)).scalar_one()
        available = max(0, max_parallel - int(running_cnt or 0))
        if available <= 0:
            current_app.logger.info(f"[internal-seo worker] saturated: running={running_cnt} / max={max_parallel}")
            return

        # å–ã‚Šå‡ºã—ä»¶æ•°ã¯ä¸Šé™ã«æ§ãˆã‚ï¼ˆå¿µã®ãŸã‚ 1 ã‚µã‚¤ãƒˆ=1 run æƒ³å®šï¼‰
        take = min(available, int(os.getenv("INTERNAL_SEO_WORKER_TAKE", "2")))

        # queued ã‚’ãƒ­ãƒƒã‚¯ã—ã¦ running ã«æ›´æ–°ï¼ˆSKIP LOCKED ã§å¤šé‡å–å¾—å›é¿ï¼‰
        rows = db.session.execute(text(f"""
            WITH picked AS (
              SELECT id
              FROM internal_seo_job_queue
              WHERE status='queued'
              ORDER BY created_at ASC, id ASC
              FOR UPDATE SKIP LOCKED
              LIMIT :take
            )
            UPDATE internal_seo_job_queue q
               SET status='running', started_at=now()
              FROM picked p
             WHERE q.id = p.id
          RETURNING q.id, q.site_id, q.pages, q.per_page, q.min_score, q.max_k,
                    q.limit_sources, q.limit_posts, q.incremental, q.job_kind;
        """), {"take": take}).mappings().all()
        db.session.commit()

        if not rows:
            current_app.logger.info("[internal-seo worker] no queued jobs")
            return

        current_app.logger.info(f"[internal-seo worker] picked {len(rows)} job(s)")

        # 1ä»¶ãšã¤å®Ÿè¡Œï¼ˆåŒãƒ—ãƒ­ã‚»ã‚¹å†…ã§é€æ¬¡ã€‚å¿…è¦ãªã‚‰ ThreadPoolExecutor ã§ä¸¦åˆ—åŒ–ã‚‚å¯ï¼‰
        for r in rows:
            j_id = r["id"]
            try:
                _internal_seo_run_one(
                    site_id       = int(r["site_id"]),
                    pages         = int(r["pages"] or os.getenv("INTERNAL_SEO_PAGES", 10)),
                    per_page      = int(r["per_page"] or os.getenv("INTERNAL_SEO_PER_PAGE", 100)),
                    min_score     = float(r["min_score"] or os.getenv("INTERNAL_SEO_MIN_SCORE", 0.05)),
                    max_k         = int(r["max_k"] or os.getenv("INTERNAL_SEO_MAX_K", 80)),
                    limit_sources = int(r["limit_sources"] or os.getenv("INTERNAL_SEO_LIMIT_SOURCES", 200)),
                    limit_posts   = int(r["limit_posts"] or os.getenv("INTERNAL_SEO_LIMIT_POSTS", 50)),
                    incremental   = bool(r["incremental"]),
                    job_kind      = r["job_kind"] or "worker",
                )
                db.session.execute(text("""
                    UPDATE internal_seo_job_queue
                       SET status='done', ended_at=now()
                     WHERE id=:id
                """), {"id": j_id})
                db.session.commit()
            except Exception as e:
                current_app.logger.exception(f"[internal-seo worker] job {j_id} failed: {e}")
                db.session.execute(text("""
                    UPDATE internal_seo_job_queue
                       SET status='error', ended_at=now(), message=:msg
                     WHERE id=:id
                """), {"id": j_id, "msg": str(e)})
                db.session.commit()

def _internal_seo_nightly_job(app):
    """
    ã™ã¹ã¦ã®ã‚µã‚¤ãƒˆï¼ˆã¾ãŸã¯ ENV æŒ‡å®šã®ã‚µã‚¤ãƒˆç¾¤ï¼‰ã«ã¤ã„ã¦ã€
    â€œå®Ÿè¡Œæœ¬ä½“â€ ã¯ãƒ¯ãƒ¼ã‚«ãƒ¼ã«ä»»ã›ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã‚­ãƒ¥ãƒ¼ã«ç©ã‚€ã ã‘ã€‚
    """
    with app.app_context():
        pages          = int(os.getenv("INTERNAL_SEO_PAGES", "10"))
        per_page       = int(os.getenv("INTERNAL_SEO_PER_PAGE", "100"))
        min_score      = float(os.getenv("INTERNAL_SEO_MIN_SCORE", "0.05"))
        max_k          = int(os.getenv("INTERNAL_SEO_MAX_K", "80"))
        limit_sources  = int(os.getenv("INTERNAL_SEO_LIMIT_SOURCES", "200"))
        limit_posts    = int(os.getenv("INTERNAL_SEO_LIMIT_POSTS", "50"))
        incremental    = os.getenv("INTERNAL_SEO_INCREMENTAL", "1") == "1"
        job_kind       = os.getenv("INTERNAL_SEO_JOB_KIND", "nightly-enqueue")

        only_ids = os.getenv("INTERNAL_SEO_SITE_IDS")
        if only_ids:
            ids = [int(x) for x in only_ids.split(",") if x.strip().isdigit()]
            sites = Site.query.filter(Site.id.in_(ids)).all()
        else:
            sites = Site.query.order_by(Site.id.asc()).all()

        enq = text("""
            INSERT INTO internal_seo_job_queue
              (site_id, pages, per_page, min_score, max_k, limit_sources, limit_posts,
               incremental, job_kind, status, created_at)
            VALUES
              (:site_id, :pages, :per_page, :min_score, :max_k, :limit_sources, :limit_posts,
               :incremental, :job_kind, 'queued', now())
        """)

        enqueued = 0
        for s in sites:
            try:
                db.session.execute(enq, dict(
                    site_id=s.id, pages=pages, per_page=per_page, min_score=min_score,
                    max_k=max_k, limit_sources=limit_sources, limit_posts=limit_posts,
                    incremental=incremental, job_kind=job_kind,
                ))
                enqueued += 1
            except Exception as e:
                current_app.logger.exception(f"[internal-seo nightly] enqueue failed site={s.id}: {e}")

        db.session.commit()
        current_app.logger.info(
            f"[internal-seo nightly] enqueued {enqueued}/{len(sites)} jobs "
            f"params={{pages:{pages}, per_page:{per_page}, min_score:{min_score}, "
            f"max_k:{max_k}, limit_sources:{limit_sources}, limit_posts:{limit_posts}, "
            f"incremental:{incremental}, job_kind:{job_kind}}}"
        )
