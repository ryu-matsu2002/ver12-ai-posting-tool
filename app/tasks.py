# app/tasks.py

import logging
from datetime import datetime
import pytz
import time 
from flask import current_app
from apscheduler.schedulers.background import BackgroundScheduler
from sqlalchemy import text  # â˜… è¿½åŠ 

from . import db
from .models import Article
from .wp_client import post_to_wp  # çµ±ä¸€ã•ã‚ŒãŸ WordPress æŠ•ç¨¿é–¢æ•°
from sqlalchemy.orm import selectinload

# âœ… GSCã‚¯ãƒªãƒƒã‚¯ãƒ»è¡¨ç¤ºå›æ•°ã®æ¯æ—¥æ›´æ–°ã‚¸ãƒ§ãƒ–ç”¨
from app.google_client import update_all_gsc_sites, fetch_new_queries_since

# æ—¢å­˜ import ã®ä¸‹ã‚ãŸã‚Šã«è¿½åŠ 
from concurrent.futures import ThreadPoolExecutor
from .models import (Site, Keyword, ExternalSEOJob,
                     BlogType, ExternalBlogAccount, ExternalArticleSchedule)
from app.models import GSCAutogenDaily  # â˜… è¿½åŠ ï¼šæ—¥æ¬¡ã‚µãƒãƒª

# app/tasks.py ï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã® BlogType ãªã©ã®ä¸‹ã‚ãŸã‚Šï¼‰
from app.services.blog_signup.livedoor_signup import signup as livedoor_signup
# æ—¢å­˜ import ç¾¤ã®ä¸‹ã«è¿½åŠ 
from app.external_seo_generator import generate_and_schedule_external_articles

from app.external_seo_generator import (
    TITLE_PROMPT as EXT_TITLE_PROMPT,
    BODY_PROMPT  as EXT_BODY_PROMPT,
)
from app.models import PromptTemplate
from app.article_generator import _generate


# === å†…éƒ¨SEO è‡ªå‹•åŒ– ã§ä½¿ã† import ===
from app.models import InternalSeoRun
from app.utils.db_retry import with_db_retry
from app.services.internal_seo.indexer import sync_site_content_index
from app.services.internal_seo.link_graph import build_link_graph_for_site
from app.services.internal_seo.planner import plan_links_for_site
from app.services.internal_seo.applier import apply_actions_for_site, apply_actions_for_user
from app.services.internal_seo import user_scheduler  # ğŸ†• è¿½åŠ 
import os
from math import inf
from typing import List, Dict, Set, Tuple, Optional
import json
from app.models import InternalLinkAction  # ğŸ†• refill é›†è¨ˆã§ä½¿ç”¨
from app.models import InternalSeoUserSchedule  # ğŸ†• ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç¢ºèªç”¨
from sqlalchemy import func  # ğŸ†• é›†è¨ˆã§ä½¿ç”¨
from app.services.internal_seo.enqueue import enqueue_refill_for_site  # ğŸ†• refillæŠ•å…¥API

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# APScheduler ï¼‹ ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãª APScheduler ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆ__init__.py ã§ start ã•ã‚Œã¦ã„ã¾ã™ï¼‰
scheduler = BackgroundScheduler(timezone="UTC")
executor = ThreadPoolExecutor(max_workers=1, thread_name_prefix="extseo")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GSCã‚ªãƒ¼ãƒˆã‚¸ã‚§ãƒ³ï¼šã‚µã‚¤ãƒˆã”ã¨ã®è»½é‡ãƒ­ãƒƒã‚¯ï¼ˆPostgreSQL advisory lockï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _lock_key(site_id: int) -> int:
    # é©å½“ãªåå‰ç©ºé–“ã‚­ãƒ¼ï¼ˆè¡çªå›é¿ç”¨ã«å›ºå®šä¿‚æ•°ï¼‰
    return 91337_00000 + int(site_id)

def _try_lock_site(site_id: int) -> bool:
    k = _lock_key(site_id)
    try:
        got = db.session.execute(text("SELECT pg_try_advisory_lock(:k)"), {"k": k}).scalar()
        return bool(got)
    except Exception:
        # DBãŒPostgreSQLä»¥å¤–ã§ã‚‚è½ã¡ãªã„ã‚ˆã†ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ­ãƒƒã‚¯ç„¡ã—ã§é€²è¡Œï¼‰
        current_app.logger.warning("[GSC-AUTOGEN] advisory lock unsupported; continue without lock (site=%s)", site_id)
        return True

def _unlock_site(site_id: int) -> None:
    k = _lock_key(site_id)
    try:
        db.session.execute(text("SELECT pg_advisory_unlock(:k)"), {"k": k})
    except Exception:
        pass

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å†…éƒ¨SEOï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®è»½é‡ãƒ­ãƒƒã‚¯ï¼ˆPostgreSQL advisory lockï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _user_lock_key(user_id: int) -> int:
    # åˆ¥åå‰ç©ºé–“ï¼ˆsiteç”¨ã¨è¡çªã—ãªã„ä¿‚æ•°ï¼‰
    return 91338_00000 + int(user_id)

def _try_lock_user(user_id: int) -> bool:
    k = _user_lock_key(user_id)
    try:
        got = db.session.execute(text("SELECT pg_try_advisory_lock(:k)"), {"k": k}).scalar()
        return bool(got)
    except Exception:
        # éPostgreSQLã§ã‚‚è½ã¡ãªã„ã‚ˆã†ã€ãƒ­ãƒƒã‚¯ç„¡ã—ã§å‰é€²
        current_app.logger.warning("[ISEO-USER] advisory lock unsupported; continue without lock (user=%s)", user_id)
        return True

def _unlock_user(user_id: int) -> None:
    k = _user_lock_key(user_id)
    try:
        db.session.execute(text("SELECT pg_advisory_unlock(:k)"), {"k": k})
    except Exception:
        pass    

# --------------------------------------------------------------------------- #
# 1) WordPress è‡ªå‹•æŠ•ç¨¿ã‚¸ãƒ§ãƒ–
# --------------------------------------------------------------------------- #
def _auto_post_job(app):
    with app.app_context():
        start = time.time()
        current_app.logger.info("Running auto_post_job")
        now = datetime.now(pytz.utc)

        try:
            pending = (
                db.session.query(Article)
                .filter(Article.status == "done", Article.scheduled_at <= now,Article.source != "external",)
                .options(selectinload(Article.site))
                .order_by(Article.scheduled_at.asc())
                .limit(20)
                .all()
            )

            for art in pending:
                if not art.site:
                    current_app.logger.warning(f"è¨˜äº‹ {art.id} ã®æŠ•ç¨¿å…ˆã‚µã‚¤ãƒˆæœªè¨­å®š")
                    continue

                try:
                    site = db.session.query(Site).get(art.site_id)
                    current_app.logger.info(f"æŠ•ç¨¿å‡¦ç†é–‹å§‹: Article {art.id}, User ID: {art.user_id}, Site ID: {art.site_id}")
                    url = post_to_wp(site, art)
                    art.posted_at = now
                    art.status = "posted"
                    art.posted_url = url 
                    db.session.commit()
                    current_app.logger.info(f"Auto-posted Article {art.id} -> {url}")

                except Exception as e:
                    db.session.rollback()
                    current_app.logger.error(f"åˆå›æŠ•ç¨¿å¤±æ•—: Article {art.id}, User ID: {art.user_id}, Site ID: {art.site_id}, ã‚¨ãƒ©ãƒ¼: {e}")
                    art.status = "error"  # æŠ•ç¨¿å¤±æ•—æ™‚ã«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ "error" ã«å¤‰æ›´
                    db.session.commit()  # ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã¨ã—ã¦ä¿å­˜

        finally:
            db.session.close()
            end = time.time()
            current_app.logger.info(f"âœ… [AutoPost] è‡ªå‹•æŠ•ç¨¿ã‚¸ãƒ§ãƒ–çµ‚äº†ï¼ˆæ‰€è¦æ™‚é–“: {end - start:.1f}ç§’ï¼‰")

# --------------------------------------------------------------------------- #
# 2) GSC ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯æ—¥æ›´æ–°
# --------------------------------------------------------------------------- #
def _gsc_metrics_job(app):
    """
    âœ… GSCã‚¯ãƒªãƒƒã‚¯ãƒ»è¡¨ç¤ºå›æ•°ã®æ¯æ—¥æ›´æ–°ã‚¸ãƒ§ãƒ–
    """
    with app.app_context():
        current_app.logger.info("ğŸ”„ GSCãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°ã‚¸ãƒ§ãƒ–ã‚’é–‹å§‹ã—ã¾ã™")
        try:
            update_all_gsc_sites()
            current_app.logger.info("âœ… GSCãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°å®Œäº†")
        except Exception as e:
            current_app.logger.error(f"âŒ GSCãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°å¤±æ•—: {str(e)}")

# --------------------------------------------------------------------------- #
# 3) GSC é€£æºã‚µã‚¤ãƒˆå‘ã‘ 1000 è¨˜äº‹ãƒ«ãƒ¼ãƒ—ç”Ÿæˆ
# --------------------------------------------------------------------------- #
def gsc_loop_generate(site):
    """
    ğŸ” GSCã‹ã‚‰ã®ã‚¯ã‚¨ãƒªã§1000è¨˜äº‹æœªæº€ãªã‚‰é€šå¸¸è¨˜äº‹ãƒ•ãƒ­ãƒ¼ã§ç”Ÿæˆã™ã‚‹ï¼ˆä¿®æ­£æ¸ˆï¼‰
    - æ–°è¦ã‚¯ã‚¨ãƒªã‚’ç™»éŒ²
    - æ—¢å­˜ã®æœªç”Ÿæˆï¼ˆpending/errorï¼‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚ã™ã¹ã¦ enqueue
    """
    from app import db
    from app.google_client import fetch_search_queries_for_site
    from app.models import Keyword, PromptTemplate
    from app.article_generator import enqueue_generation
    from flask import current_app

    if not site.gsc_connected:
        current_app.logger.info(f"[GSC LOOP] ã‚¹ã‚­ãƒƒãƒ—ï¼šæœªæ¥ç¶šã‚µã‚¤ãƒˆ {site.name}")
        return

    total_keywords = Keyword.query.filter_by(site_id=site.id).count()
    if total_keywords >= 1000:
        current_app.logger.info(f"[GSC LOOP] {site.name} ã¯æ—¢ã«1000è¨˜äº‹ã«åˆ°é”æ¸ˆã¿")
        return

    # âœ… GSCã‚¯ã‚¨ãƒªã‚’å–å¾—
    try:
        queries = fetch_search_queries_for_site(site, days=28)
    except Exception as e:
        current_app.logger.warning(f"[GSC LOOP] ã‚¯ã‚¨ãƒªå–å¾—å¤±æ•— - {site.url}: {e}")
        return

    # âœ… æ–°è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦è¿½åŠ 
    existing_keywords = set(
        k.keyword for k in Keyword.query.filter_by(site_id=site.id).all()
    )
    new_keywords = [q for q in queries if q not in existing_keywords]

    for kw in new_keywords:
        db.session.add(Keyword(
            keyword=kw,
            site_id=site.id,
            user_id=site.user_id,
            source='gsc',
            status='pending',
            used=False
        ))

    if new_keywords:
        current_app.logger.info(f"[GSC LOOP] {site.name} ã«æ–°è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {len(new_keywords)} ä»¶ç™»éŒ²")

    db.session.commit()

    # âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ï¼ˆãªã‘ã‚Œã°ç©ºï¼‰
    prompt = PromptTemplate.query.filter_by(user_id=site.user_id).order_by(PromptTemplate.id.desc()).first()
    title_prompt = prompt.title_pt if prompt else ""
    body_prompt  = prompt.body_pt  if prompt else ""

    # âœ… ä¿®æ­£ï¼šæœªç”Ÿæˆï¼ˆpending ã¾ãŸã¯ errorï¼‰ã‚’ã™ã¹ã¦ã‚­ãƒ¥ãƒ¼ã«æµã™
    from sqlalchemy import or_

    ungenerated_keywords = (
        Keyword.query
        .filter(
            Keyword.site_id == site.id,
            Keyword.source == "gsc",
            Keyword.status.in_(["pending", "error"])
        )
        .order_by(Keyword.id.asc())
        .all()
    )

    if not ungenerated_keywords:
        current_app.logger.info(f"[GSC LOOP] {site.name} ã«ç”Ÿæˆå¾…ã¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã—")
        return

    BATCH = 40
    for i in range(0, len(ungenerated_keywords), BATCH):
        batch_keywords = ungenerated_keywords[i:i+BATCH]
        keyword_strings = [k.keyword for k in batch_keywords]

        enqueue_generation(
            user_id      = site.user_id,
            site_id      = site.id,
            keywords     = keyword_strings,
            title_prompt = title_prompt,
            body_prompt  = body_prompt,
            format       = "html",
            self_review  = False,
        )

        # âœ… ä¿®æ­£ï¼šã‚­ãƒ¥ãƒ¼æŠ•å…¥æ¸ˆã¿ã¨ã—ã¦ status ã‚’æ›´æ–°
        for k in batch_keywords:
            k.status = "queued"

        db.session.commit()

        current_app.logger.info(
            f"[GSC LOOP] {site.name} â†’ batch {i//BATCH+1}: {len(batch_keywords)} ä»¶ã‚­ãƒ¥ãƒ¼æŠ•å…¥"
        )


def _gsc_generation_job(app):
    """
    âœ… GSCè¨˜äº‹è‡ªå‹•ç”Ÿæˆã‚¸ãƒ§ãƒ–
    """
    with app.app_context():
        from app.models import Site

        current_app.logger.info("ğŸ“ GSCè¨˜äº‹ç”Ÿæˆã‚¸ãƒ§ãƒ–ã‚’é–‹å§‹ã—ã¾ã™")
        sites = Site.query.filter_by(gsc_connected=True).all()

        for site in sites:
            try:
                gsc_loop_generate(site)
            except Exception as e:
                current_app.logger.warning(f"[GSCè‡ªå‹•ç”Ÿæˆ] å¤±æ•— - {site.url}: {e}")

        current_app.logger.info("âœ… GSCè¨˜äº‹ç”Ÿæˆã‚¸ãƒ§ãƒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ†• GSCã‚ªãƒ¼ãƒˆã‚¸ã‚§ãƒ³ï¼ˆæ—¥æ¬¡ãƒ»æ–°ç€é™å®šãƒ»ä¸Šé™ãƒ»DRYRUNãƒ»è¦‹ãˆã‚‹åŒ–ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def gsc_autogen_daily_job(app):
    """
    ENVã®UTCæ™‚åˆ»ã«æ—¥æ¬¡ã§èµ·å‹•ï¼š
      - å¯¾è±¡ï¼šgsc_connected=True & gsc_generation_started=True
      - æ–°ç€æŠ½å‡ºï¼šfetch_new_queries_since(site)
      - äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ï¼šKeyword/Article æ—¢å­˜æ’é™¤
      - ä¸Šé™ï¼šENV GSC_AUTOGEN_LIMIT
      - DRYRUNï¼šENV GSC_AUTOGEN_DRYRUN=1 ãªã‚‰æŠ•å…¥ã›ãšã‚«ã‚¦ãƒ³ãƒˆã®ã¿
      - ã‚µãƒãƒªä¿å­˜ï¼šGSCAutogenDailyï¼ˆrun_date=JSTï¼‰
    """
    from app.models import PromptTemplate  # å±€æ‰€ importï¼ˆå¾ªç’°å›é¿ï¼‰
    from app.article_generator import enqueue_generation
    JST = pytz.timezone("Asia/Tokyo")
    jst_today = datetime.utcnow().replace(tzinfo=pytz.utc).astimezone(JST).date()

    limit_per_site = int(os.getenv("GSC_AUTOGEN_LIMIT", "50"))
    dryrun = os.getenv("GSC_AUTOGEN_DRYRUN", "1") == "1"

    with app.app_context():
        # æ—¥æ¬¡ã‚ªãƒ¼ãƒˆã‚¸ã‚§ãƒ³æœ‰åŠ¹ãƒ•ãƒ©ã‚°ï¼ˆmigration: gsc_autogen_dailyï¼‰ã‚’ä½¿ç”¨
        sites = Site.query.filter_by(gsc_connected=True, gsc_autogen_daily=True).all()
        current_app.logger.info("[GSC-AUTOGEN] start: targets=%s limit=%s dryrun=%s", len(sites), limit_per_site, int(dryrun))

        for site in sites:
            if not _try_lock_site(site.id):
                current_app.logger.info("[GSC-AUTOGEN] skip (locked) site=%s", site.id)
                continue
            started_at = datetime.utcnow()
            error_msg: Optional[str] = None
            try:
                # 1) æ–°ç€æŠ½å‡ºï¼ˆcutoff ä»¥é™ãƒ»28d impr ã—ãã„å€¤ã¯é–¢æ•°å†…ã§ENVåæ˜ ï¼‰
                rows = fetch_new_queries_since(site)
                candidate_keywords = [r["query"] for r in rows]
                picked_cnt = len(candidate_keywords)

                # 2) äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆé‡è¤‡ã‚„æ—¢å­˜è¨˜äº‹ã®é™¤å¤–ï¼‰
                filt = filter_autogen_candidates(site.id, candidate_keywords)
                deduped = filt["deduped"]
                dup_cnt = len(filt["dup_keywords"]) + len(filt["art_dup_keywords"])

                # 3) ä¸Šé™
                allowed = deduped[: max(0, limit_per_site)]
                limit_skipped = max(0, len(deduped) - len(allowed))

                queued_cnt = 0
                sample = allowed[:10]

                # 4) DRYRUN or å®ŸæŠ•å…¥
                if dryrun or not allowed:
                    pass  # ä½•ã‚‚ã—ãªã„ï¼ˆã‚«ã‚¦ãƒ³ãƒˆã®ã¿ï¼‰
                else:
                    # 4-1) Keyword ã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„ã¯ãšã ãŒå¿µã®ãŸã‚é‡è¤‡æ’é™¤ï¼‰
                    existing = {
                        r[0]
                        for r in db.session.query(Keyword.keyword)
                        .filter(Keyword.site_id == site.id, Keyword.keyword.in_(allowed))
                        .all()
                    }
                    to_insert = [kw for kw in allowed if kw not in existing]
                    for kw in to_insert:
                        db.session.add(Keyword(
                            keyword=kw,
                            site_id=site.id,
                            user_id=site.user_id,
                            source="gsc",
                            status="pending",
                            used=False
                        ))
                    if to_insert:
                        db.session.commit()

                    # 4-2) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå–å¾—
                    prompt = (PromptTemplate.query
                              .filter_by(user_id=site.user_id)
                              .order_by(PromptTemplate.id.desc())
                              .first())
                    title_pt = prompt.title_pt if prompt else ""
                    body_pt  = prompt.body_pt  if prompt else ""

                    # 4-3) ã‚­ãƒ¥ãƒ¼æŠ•å…¥ï¼ˆæ—¢å­˜ã® enqueue_generation ã‚’åˆ©ç”¨ï¼‰
                    enqueue_generation(
                        user_id=site.user_id,
                        site_id=site.id,
                        keywords=allowed,
                        title_prompt=title_pt,
                        body_prompt=body_pt,
                        format="html",
                        self_review=False,
                        source="gsc",
                    )
                    queued_cnt = len(allowed)

                # 5) ã‚µãƒãƒªä¿å­˜ï¼ˆupsertï¼‰
                rec = GSCAutogenDaily.query.filter_by(site_id=site.id, run_date=jst_today).first()
                if not rec:
                    rec = GSCAutogenDaily(site_id=site.id, run_date=jst_today)
                rec.picked = int(picked_cnt)
                rec.queued = int(queued_cnt)
                rec.dup = int(dup_cnt)
                rec.limit_skipped = int(limit_skipped)
                rec.dryrun = int(dryrun)
                rec.sample_keywords_json = json.dumps(sample, ensure_ascii=False)
                rec.started_at = rec.started_at or started_at
                rec.finished_at = datetime.utcnow()
                rec.error = None
                db.session.add(rec)
                db.session.commit()

                current_app.logger.info(
                    "[GSC-AUTOGEN] site=%s pick=%s queued=%s dup=%s limit=%s dryrun=%s",
                    site.id, picked_cnt, queued_cnt, dup_cnt, limit_skipped, int(dryrun)
                )
            except Exception as e:
                db.session.rollback()
                error_msg = str(e)
                # ã‚µãƒãƒªã«ã‚‚ã‚¨ãƒ©ãƒ¼ã‚’æ®‹ã™
                try:
                    rec = GSCAutogenDaily.query.filter_by(site_id=site.id, run_date=jst_today).first()
                    if not rec:
                        rec = GSCAutogenDaily(site_id=site.id, run_date=jst_today)
                    rec.started_at = rec.started_at or started_at
                    rec.finished_at = datetime.utcnow()
                    rec.error = error_msg
                    db.session.add(rec)
                    db.session.commit()
                except Exception:
                    pass
                current_app.logger.exception("[GSC-AUTOGEN] failed site=%s: %s", site.id, error_msg)
            finally:
                _unlock_site(site.id)


# --------------------------------------------------------------------------- #
# ğŸ†• Pending Regenerator Jobï¼ˆé€šå¸¸ & å¤–éƒ¨SEOï¼‰â€” æ‰‹å‹•å†ç”Ÿæˆã¨åŒã˜ãƒ•ãƒ­ãƒ¼ã‚’è‡ªå‹•ã§å®Ÿè¡Œ
# --------------------------------------------------------------------------- #
def _pending_regenerator_job(app):
    """
    40åˆ†ãŠãã«å®Ÿè¡Œ:
      - é€šå¸¸è¨˜äº‹ï¼ˆsource <> 'external'ï¼‰ã§ status IN ('pending','gen') ã‚’å†ç”Ÿæˆ
        * ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«ç›´è¿‘ã® PromptTemplate ã‚’ä½¿ã£ã¦ã€_generate() ã‚’å‘¼ã¶
      - å¤–éƒ¨SEOï¼ˆsource='external'ï¼‰ã§ status IN ('pending','gen') ã‚‚å†ç”Ÿæˆ
        * external_seo_generator ã®å›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ã£ã¦ã€_generate() ã‚’å‘¼ã¶
    æ—¢å­˜ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«/æŠ•ç¨¿ãƒ•ãƒ­ãƒ¼ã«ã¯ä¸€åˆ‡æ‰‹ã‚’åŠ ãˆãªã„ï¼ˆå‰¯ä½œç”¨ãªã—ï¼‰ã€‚
    """
    with app.app_context():
        try:
            # ======== ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹ä¸Šé™ï¼ˆå®‰å…¨ï¼‰ =========
            normal_per_run      = int(os.getenv("PENDING_REGEN_NORMAL_PER_RUN", "200"))
            normal_per_user_max = int(os.getenv("PENDING_REGEN_NORMAL_PER_USER", "60"))
            ext_per_run         = int(os.getenv("PENDING_REGEN_EXT_PER_RUN", "20"))
            normal_workers      = int(os.getenv("PENDING_REGEN_WORKERS", "10"))
            ext_workers         = int(os.getenv("PENDING_REGEN_EXT_WORKERS", "4"))

            # ------------------------------
            # 1) é€šå¸¸è¨˜äº‹ï¼ˆsource <> 'external'ï¼‰
            # ------------------------------
            if normal_per_run > 0:
                # pending/gen ã‚’æŒã¤ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å¤šã„é †ã«æŠ½å‡º
                user_rows = db.session.execute(text("""
                    WITH u_has_prompt AS (
                      SELECT user_id, 1 AS has_prompt
                      FROM prompt_template
                      GROUP BY user_id
                    )
                    SELECT
                      a.user_id,
                      COUNT(*) AS pending_cnt,
                      COALESCE(MAX(u.has_prompt), 0) AS has_prompt
                    FROM articles a
                    LEFT JOIN u_has_prompt u ON u.user_id = a.user_id
                    WHERE a.status IN ('pending','gen') AND (a.source IS NULL OR a.source <> 'external')
                    GROUP BY a.user_id
                    ORDER BY pending_cnt DESC
                """)).mappings().all()

                picked_normal = []
                for row in user_rows:
                    if len(picked_normal) >= normal_per_run:
                        break
                    uid = int(row["user_id"])
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒç„¡ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ‰‹å‹•å†ç”Ÿæˆã®ä»•æ§˜ã«åˆã‚ã›ã‚‹ï¼‰
                    prompt = (PromptTemplate.query
                              .filter_by(user_id=uid)
                              .order_by(PromptTemplate.id.desc())
                              .first())
                    if not prompt:
                        current_app.logger.info(f"[pending-regenerator] skip user {uid}: no PromptTemplate")
                        continue

                    remain = normal_per_run - len(picked_normal)
                    take   = min(normal_per_user_max, remain)
                    if take <= 0:
                        break

                    arts = (Article.query
                            .filter(Article.user_id == uid,
                                    Article.status.in_(["pending","gen"]),
                                    (Article.source == None) | (Article.source != "external"))  # noqa: E711
                            .order_by(Article.created_at.asc())
                            .limit(take)
                            .all())
                    for a in arts:
                        # æ—¢ã« posted/done ãªã©ã«èª¤ã£ã¦æ··å…¥ã—ã¦ã„ãªã„ã‹ä¿é™º
                        if a.status not in ("pending","gen"):
                            continue
                        picked_normal.append((a.id, uid, prompt.title_pt or "", prompt.body_pt or ""))
                        if len(picked_normal) >= normal_per_run:
                            break

                if picked_normal:
                    current_app.logger.info(f"[pending-regenerator] normal picked={len(picked_normal)} users={len(user_rows)}")
                    from concurrent.futures import ThreadPoolExecutor, as_completed
                    with ThreadPoolExecutor(max_workers=normal_workers) as ex:
                        futs = [
                            ex.submit(_generate, app, aid, tpt, bpt, "html", False, user_id=uid)
                            for (aid, uid, tpt, bpt) in picked_normal
                        ]
                        for f in as_completed(futs):
                            try:
                                f.result()
                            except Exception as e:
                                current_app.logger.exception(f"[pending-regenerator] normal generate error: {e}")

            # ------------------------------
            # 2) å¤–éƒ¨SEOï¼ˆsource = 'external'ï¼‰
            # ------------------------------
            if ext_per_run > 0:
                ext_articles = (Article.query
                                .filter(Article.status.in_(["pending","gen"]),
                                        Article.source == "external")
                                .order_by(Article.created_at.asc())
                                .limit(ext_per_run)
                                .all())
                if ext_articles:
                    current_app.logger.info(f"[pending-regenerator] external picked={len(ext_articles)}")
                    from concurrent.futures import ThreadPoolExecutor, as_completed
                    with ThreadPoolExecutor(max_workers=ext_workers) as ex:
                        futs = [
                            # external ã¯å›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å†ç”Ÿæˆï¼ˆæ—¢å­˜ã®è‡ªå‹•ç”Ÿæˆã¨åŒä¸€ãƒ­ã‚¸ãƒƒã‚¯ã®æ ¸ã‚’ä½¿ç”¨ï¼‰
                            ex.submit(_generate, app, art.id, EXT_TITLE_PROMPT, EXT_BODY_PROMPT, "html", False, user_id=art.user_id)
                            for art in ext_articles
                        ]
                        for f in as_completed(futs):
                            try:
                                f.result()
                            except Exception as e:
                                current_app.logger.exception(f"[pending-regenerator] external generate error: {e}")

        except Exception as e:
            current_app.logger.exception(f"[pending-regenerator] job failed: {e}")

# --------------------------------------------------------------------------- #
# ğŸ†• Internal SEO Refill Jobï¼ˆAæ¡ˆï¼‰â€” ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ã«â€œå¼¾ï¼ˆpendingï¼‰â€ã‚’è£œå……ã™ã‚‹ã ã‘
# --------------------------------------------------------------------------- #
def _internal_seo_user_refill_job(app):
    """
    ç›®çš„:
      - ãƒ¦ãƒ¼ã‚¶ãƒ¼å˜ä½ã§ pendingï¼ˆInternalLinkAction.status='pending'ï¼‰ã®â€œdistinct post_id æ•°â€ã‚’é›†è¨ˆ
      - ã—ãã„å€¤(INTERNAL_SEO_REFILL_TARGET)ã‚’ä¸‹å›ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦ã€
        ãã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚µã‚¤ãƒˆã‹ã‚‰é‡è¤‡ç„¡ã internal_seo_job_queue ã¸ job_kind='refill' ã‚’æŠ•å…¥
      - refill ã‚¸ãƒ§ãƒ–ã¯ limit_posts=0 ã‚’å¼·åˆ¶ã—ã€Applier ã‚’å®Ÿè³ªã‚¹ã‚­ãƒƒãƒ—ï¼ˆï¼è£œçµ¦å°‚ç”¨ï¼‰
      - é©ç”¨ï¼ˆapplyï¼‰ã¯ user_scheduler å´ãŒå›ã™æƒ³å®š
    """
    with app.app_context():
        try:
            # ENVï¼ˆå®‰å…¨ãªæ—¢å®šå€¤ä»˜ãï¼‰
            enabled = os.getenv("INTERNAL_SEO_REFILL_ENABLED", "1") != "0"
            if not enabled:
                current_app.logger.info("[refill] disabled by env")
                return
            target = int(os.getenv("INTERNAL_SEO_REFILL_TARGET", "50"))  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®ç›®æ¨™ pending è¨˜äº‹æ•°
            per_user_cap = int(os.getenv("INTERNAL_SEO_REFILL_MAX_ENQUEUE_PER_USER", "2"))  # 1tickã‚ãŸã‚ŠæŠ•å…¥ä¸Šé™/ãƒ¦ãƒ¼ã‚¶ãƒ¼

            # Plannerç”¨ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ ï¼ˆæœªæŒ‡å®šæ™‚ã¯æ—¢å­˜ENVã«ç´ ç›´ã«å¾“ã†ï¼‰
            pages         = int(os.getenv("INTERNAL_SEO_REFILL_PAGES",         os.getenv("INTERNAL_SEO_PAGES", "10")))
            per_page      = int(os.getenv("INTERNAL_SEO_REFILL_PER_PAGE",      os.getenv("INTERNAL_SEO_PER_PAGE", "100")))
            min_score     = float(os.getenv("INTERNAL_SEO_REFILL_MIN_SCORE",   os.getenv("INTERNAL_SEO_MIN_SCORE", "0.05")))
            max_k         = int(os.getenv("INTERNAL_SEO_REFILL_MAX_K",         os.getenv("INTERNAL_SEO_MAX_K", "80")))
            limit_sources = int(os.getenv("INTERNAL_SEO_REFILL_LIMIT_SOURCES", os.getenv("INTERNAL_SEO_LIMIT_SOURCES", "200")))
            # refill ã¯ Applier ã‚’å›ã•ãªã„ãŸã‚ 0 å¼·åˆ¶
            limit_posts   = 0

            # 1) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã® pending è¨˜äº‹æ•°ï¼ˆdistinct post_idï¼‰ã‚’é›†è¨ˆ
            #    FROM internal_link_actions â†’ JOIN site ï¼ˆå®‰å…¨ã«æ˜ç¤ºï¼‰
            pend_rows = (
                db.session.query(
                    Site.user_id.label("user_id"),
                    func.count(func.distinct(InternalLinkAction.post_id)).label("pending_posts"),
                )
                .select_from(InternalLinkAction)
                .join(Site, Site.id == InternalLinkAction.site_id)
                .filter(InternalLinkAction.status == "pending")
                .group_by(Site.user_id)
                .all()
            )
            pending_map = {int(r.user_id): int(r.pending_posts) for r in pend_rows}

            # 2) å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’åˆ—æŒ™ï¼ˆpending ãŒ 0 ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚‚å¯¾è±¡ã«ã™ã‚‹ãŸã‚ Site ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ï¼‰
            user_rows = db.session.query(Site.user_id).group_by(Site.user_id).all()
            all_user_ids = [int(u[0]) for u in user_rows]
            if not all_user_ids:
                current_app.logger.info("[refill] no users found (no sites)")
                return

            # 3) ã—ãã„å€¤ã‚’ä¸‹å›ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å¯¾è±¡ã«ã€æœªã‚­ãƒ¥ãƒ¼ã®ã‚µã‚¤ãƒˆã¸ 'refill' ã‚’æŠ•å…¥
            enq_total = 0
            skipped_locked = 0
            for uid in all_user_ids:
                # ğŸ›¡ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆé–‹å§‹ãƒœã‚¿ãƒ³æœªæŠ¼ä¸‹/ä¸€æ™‚åœæ­¢/ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç„¡ã—ã¯è£œçµ¦ã—ãªã„ï¼‰
                sched = InternalSeoUserSchedule.query.filter_by(user_id=uid).one_or_none()
                if not sched:
                    current_app.logger.info("[refill] skip uid=%s (no user schedule)", uid)
                    continue
                if not sched.is_enabled:
                    current_app.logger.info("[refill] skip uid=%s (user schedule disabled)", uid)
                    continue
                if getattr(sched, "status", None) == "paused":
                    current_app.logger.info("[refill] skip uid=%s (user schedule paused)", uid)
                    continue
                cur = pending_map.get(uid, 0)
                if cur >= target:
                    continue  # ç›®æ¨™ã«é”ã—ã¦ã„ã‚‹

                # ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¯¾è±¡ã‚µã‚¤ãƒˆï¼ˆæ—¢ã« queued/running ãŒç„¡ã„ã‚µã‚¤ãƒˆã‚’æŠ½å‡ºï¼‰
                rows = db.session.execute(text("""
                    SELECT s.id
                      FROM site s
                 LEFT JOIN internal_seo_job_queue q
                        ON q.site_id = s.id AND q.status IN ('queued','running')
                     WHERE s.user_id = :uid
                       AND q.site_id IS NULL
                """), {"uid": uid}).fetchall()
                site_ids = [int(r[0]) for r in rows]
                if not site_ids:
                    continue

                need = min(per_user_cap, max(1, (target - cur + 1) // 2))  # æ¬ ä¹åº¦ã«å¿œã˜ã¦æ§ãˆã‚ã«æŠ•å…¥
                picked = site_ids[:need]

                # 4) enqueue API ã‚’ä½¿ç”¨ï¼ˆå†…éƒ¨ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼†commit æ¸ˆã¿ï¼‰
                for sid in picked:
                    res = enqueue_refill_for_site(
                        sid,
                        pages=pages,
                        per_page=per_page,
                        min_score=min_score,
                        max_k=max_k,
                        limit_sources=limit_sources,
                        incremental=True,
                        job_kind="refill",
                    )
                    if res.get("enqueued"):
                        enq_total += 1
                    else:
                        # æ—¢ã« queued/running ãªã©
                        if res.get("reason") == "already-queued-or-running":
                            skipped_locked += 1

            current_app.logger.info(
                f"[refill] enqueued={enq_total} skipped_locked={skipped_locked} "
                f"target={target} cap/u={per_user_cap} params={{pages:{pages}, per_page:{per_page}, "
                f"min_score:{min_score}, max_k:{max_k}, limit_sources:{limit_sources}}}"
            )
        except Exception as e:
            db.session.rollback()
            current_app.logger.exception(f"[refill] job failed: {e}")



# app/tasks.py ã©ã“ã§ã‚‚ OK ã§ã™ãŒ _run_external_seo_job ã®ç›´å‰ã‚ãŸã‚ŠãŒèª­ã¿ã‚„ã™ã„
def _run_livedoor_signup(app, site_id: int) -> None:
    """
    1) livedoor_signup.signup() ã‚’å‘¼ã³å‡ºã— ExternalBlogAccount ã‚’æ–°è¦ä½œæˆ
       - æˆåŠŸæ™‚ã¯ ExternalSEOJob ã‚’ step='generate' ã§ä½œæˆ / æ›´æ–°
       - å¤±æ•—æ™‚ã¯ status='error' ã®ã‚¸ãƒ§ãƒ–ã‚’æ®‹ã™
    2) API ã‚­ãƒ¼ãªã©è©³ç´°ãƒ­ã‚°ã¯ ExternalBlogAccount ã®ã‚«ãƒ©ãƒ ã«ä¿å­˜æ¸ˆã¿
    """
    with app.app_context():
        try:
            # â‘  Site ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ã—ã¦å­˜åœ¨ç¢ºèª
            site = Site.query.get(site_id)
            if not site:
                raise ValueError(f"Site id={site_id} not found")

            # â‘¡ livedoor ã‚¢ã‚«ã‚¦ãƒ³ãƒˆè‡ªå‹•ç™»éŒ²
            acc = livedoor_signup(site)   # â† Site ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™
            current_app.logger.info(
                "[LD-Signup] success: site=%s account_id=%s", site_id, acc.id
            )

            # â‘¢ ã‚¸ãƒ§ãƒ–è¡Œã‚’ running â†’ generate ã¸
            job = ExternalSEOJob(
                site_id     = site_id,
                blog_type   = BlogType.LIVEDOOR,
                status      = "running",
                step        = "generate",
                article_cnt = 0,
                message     = "signup OK",
            )
            db.session.add(job)
            db.session.commit()

            # TODO: enqueue_generation(job.id) ãªã©ã§è¨˜äº‹ç”Ÿæˆã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥ã™ã‚‹å ´åˆã¯ã“ã“

        except Exception as e:
            # â‘£ å¤±æ•—æ™‚ï¼šã‚¨ãƒ©ãƒ¼ã‚¸ãƒ§ãƒ–ã‚’æ®‹ã™ & ãƒ­ã‚°
            current_app.logger.exception("[LD-Signup] failed: %s", e)

            job = ExternalSEOJob(
                site_id   = site_id,
                blog_type = BlogType.LIVEDOOR,
                status    = "error",
                step      = "error",
                message   = str(e),
            )
            db.session.add(job)
            db.session.commit()

            
def enqueue_livedoor_signup(site_id: int):
    """
    å¤–éƒ¨SEOé–‹å§‹ãƒœã‚¿ãƒ³ â†’ ã“ã®é–¢æ•°ã‚’å‘¼ã¶
    """
    app = current_app._get_current_object()
    executor.submit(_run_livedoor_signup, app, site_id)


def _run_generate_and_schedule(app, user_id: int, site_id: int, blog_account_id: int,
                               count: int = 100, per_day: int = 10, start_day_jst=None):
    """
    å¤–éƒ¨SEO 100æœ¬ç”Ÿæˆï¼‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚’â€œã‚¢ãƒ—ãƒªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã§â€å®Ÿè¡Œã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼æœ¬ä½“
    """
    with app.app_context():
        try:
            created = generate_and_schedule_external_articles(
                user_id=user_id,
                site_id=site_id,
                blog_account_id=blog_account_id,
                count=count,
                per_day=per_day,
                start_day_jst=start_day_jst,
            )
            current_app.logger.info(
                "[external-seo] generate+schedule done: site=%s acct=%s created=%s",
                site_id, blog_account_id, created
            )
        except Exception as e:
            current_app.logger.exception("[external-seo] generate+schedule failed: %s", e)


def enqueue_generate_and_schedule(user_id: int, site_id: int, blog_account_id: int,
                                  count: int = 100, per_day: int = 10, start_day_jst=None):
    """
    ãƒ«ãƒ¼ãƒˆã‹ã‚‰å‘¼ã¶è»½é‡é–¢æ•°ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã§éåŒæœŸå®Ÿè¡Œï¼‰
    """
    app = current_app._get_current_object()
    executor.submit(
        _run_generate_and_schedule, app,
        user_id, site_id, blog_account_id, count, per_day, start_day_jst
    )


# --------------------------------------------------------------------------- #
# 4) å¤–éƒ¨SEO â‘  ã‚­ãƒ¥ãƒ¼ä½œæˆã‚¸ãƒ§ãƒ–
# --------------------------------------------------------------------------- #



# --------------------------------------------------------------------------- #
# 5) å¤–éƒ¨SEO â‘¡ æŠ•ç¨¿ã‚¸ãƒ§ãƒ–
# --------------------------------------------------------------------------- #

def _finalize_external_job(job_id: int):
    """åŒä¸€ã‚µã‚¤ãƒˆã®â€œã“ã®ã‚¸ãƒ§ãƒ–ä»¥é™ã«ä½œã‚‰ã‚ŒãŸâ€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å®Œäº†ã‚’é›†è¨ˆã—ã¦ã€å…¨éƒ¨ posted ãªã‚‰ success ã«ã™ã‚‹"""
    job = ExternalSEOJob.query.get(job_id)
    if not job:
        return

    # ã“ã®ã‚¸ãƒ§ãƒ–ä½œæˆä»¥é™ã«ç”Ÿæˆã•ã‚ŒãŸã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã ã‘ã‚’å¯¾è±¡ã«ã™ã‚‹ï¼ˆåŒä¸€ã‚µã‚¤ãƒˆã®åˆ¥ãƒãƒƒãƒã¨æ··ã–ã‚‰ãªã„ãŸã‚ï¼‰
    q_base = (ExternalArticleSchedule.query
              .join(ExternalBlogAccount,
                    ExternalArticleSchedule.blog_account_id == ExternalBlogAccount.id)
              .filter(ExternalBlogAccount.site_id == job.site_id,
                      ExternalArticleSchedule.created_at >= job.created_at))

    total = q_base.count()
    posted = q_base.filter(ExternalArticleSchedule.status == "posted").count()

    # total ãŒ 0 ã®é–“ã¯åˆ¤å®šã—ãªã„
    if total and posted >= total:
        job.step = "done"
        job.status = "success"
        db.session.commit()
        

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¤–éƒ¨ãƒ–ãƒ­ã‚°æŠ•ç¨¿ã‚¸ãƒ§ãƒ–ï¼ˆå¤–éƒ¨SEOãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å³å¯†ç´ä»˜ã‘ç‰ˆï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _run_external_post_job(app, schedule_id: int | None = None):
    """
    å¤–éƒ¨SEOè¨˜äº‹ã‚’å¤–éƒ¨ãƒ–ãƒ­ã‚°ã«æŠ•ç¨¿ã™ã‚‹ã‚¸ãƒ§ãƒ–
    - ExternalArticleSchedule ã® pending ã‚’å¯¾è±¡
    - ExternalBlogAccount ãŒ LIVEDOOR ã®ã¿å¯¾å¿œ
    - âœ… Article ã¯ sched.article_id ã§ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå–å¾—ï¼ˆkeyword ç…§åˆã¯å»ƒæ­¢ï¼‰
    - äºŒé‡æŠ•ç¨¿é˜²æ­¢ï¼šArticle.posted_url ãŒç©ºã®ã‚‚ã®ã®ã¿
    """
    from datetime import datetime
    from flask import current_app
    from sqlalchemy import or_
    from app import db
    from app.models import (
        ExternalArticleSchedule,
        ExternalBlogAccount,
        ExternalSEOJob,
        BlogType,
        Article,
    )
    from app.services.blog_post.livedoor_post import post_livedoor_article

    with app.app_context():
        now = datetime.utcnow()
        current_app.logger.info(f"[ExtPost] Job start at {now}")

        # â† ã“ã“ã‚’ rows_q ã«ã—ã¦ã€ãã®å¾Œã‚‚ rows_q ã‚’ä½¿ã†
        rows_q = (
            db.session.query(ExternalArticleSchedule, ExternalBlogAccount, Article)
            .join(ExternalBlogAccount, ExternalArticleSchedule.blog_account_id == ExternalBlogAccount.id)
            .join(Article, ExternalArticleSchedule.article_id == Article.id)
            .filter(
                ExternalArticleSchedule.status == "pending",
                ExternalArticleSchedule.scheduled_date <= now,
                Article.source == "external",
                or_(Article.posted_url == None, Article.posted_url == ""),  # noqa: E711
                Article.status == "done",
            )
        )

        if schedule_id is not None:
            rows_q = rows_q.filter(ExternalArticleSchedule.id == schedule_id)

        rows = (
            rows_q
            .order_by(ExternalArticleSchedule.scheduled_date.asc())
            .limit(1 if schedule_id is not None else 12)
            .all()
        )

        current_app.logger.info(f"[ExtPost] Found {len(rows)} pending schedules")
        if not rows:
            return

        for sched, acct, art in rows:
            try:
                # ãƒ–ãƒ­ã‚°ç¨®åˆ¥ãƒã‚§ãƒƒã‚¯
                if acct.blog_type != BlogType.LIVEDOOR:
                    sched.status = "error"
                    sched.message = f"æœªå¯¾å¿œãƒ–ãƒ­ã‚°ã‚¿ã‚¤ãƒ—: {acct.blog_type}"
                    db.session.commit()
                    current_app.logger.warning(
                        f"[ExtPost] Unsupported blog type {acct.blog_type} (sched_id={sched.id})"
                    )
                    continue

                # è¿½åŠ ã®å®‰å…¨ãƒã‚§ãƒƒã‚¯ï¼šè¨˜äº‹ã®ã‚µã‚¤ãƒˆã¨ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ã‚µã‚¤ãƒˆãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹
                if art.site_id != acct.site_id:
                    sched.status = "error"
                    sched.message = f"site mismatch: article.site_id={art.site_id} / account.site_id={acct.site_id}"
                    db.session.commit()
                    current_app.logger.error(
                        f"[ExtPost] Site mismatch (sched_id={sched.id}, art_id={art.id})"
                    )
                    continue

                current_app.logger.info(
                    f"[ExtPost] Posting article_id={art.id} (kw='{art.keyword}') to Livedoor..."
                )
                res = post_livedoor_article(acct, art.title, art.body)

                if res.get("ok"):
                    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ›´æ–°
                    sched.status = "posted"
                    sched.posted_url = res.get("url")
                    sched.posted_at = res.get("posted_at")

                    # è¨˜äº‹å´ã‚‚ posted
                    art.status = "posted"
                    if res.get("url"):
                        art.posted_url = res["url"]
                    art.posted_at = res.get("posted_at") or now

                    # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æŠ•ç¨¿ã‚«ã‚¦ãƒ³ãƒˆ
                    acct.posted_cnt = (acct.posted_cnt or 0) + 1

                    db.session.commit()
                    current_app.logger.info(f"[ExtPost] Success: {res.get('url')}")

                    # ã‚¸ãƒ§ãƒ–å®Œäº†åˆ¤å®šï¼ˆæœ€æ–°ã‚¸ãƒ§ãƒ–ã®ã¿ï¼‰
                    latest_job = (
                        ExternalSEOJob.query.filter_by(site_id=acct.site_id)
                        .order_by(ExternalSEOJob.id.desc())
                        .first()
                    )
                    if latest_job:
                        _finalize_external_job(latest_job.id)

                else:
                    # æŠ•ç¨¿å¤±æ•—
                    err = res.get("error") or "unknown error"
                    sched.status = "error"
                    sched.message = err
                    db.session.commit()
                    current_app.logger.error(f"[ExtPost] Failed: {err}")

            except Exception as e:
                current_app.logger.exception(f"[ExtPost] Exception during posting: {e}")
                sched.status = "error"
                sched.message = str(e)
                db.session.commit()




# --------------------------------------------------------------------------- #
# 6) å¤–éƒ¨SEO ãƒãƒƒãƒç›£è¦–ã‚¸ãƒ§ãƒ–ï¼ˆ100 æœ¬å®Œäº†ã”ã¨ã«æ¬¡ãƒãƒƒãƒï¼‰
# --------------------------------------------------------------------------- #


def init_scheduler(app):
    """
    Flask ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«å‘¼ã³å‡ºã—ã¦:
      1) APScheduler ã«è‡ªå‹•æŠ•ç¨¿ã‚¸ãƒ§ãƒ–ã‚’ç™»éŒ²
      2) 3åˆ†é–“éš”ã§ _auto_post_job ã‚’å®Ÿè¡Œã™ã‚‹ã‚ˆã†ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
      - GSCè¨˜äº‹ç”Ÿæˆã‚¸ãƒ§ãƒ–ï¼š10åˆ†é–“éš”ï¼ˆâ†ã“ã“ä¿®æ­£ï¼‰
    """
    scheduler.add_job(
        func=_auto_post_job,
        trigger="interval",
        minutes=3,
        args=[app],
        id="auto_post_job",
        replace_existing=True,
        max_instances=5
    )


    # ğŸ†• Pending å†ç”Ÿæˆã‚¸ãƒ§ãƒ–ï¼ˆ40åˆ†ãŠãï¼‰
    scheduler.add_job(
        func=_pending_regenerator_job,
        trigger="interval",
        minutes=40,
        args=[app],
        id="pending_regenerator_job",
        replace_existing=True,
        max_instances=1,
    )

    # âœ… GSCã‚¯ãƒªãƒƒã‚¯ãƒ»è¡¨ç¤ºå›æ•°ã‚’æ¯æ—¥0æ™‚ã«è‡ªå‹•æ›´æ–°ã™ã‚‹ã‚¸ãƒ§ãƒ–
    scheduler.add_job(
        func=_gsc_metrics_job,
        trigger="cron",
        hour=0,
        minute=0,
        args=[app],
        id="gsc_metrics_job",
        replace_existing=True,
        max_instances=1
    )

    # ğŸ†• âœ… GSCã‚ªãƒ¼ãƒˆã‚¸ã‚§ãƒ³ï¼ˆæ—¥æ¬¡ãƒ»æ–°ç€é™å®šï¼‰
    gsc_utc_hour = int(os.getenv("GSC_AUTOGEN_UTC_HOUR", "18"))
    gsc_utc_min  = int(os.getenv("GSC_AUTOGEN_UTC_MIN", "0"))
    scheduler.add_job(
        func=gsc_autogen_daily_job,
        trigger="cron",
        hour=gsc_utc_hour,
        minute=gsc_utc_min,
        args=[app],
        id="gsc_autogen_daily_job",
        replace_existing=True,
        max_instances=1
    )

    # âœ… å¤–éƒ¨ãƒ–ãƒ­ã‚°æŠ•ç¨¿ã‚¸ãƒ§ãƒ–ï¼ˆ10åˆ†ãŠãï¼‰
    scheduler.add_job(
        func=_run_external_post_job,
        trigger="interval",
        minutes=10,
        args=[app],
        id="external_post_job",
        replace_existing=True,
        max_instances=1
    )

    # âœ… å†…éƒ¨SEO ãƒŠã‚¤ãƒˆãƒªãƒ¼å®Ÿè¡Œï¼ˆç’°å¢ƒå¤‰æ•°ã§ON/OFFå¯èƒ½ï¼ãƒ¬ã‚¬ã‚·ãƒ¼é‹ç”¨ï¼‰
    #   - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æ¯æ—¥ 18:15 UTC = JST 03:15
    if os.getenv("INTERNAL_SEO_ENABLED", "1") == "1":
        utc_hour = int(os.getenv("INTERNAL_SEO_UTC_HOUR", "18"))
        utc_min  = int(os.getenv("INTERNAL_SEO_UTC_MIN",  "15"))
        scheduler.add_job(
            func=_internal_seo_nightly_job,
            trigger="cron",
            hour=utc_hour,
            minute=utc_min,
            args=[app],
            id="internal_seo_job",
            replace_existing=True,
            max_instances=1,
        )
        app.logger.info(f"Scheduler started: internal_seo_job daily at {utc_hour:02d}:{utc_min:02d} UTC")
    else:
        # æ˜ç¤ºçš„ã«ãƒ¬ã‚¬ã‚·ãƒ¼ãƒŠã‚¤ãƒˆãƒªãƒ¼ã‚’åœæ­¢ã—ã¦ã„ã‚‹ã“ã¨ã‚’èµ·å‹•ãƒ­ã‚°ã«æ®‹ã™
        app.logger.info("legacy internal-seo nightly OFF (INTERNAL_SEO_ENABLED!=1): skipping internal_seo_job")

    # âœ… å†…éƒ¨SEO ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆã‚­ãƒ¥ãƒ¼æ¶ˆåŒ–ï¼‰â€»æ¯åˆ†
    if os.getenv("INTERNAL_SEO_WORKER_ENABLED", "1") == "1":
        scheduler.add_job(
            func=_internal_seo_worker_tick,
            trigger="interval",
            minutes=int(os.getenv("INTERNAL_SEO_WORKER_INTERVAL_MIN", "1")),
            args=[app],
            id="internal_seo_worker_tick",
            replace_existing=True,
            max_instances=1,
        )
        app.logger.info("Scheduler started: internal_seo_worker_tick every minute")
    else:
        app.logger.info("Scheduler skipped: internal_seo_worker_tick (INTERNAL_SEO_WORKER_ENABLED!=1)")

    # âœ… å†…éƒ¨SEO ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®è‡ªå‹•ã‚¸ãƒ§ãƒ–ï¼ˆENVã§ON/OFFå¯èƒ½ï¼‰
    if os.getenv("INTERNAL_SEO_USER_ENABLED", "1") == "1":
        scheduler.add_job(
            func=user_scheduler.user_scheduler_tick,
            trigger="interval",
            minutes=int(os.getenv("INTERNAL_SEO_USER_INTERVAL_MIN", "1")),
            args=[app],
            id="internal_seo_user_scheduler_job",
            replace_existing=True,
            max_instances=1,
        )
        app.logger.info("Scheduler started: internal_seo_user_scheduler_job (user-scope tick)")
    else:
        app.logger.info("Scheduler skipped: internal_seo_user_scheduler_job (INTERNAL_SEO_USER_ENABLED!=1)")

    # ğŸ†• å†…éƒ¨SEO ãƒ¦ãƒ¼ã‚¶ãƒ¼é©ç”¨ãƒ«ãƒ¼ãƒ—ï¼ˆâ€œé–‹å§‹ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã ã‘â€é©ç”¨ã‚’å›ã™ï¼‰
    if os.getenv("INTERNAL_SEO_USER_APPLY_ENABLED", "1") == "1":
        scheduler.add_job(
            func=_internal_seo_user_apply_tick,
            trigger="interval",
            minutes=int(os.getenv("INTERNAL_SEO_USER_APPLY_INTERVAL_MIN", "3")),
            args=[app],
            id="internal_seo_user_apply_tick",
            replace_existing=True,
            max_instances=1,
        )
        app.logger.info("Scheduler started: internal_seo_user_apply_tick (user apply loop)")
    else:
        app.logger.info("Scheduler skipped: internal_seo_user_apply_tick (INTERNAL_SEO_USER_APPLY_ENABLED!=1)")            

    # ğŸ†• Internal SEO Refill Jobï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ã®â€œå¼¾è£œçµ¦â€å°‚ç”¨ï¼‰â€»ENVã§ON/OFF
    if os.getenv("INTERNAL_SEO_REFILL_ENABLED", "1") == "1":
        scheduler.add_job(
            func=_internal_seo_user_refill_job,
            trigger="interval",
            minutes=int(os.getenv("INTERNAL_SEO_REFILL_INTERVAL_MIN", "10")),
            args=[app],
            id="internal_seo_user_refill_job",
            replace_existing=True,
            max_instances=1,
        )
        app.logger.info("Scheduler started: internal_seo_user_refill_job (user refill)")
    else:
        app.logger.info("Scheduler skipped: internal_seo_user_refill_job (INTERNAL_SEO_REFILL_ENABLED!=1)")    
 
    scheduler.start()
    app.logger.info("Scheduler started: auto_post_job every 3 minutes")
    app.logger.info("Scheduler started: external_post_job every 10 minutes")
    app.logger.info("Scheduler started: gsc_metrics_job daily at 0:00")
    app.logger.info(f"Scheduler started: gsc_autogen_daily_job daily at {gsc_utc_hour:02d}:{gsc_utc_min:02d} UTC")
    app.logger.info("Scheduler started: pending_regenerator_job every 40 minutes")
    app.logger.info("Scheduler maybe started: internal_seo_user_refill_job (see env)")

    
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GSCã‚ªãƒ¼ãƒˆã‚¸ã‚§ãƒ³ï¼šäº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆã‚¿ã‚¹ã‚¯5ã§åˆ©ç”¨ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _build_dup_sets(site_id: int, candidates: List[str]) -> Tuple[Set[str], Set[str]]:
    """
    äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ã®ãŸã‚ã®â€œæ—¢å­˜é›†åˆâ€ã‚’ç”¨æ„ã€‚
    æˆ»ã‚Šå€¤:
      (gsc_keywords_set, article_dup_set)
        - gsc_keywords_set â€¦ Keyword(source='gsc') ã¨ã—ã¦æ—¢ã«å­˜åœ¨ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        - article_dup_set  â€¦ Article ã® pending/gen/done/posted ã«æ—¢å­˜ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    """
    from app.models import Keyword, Article
    gsc_keywords_set: Set[str] = {
        r[0] for r in db.session.query(Keyword.keyword)
        .filter(
            Keyword.site_id == site_id,
            Keyword.source == "gsc",
            Keyword.keyword.in_(candidates)
        ).all()
    }
    article_dup_set: Set[str] = {
        r[0] for r in db.session.query(Article.keyword)
        .filter(
            Article.site_id == site_id,
            Article.keyword.in_(candidates),
            Article.status.in_(["pending", "gen", "done", "posted"])
        ).all()
    }
    return gsc_keywords_set, article_dup_set

def filter_autogen_candidates(site_id: int, candidates: List[str]) -> Dict[str, List[str]]:
    """
    æ–°è¦æŠ•å…¥å‰ã®â€œäº‹å‰ãƒ•ã‚£ãƒ«ã‚¿â€ï¼šé‡è¤‡ã‚„è¡çªã®ã‚ã‚‹å€™è£œã‚’é™¤å¤–ã€‚
    æˆ»ã‚Šå€¤:
      {
        "deduped": [...],           # æŠ•å…¥å€™è£œï¼ˆé‡è¤‡é™¤å¤–å¾Œï¼‰
        "dup_keywords": [...],      # æ—¢å­˜ Keyword(source='gsc') ç”±æ¥ã®é™¤å¤–
        "art_dup_keywords": [...],  # æ—¢å­˜ Article ç”±æ¥ã®é™¤å¤–
      }
    """
    if not candidates:
        return {"deduped": [], "dup_keywords": [], "art_dup_keywords": []}
    gsc_dup, art_dup = _build_dup_sets(site_id, candidates)
    dup_keywords = sorted(list(gsc_dup))
    art_dup_keywords = sorted(list(art_dup))
    blocked = gsc_dup.union(art_dup)
    deduped = [kw for kw in candidates if kw not in blocked]
    return {
        "deduped": deduped,
        "dup_keywords": dup_keywords,
        "art_dup_keywords": art_dup_keywords,
    }    

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å†…éƒ¨SEO è‡ªå‹•åŒ–ã‚¸ãƒ§ãƒ–
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@with_db_retry(max_retries=3, backoff=1.8)
def _internal_seo_run_one(site_id: int,
                          pages: int,
                          per_page: int,
                          min_score: float,
                          max_k: int,
                          limit_sources: int,
                          limit_posts: int,
                          incremental: bool,
                          job_kind: str = "scheduler"):
    """
    1ã‚µã‚¤ãƒˆåˆ†ã®å†…éƒ¨SEOãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã€InternalSeoRun ã«è¨˜éŒ²ã™ã‚‹ã€‚
    CLIã®å®Ÿè£…ã¨åŒã˜ã‚¹ãƒ†ãƒƒãƒ—/åŒã˜çµ±è¨ˆã‚­ãƒ¼ã§ä¿å­˜ã™ã‚‹ã€‚
    """
    # ãƒ©ãƒ³ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆï¼ˆrunningï¼‰
    run = InternalSeoRun(
        site_id=site_id,
        job_kind=job_kind,
        status="running",
        started_at=datetime.utcnow(),
        stats={},
    )
    db.session.add(run)
    db.session.commit()

    t0 = time.perf_counter()
    try:
        # Indexer
        current_app.logger.info(f"[Indexer] site={site_id} incremental={incremental} pages={pages} per_page={per_page}")
        stats_idx = sync_site_content_index(site_id, per_page=per_page, max_pages=pages, incremental=incremental)
        current_app.logger.info(f"[Indexer] -> {stats_idx}")

        # LinkGraph
        current_app.logger.info(f"[LinkGraph] site={site_id} max_k={max_k} min_score={min_score}")
        stats_graph = build_link_graph_for_site(site_id, max_targets_per_source=max_k, min_score=min_score)
        current_app.logger.info(f"[LinkGraph] -> {stats_graph}")

        # Planner
        current_app.logger.info(f"[Planner] site={site_id} limit_sources={limit_sources} max_candidates={max_k} min_score={min_score}")
        stats_plan = plan_links_for_site(site_id, limit_sources=limit_sources, mode_swap_check=True,
                                         min_score=min_score, max_candidates=max_k)
        current_app.logger.info(f"[Planner] -> {stats_plan}")

        # Applierï¼ˆrefill ãªã© limit_posts<=0 ã®å ´åˆã¯å®Œå…¨ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        if int(limit_posts) <= 0:
            res_apply = {"applied": 0, "swapped": 0, "skipped": 0, "processed_posts": 0, "note": "applier skipped (limit_posts<=0)"}
            current_app.logger.info(f"[Applier] skipped (limit_posts<=0) site={site_id}")
        else:
            current_app.logger.info(f"[Applier] site={site_id} limit_posts={limit_posts}")
            res_apply = apply_actions_for_site(site_id, limit_posts=limit_posts, dry_run=False)
            current_app.logger.info(f"[Applier] -> {res_apply}")

        # æˆåŠŸã§ç¢ºå®š
        run.status = "success"
        run.ended_at = datetime.utcnow()
        run.duration_ms = int((time.perf_counter() - t0) * 1000)
        run.stats = {
            "indexer": stats_idx,
            "link_graph": stats_graph,
            "planner": stats_plan,
            "applier": res_apply,
            "params": {
                "incremental": incremental,
                "pages": pages,
                "per_page": per_page,
                "min_score": min_score,
                "max_k": max_k,
                "limit_sources": limit_sources,
                "limit_posts": limit_posts,
                "job_kind": job_kind,
            },
        }
        db.session.commit()
    except Exception as e:
        db.session.rollback()
        run.status = "error"
        run.ended_at = datetime.utcnow()
        run.duration_ms = int((time.perf_counter() - t0) * 1000)
        run.stats = (run.stats or {})
        run.stats["error"] = {"type": e.__class__.__name__, "message": str(e)}
        db.session.add(run)
        db.session.commit()
        current_app.logger.exception(f"[internal-seo] failed for site {site_id}: {e}")

@with_db_retry(max_retries=3, backoff=1.8)
def _internal_seo_worker_tick(app):
    """
    internal_seo_job_queue ã‹ã‚‰ 'queued' ã‚’å®‰å…¨ã«å–ã‚Šå‡ºã—ã€
    åŒæ™‚å®Ÿè¡Œä¸Šé™ã‚’å®ˆã‚Šã¤ã¤ _internal_seo_run_one ã‚’å›ã™ã€‚
    """
    with app.app_context():
        # åŒæ™‚å®Ÿè¡Œä¸Šé™ï¼ˆENV ã§èª¿æ•´ï¼‰
        max_parallel = int(os.getenv("INTERNAL_SEO_WORKER_PARALLELISM", "3"))
        # ã„ã¾èµ°ã£ã¦ã„ã‚‹ run æ•°
        running_cnt = db.session.execute(text("""
            SELECT COUNT(*) FROM internal_seo_runs WHERE status='running'
        """)).scalar_one()
        available = max(0, max_parallel - int(running_cnt or 0))
        if available <= 0:
            current_app.logger.info(f"[internal-seo worker] saturated: running={running_cnt} / max={max_parallel}")
            return

        # å–ã‚Šå‡ºã—ä»¶æ•°ã¯ä¸Šé™ã«æ§ãˆã‚ï¼ˆå¿µã®ãŸã‚ 1 ã‚µã‚¤ãƒˆ=1 run æƒ³å®šï¼‰
        take = min(available, int(os.getenv("INTERNAL_SEO_WORKER_TAKE", "2")))

        # queued ã‚’ãƒ­ãƒƒã‚¯ã—ã¦ running ã«æ›´æ–°ï¼ˆSKIP LOCKED ã§å¤šé‡å–å¾—å›é¿ï¼‰
        rows = db.session.execute(text(f"""
            WITH picked AS (
              SELECT id
              FROM internal_seo_job_queue
              WHERE status='queued'
              ORDER BY created_at ASC, id ASC
              FOR UPDATE SKIP LOCKED
              LIMIT :take
            )
            UPDATE internal_seo_job_queue q
               SET status='running', started_at=now()
              FROM picked p
             WHERE q.id = p.id
          RETURNING q.id, q.site_id, q.pages, q.per_page, q.min_score, q.max_k,
                    q.limit_sources, q.limit_posts, q.incremental, q.job_kind;
        """), {"take": take}).mappings().all()
        db.session.commit()

        if not rows:
            current_app.logger.info("[internal-seo worker] no queued jobs")
            return

        current_app.logger.info(f"[internal-seo worker] picked {len(rows)} job(s)")

        # 1ä»¶ãšã¤å®Ÿè¡Œï¼ˆåŒãƒ—ãƒ­ã‚»ã‚¹å†…ã§é€æ¬¡ã€‚å¿…è¦ãªã‚‰ ThreadPoolExecutor ã§ä¸¦åˆ—åŒ–ã‚‚å¯ï¼‰
        for r in rows:
            j_id = r["id"]
            try:
                _internal_seo_run_one(
                    site_id       = int(r["site_id"]),
                    pages         = int(r["pages"] or os.getenv("INTERNAL_SEO_PAGES", 10)),
                    per_page      = int(r["per_page"] or os.getenv("INTERNAL_SEO_PER_PAGE", 100)),
                    min_score     = float(r["min_score"] or os.getenv("INTERNAL_SEO_MIN_SCORE", 0.05)),
                    max_k         = int(r["max_k"] or os.getenv("INTERNAL_SEO_MAX_K", 80)),
                    limit_sources = int(r["limit_sources"] or os.getenv("INTERNAL_SEO_LIMIT_SOURCES", 200)),
                    limit_posts   = int(r["limit_posts"] or os.getenv("INTERNAL_SEO_LIMIT_POSTS", 50)),
                    incremental   = bool(r["incremental"]),
                    job_kind      = r["job_kind"] or "worker",
                )
                db.session.execute(text("""
                    UPDATE internal_seo_job_queue
                       SET status='done', ended_at=now()
                     WHERE id=:id
                """), {"id": j_id})
                db.session.commit()
            except Exception as e:
                current_app.logger.exception(f"[internal-seo worker] job {j_id} failed: {e}")
                db.session.execute(text("""
                    UPDATE internal_seo_job_queue
                       SET status='error', ended_at=now(), message=:msg
                     WHERE id=:id
                """), {"id": j_id, "msg": str(e)})
                db.session.commit()

def _internal_seo_nightly_job(app):
    """
    ã™ã¹ã¦ã®ã‚µã‚¤ãƒˆï¼ˆã¾ãŸã¯ ENV æŒ‡å®šã®ã‚µã‚¤ãƒˆç¾¤ï¼‰ã«ã¤ã„ã¦ã€
    â€œå®Ÿè¡Œæœ¬ä½“â€ ã¯ãƒ¯ãƒ¼ã‚«ãƒ¼ã«ä»»ã›ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã‚­ãƒ¥ãƒ¼ã«ç©ã‚€ã ã‘ã€‚
    æ—¢ã« queued/running ã®ã‚µã‚¤ãƒˆã¯é™¤å¤–ã—ã¦é‡è¤‡æŠ•å…¥ã‚’é˜²ãã€‚
    """
    with app.app_context():
        pages          = int(os.getenv("INTERNAL_SEO_PAGES", "10"))
        per_page       = int(os.getenv("INTERNAL_SEO_PER_PAGE", "100"))
        min_score      = float(os.getenv("INTERNAL_SEO_MIN_SCORE", "0.05"))
        max_k          = int(os.getenv("INTERNAL_SEO_MAX_K", "80"))
        limit_sources  = int(os.getenv("INTERNAL_SEO_LIMIT_SOURCES", "200"))
        limit_posts    = int(os.getenv("INTERNAL_SEO_LIMIT_POSTS", "50"))
        incremental    = os.getenv("INTERNAL_SEO_INCREMENTAL", "1") == "1"
        job_kind       = os.getenv("INTERNAL_SEO_JOB_KIND", "nightly-enqueue")

        only_ids = os.getenv("INTERNAL_SEO_SITE_IDS")
        if only_ids:
            ids = [int(x) for x in only_ids.split(",") if x.strip().isdigit()]
            site_predicate = "WHERE s.id = ANY(:ids)"
            params = {
                "ids": ids,
                "pages": pages, "per_page": per_page, "min_score": min_score, "max_k": max_k,
                "limit_sources": limit_sources, "limit_posts": limit_posts,
                "incremental": incremental, "job_kind": job_kind,
            }
        else:
            site_predicate = ""
            params = {
                "pages": pages, "per_page": per_page, "min_score": min_score, "max_k": max_k,
                "limit_sources": limit_sources, "limit_posts": limit_posts,
                "incremental": incremental, "job_kind": job_kind,
            }

        # æ—¢ã« queued/running ã®ã‚µã‚¤ãƒˆã¯é™¤å¤–ã—ãŸã†ãˆã§ä¸€æ‹¬INSERT
        sql = text(f"""
            INSERT INTO internal_seo_job_queue
              (site_id, pages, per_page, min_score, max_k, limit_sources, limit_posts,
               incremental, job_kind, status, created_at)
            SELECT
              s.id, :pages, :per_page, :min_score, :max_k, :limit_sources, :limit_posts,
              :incremental, :job_kind, 'queued', now()
            FROM site s
            LEFT JOIN internal_seo_job_queue q
                   ON q.site_id = s.id
                  AND q.status IN ('queued','running')
            {site_predicate}
            WHERE q.site_id IS NULL
        """)

        res = db.session.execute(sql, params)
        db.session.commit()
        inserted = res.rowcount or 0
        # å‚è€ƒã¾ã§ã®ç·ã‚µã‚¤ãƒˆæ•°ï¼ˆçµã‚Šè¾¼ã¿æ™‚ã¯ ids ã®é•·ã•ï¼‰
        total_sites = len(params["ids"]) if only_ids else (db.session.execute(text("SELECT COUNT(*) FROM site")).scalar() or 0)

        current_app.logger.info(
            f"[internal-seo nightly] enqueued {inserted}/{total_sites} "
            f"params={{pages:{pages}, per_page:{per_page}, min_score:{min_score}, "
            f"max_k:{max_k}, limit_sources:{limit_sources}, limit_posts:{limit_posts}, "
            f"incremental:{incremental}, job_kind:{job_kind}}}"
        )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ†• å†…éƒ¨SEO ãƒ¦ãƒ¼ã‚¶ãƒ¼é©ç”¨ãƒ†ã‚£ãƒƒã‚¯ï¼ˆå·¡å›ãƒ«ãƒ¼ãƒ—æœ¬ä½“ï¼‰
#   - æ¡ä»¶: InternalSeoUserSchedule.is_enabled=True ã‹ã¤ status<>'paused'
#   - ãã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦ apply_actions_for_user() ã‚’å®Ÿè¡Œ
#   - 1ãƒ†ã‚£ãƒƒã‚¯ã®ç·äºˆç®—ï¼†ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸Šé™ã¯ ENV ã§åˆ¶å¾¡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _internal_seo_user_apply_tick(app):
    with app.app_context():
        try:
            if os.getenv("INTERNAL_SEO_USER_APPLY_ENABLED", "1") != "1":
                return

            # âœ… 1ãƒ†ã‚£ãƒƒã‚¯ã®ç·å‡¦ç†è¨˜äº‹æ•°ï¼ˆå…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆç®—ã®ä¸Šé™ï¼‰
            total_budget = int(os.getenv("INTERNAL_SEO_USER_APPLY_BUDGET", "200"))
            # âœ… 1ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚ãŸã‚Šã®ä¸Šé™
            per_user_cap = int(os.getenv("INTERNAL_SEO_USER_APPLY_PER_USER", "50"))
            if total_budget <= 0 or per_user_cap <= 0:
                current_app.logger.info("[ISEO-USER] apply disabled by zero budget/cap")
                return

            # å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆé–‹å§‹ãƒœã‚¿ãƒ³ON & ä¸€æ™‚åœæ­¢ã§ãªã„ï¼‰
            rows = (InternalSeoUserSchedule.query
                    .filter(InternalSeoUserSchedule.is_enabled == True)  # noqa: E712
                    .filter((InternalSeoUserSchedule.status.is_(None)) | (InternalSeoUserSchedule.status != "paused"))
                    .order_by(InternalSeoUserSchedule.user_id.asc())
                    .all())
            if not rows:
                current_app.logger.info("[ISEO-USER] no eligible users")
                return

            remaining = total_budget
            picked = 0
            for sched in rows:
                if remaining <= 0:
                    break
                uid = int(sched.user_id)
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ­ãƒƒã‚¯ï¼ˆå¤šé‡å®Ÿè¡Œé˜²æ­¢ï¼‰
                if not _try_lock_user(uid):
                    current_app.logger.info("[ISEO-USER] skip (locked) user=%s", uid)
                    continue
                try:
                    quota = min(per_user_cap, remaining)
                    if quota <= 0:
                        break
                    # applier: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨ã‚µã‚¤ãƒˆã«å‡ç­‰é…åˆ†ã—ã¦ pending ã‚’å®Ÿåæ˜ 
                    res = apply_actions_for_user(user_id=uid, limit_posts=quota, dry_run=False)
                    # å®Ÿéš›ã«å‡¦ç†ã§ããŸè¨˜äº‹æ•°ã§äºˆç®—ã‚’æ¸›ã‚‰ã™
                    processed = int(res.get("processed_posts", 0) or 0)
                    remaining -= max(0, processed)
                    picked += 1
                    current_app.logger.info(
                        "[ISEO-USER] uid=%s processed=%s applied=%s swapped=%s skipped=%s pending_total=%s remaining_budget=%s",
                        uid,
                        processed,
                        int(res.get("applied", 0) or 0),
                        int(res.get("swapped", 0) or 0),
                        int(res.get("skipped", 0) or 0),
                        int(res.get("pending_total", 0) or 0),
                        remaining
                    )
                except Exception as e:
                    current_app.logger.exception("[ISEO-USER] apply failed uid=%s: %s", uid, e)
                finally:
                    _unlock_user(uid)

            current_app.logger.info("[ISEO-USER] tick done: users=%s total_budget=%s remaining=%s",
                                    picked, total_budget, remaining)
        except Exception as e:
            current_app.logger.exception("[ISEO-USER] tick error: %s", e)