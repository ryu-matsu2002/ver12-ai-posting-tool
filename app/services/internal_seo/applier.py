# app/services/internal_seo/applier.py
from __future__ import annotations

import logging
from sqlalchemy import and_
import re
from dataclasses import dataclass
from datetime import datetime, UTC
from html import unescape
from typing import Dict, List, Optional, Tuple
import os
import time
import random
import math

from app import db
from app.models import (
    ContentIndex,
    InternalLinkAction,
    InternalLinkGraph,
    InternalSeoConfig,
    Site,
)
from app.wp_client import fetch_single_post, update_post_content
from app.services.internal_seo.legacy_cleaner import find_and_remove_legacy_links
from app.services.internal_seo.utils import nfkc_norm, is_ng_anchor, title_tokens, extract_h2_sections, is_topic_url

logger = logging.getLogger(__name__)

_GEN_SUFFIX = "ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"

def _is_ng_anchor_generated_line(text: str, tgt_title: str | None = None) -> bool:
    """
    V4ã®â€œæ–‡ã‚¹ã‚¿ã‚¤ãƒ«â€ã‚¢ãƒ³ã‚«ãƒ¼ç”¨ã®NGåˆ¤å®šãƒ©ãƒƒãƒ‘ã€‚
    å›ºå®šçµ‚æ­¢å¥ã€Œã€œã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©ã€ã‚’å¤–ã—ãŸâ€œã‚³ã‚¢èªâ€ã§NGã‚’åˆ¤å®šã™ã‚‹ã€‚
    """
    if not text:
        return True
    core = re.sub(rf"{re.escape(_GEN_SUFFIX)}$", "", text).strip()
    # ã‚³ã‚¢ãŒæ¶ˆãˆã¦ã—ã¾ã†ï¼ˆ=å›ºå®šå¥ã ã‘ï¼‰ãªã‚‰å®‰å…¨ãƒ†ãƒ³ãƒ—ãƒ¬æ‰±ã„ã§NGã«ã—ãªã„
    if not core:
        return False
    return is_ng_anchor(core, tgt_title)


# ---- HTMLãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ----

# ====== æ–°æ–¹å¼ï¼šãƒ•ã‚¡ã‚¤ãƒ«å†…å®Œçµã®è¨­å®šãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ======
import os as _os
from typing import Any
try:
    from openai import OpenAI as _OpenAI, BadRequestError as _BadRequestError
    _OPENAI_CLIENT = _OpenAI(api_key=_os.getenv("OPENAI_API_KEY", ""))
except Exception:
    _OPENAI_CLIENT = None
    _BadRequestError = Exception

# â–¼ æ–°æ—§ã®åˆ‡æ›¿ï¼ˆã“ã“ã‚’ "legacy_phrase" ã«ã™ã‚Œã°å¾“æ¥å‹•ä½œï¼‰
ANCHOR_MODE: str = "generated_line"

# â–¼ ã‚¢ãƒ³ã‚«ãƒ¼ç”Ÿæˆã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆllm / templateï¼‰
#   - llm: ChatGPTã§ã€ŒKWã‚’è‡ªç„¶ã«å«ã‚€å®šå‹æ–‡ã€ã‚’ç”Ÿæˆï¼ˆæ¨å¥¨ãƒ»æ—¢å®šï¼‰
#   - template: LLMã‚’ä½¿ã‚ãš {KW}ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ© ã®å›ºå®šæ–‡
ANCHOR_STYLE: str = _os.getenv("INTERNAL_SEO_ANCHOR_STYLE", "llm").strip().lower() or "llm"

# â–¼ LLM å‘¼ã³å‡ºã—è¨­å®šï¼ˆè¨˜äº‹ç”Ÿæˆã¨åŒç­‰ã®æµå„€ã‚’æœ€å°é™è¸è¥²ï¼‰
ISEO_ANCHOR_MODEL: str = _os.getenv("OPENAI_MODEL", "gpt-4o-mini")
ISEO_ANCHOR_TEMPERATURE: float = 0.30
ISEO_ANCHOR_TOP_P: float = 0.9
ISEO_CTX_LIMIT: int = 4000
ISEO_SHRINK: float = 0.85
ISEO_MAX_TOKENS: int = 120           # çŸ­æ–‡ç”¨ã«ã‚„ã‚„æŠ‘åˆ¶
ISEO_ANCHOR_MAX_CHARS: int = 58      # ä¸Šé™ã‚’å°‘ã—ã‚¿ã‚¤ãƒˆã«

ANCHOR_SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯SEOã«é…æ…®ã™ã‚‹æ—¥æœ¬èªç·¨é›†è€…ã§ã™ã€‚"
    "å†…éƒ¨ãƒªãƒ³ã‚¯ã®ã‚¢ãƒ³ã‚«ãƒ¼æ–‡ï¼ˆ1è¡Œï¼‰ã‚’ä½œæˆã—ã¾ã™ã€‚å¿…ãšä»¥ä¸‹ã‚’å³å®ˆï¼š\n"
    "ãƒ»æ—¥æœ¬èªã§1æ–‡ã®ã¿ï¼ˆæ”¹è¡Œ/å¼•ç”¨ç¬¦/çµµæ–‡å­—/è£…é£¾ãªã—ï¼‰\n"
    "ãƒ»ãƒªãƒ³ã‚¯å…ˆã®ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’1ã€œ2èªã ã‘å«ã‚ã‚‹ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ä¸¸å†™ã—ç¦æ­¢ï¼‰\n"
    "ãƒ»æ–‡æœ«ã¯å¿…ãšã€Œã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©ã€ã§çµ‚ãˆã‚‹ï¼ˆå¥ç‚¹ã‚„èª­ç‚¹ã‚’ä»˜ã‘ãªã„ï¼‰\n"
    "ãƒ»ç…½ã‚Šèªã‚„å†—é•·è¡¨ç¾ã¯ç¦æ­¢ï¼ˆä¾‹ï¼šãœã²ï¼ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„ï¼å‚è€ƒã«ã—ã¦ãã ã•ã„ ãªã©ï¼‰\n"
    "ãƒ»å…¨ä½“ã§40ã€œ58å­—ã«åã‚ã‚‹\n"
)
ANCHOR_USER_PROMPT_TEMPLATE = (
    "ã€ãƒªãƒ³ã‚¯å…ˆã‚¿ã‚¤ãƒˆãƒ«ã€‘{dst_title}\n"
    "ã€ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé‡è¦åº¦é †ï¼‰ã€‘{dst_keywords}\n"
    "ã€æ®µè½ã®è¦æ—¨ï¼ˆæ–‡è„ˆãƒ’ãƒ³ãƒˆï¼‰ã€‘{src_hint}\n"
    "è¦ä»¶: ä¸Šè¨˜ã®ä»•æ§˜ã©ãŠã‚Šã€1æ–‡ãƒ»40ã€œ60å­—ã§ã€"
    "ã€ãƒªãƒ³ã‚¯å…ˆã‚¿ã‚¤ãƒˆãƒ«ã€‘ã®åè©ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¿…ãš1èªä»¥ä¸Šå«ã‚ã€ã€Œã€œã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©ã€ã§çµã¶ã‚¢ãƒ³ã‚«ãƒ¼æ–‡ã®ã¿å‡ºåŠ›ã€‚"
    "å‹•è©ã‚„é€£ä½“ä¿®é£¾ã ã‘ã§ä¸»èªãŒç„¡ã„æ–‡ï¼ˆä¾‹: å‘ä¸Šã•ã›ã‚‹ãŸã‚â€¦ï¼‰ã¯ä¸å¯ã€‚"
)

_P_CLOSE = re.compile(r"</p\s*>", re.IGNORECASE)
_A_TAG = re.compile(r'<a\s+[^>]*href=["\']([^"\']+)["\'][^>]*>(.*?)</a\s*>', re.IGNORECASE | re.DOTALL)
_TAG_STRIP = re.compile(r"<[^>]+>")
_SEO_CLASS = "ai-ilink"  # äº’æ›ç”¨ï¼ˆç”Ÿæˆæ™‚ã¯ä½¿ã‚ãªã„ã€‚æ—¢å­˜ã®å¾Œæ–¹äº’æ›å‡¦ç†ã§ã®ã¿å‚ç…§ï¼‰

_H_TAG = re.compile(r"<h[1-6]\b[^>]*>", re.IGNORECASE)
_H_BLOCK = re.compile(r"(<h[1-6]\b[^>]*>)(.*?)(</h[1-6]\s*>)", re.IGNORECASE | re.DOTALL)
_TOC_HINT = re.compile(
    r'(id=["\']toc["\']|class=["\'][^"\']*(?:\btoctitle\b|\btoc\b|\bez\-toc\b)[^"\']*["\']|\[/?toc[^\]]*\])',
    re.IGNORECASE
)
_STYLE_BLOCK = re.compile(r"<style\b[^>]*>.*?</style\s*>", re.IGNORECASE | re.DOTALL)

# ==== å†…éƒ¨SEO ä»•æ§˜ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆæ–°è¦ï¼‰ ====
# <a> ã«ã¯ä¸€åˆ‡å±æ€§ã‚’ä»˜ã‘ãªã„æ–¹é‡ã€‚ä»£æ›¿ã¨ã—ã¦ç›´å‰ã‚³ãƒ¡ãƒ³ãƒˆã§ç‰ˆç®¡ç†ã‚’è¡Œã†ã€‚
INTERNAL_SEO_SPEC_VERSION = "v14"
INTERNAL_SEO_SPEC_MARK    = f"<!-- ai-internal-link:{INTERNAL_SEO_SPEC_VERSION} -->"
ILINK_BOX_MARK            = f"<!-- ai-internal-link-box:{INTERNAL_SEO_SPEC_VERSION} -->"

# ã‚¹ã‚¿ã‚¤ãƒ«ãƒãƒ¼ã‚«ãƒ¼ã‚‚å¿…ãšåŒã˜ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«åŒæœŸã•ã›ã‚‹
_AI_STYLE_MARK = f"<!-- ai-internal-link-style:{INTERNAL_SEO_SPEC_VERSION} -->"

def _link_version_int() -> int:
    """
    INTERNAL_SEO_SPEC_VERSION (ä¾‹: 'v14') ã‚’æ•´æ•°ç‰ˆã«æ­£è¦åŒ–ã—ã¦è¿”ã™ã€‚
    ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ link_version ãŒ NOT NULL ãªã®ã§ã€ãƒ­ã‚°æŒ¿å…¥æ™‚ã«å¿…ãšä½¿ç”¨ã€‚
    """
    try:
        m = re.search(r"(\d+)", INTERNAL_SEO_SPEC_VERSION)
        return int(m.group(1)) if m else 0
    except Exception:
        return 0

def _split_paragraphs(html: str) -> List[str]:
    if not html:
        return []
    # ã¾ãš </p> ã§åˆ†å‰²
    parts = [p for p in _P_CLOSE.split(html) if p is not None]
    # æ®µè½ãŒ1ã¤ã—ã‹å–ã‚Œãªã‹ã£ãŸå ´åˆã¯ <br> ã§ã‚‚åˆ†å‰²
    if len(parts) <= 1:
        parts = re.split(r"<br\s*/?>", html, flags=re.IGNORECASE)
    # æœ€çµ‚çš„ã«ç©ºè¦ç´ ã‚’é™¤å»
    parts = [p.strip() for p in parts if p and p.strip()]
    # å…¨ãåˆ†å‰²ã§ããªã‘ã‚Œã°æœ¬æ–‡å…¨ä½“ã‚’1æ®µè½ã¨ã—ã¦è¿”ã™
    return parts or [html]

# ---- LLMãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆapplierå†…ã ã‘ã§å®Œçµï¼‰----
def _clean_gpt_output(text: str) -> str:
    text = re.sub(r"```(?:html)?", "", text or "")
    text = re.sub(r"```", "", text)
    text = text.replace("\u3000", " ")
    text = text.strip()
    # æ”¹è¡Œã¯1è¡Œã«æ½°ã™
    text = re.sub(r"\s*\n+\s*", " ", text)
    # å…ˆé ­æœ«å°¾ã®å¼•ç”¨ç¬¦ãƒ»é‰¤æ‹¬å¼§ç³»ã¯å‰¥ãŒã™
    text = re.sub(r'^[\'"ã€Œã€ï¼ˆ\(\[]\s*', "", text)
    text = re.sub(r'\s*[\'"ã€ã€ï¼‰\)\]]$', "", text)
    # ç¦æ­¢ãƒ»å†—é•·ãƒ•ãƒ¬ãƒ¼ã‚ºã®ç°¡æ˜“é™¤å»ï¼ˆé †åºå¤§äº‹ï¼šé•·ã„ã‚‚ã®â†’çŸ­ã„ã‚‚ã®ï¼‰
    STOP_PHRASES = [
        "ãœã²ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„",
        "ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„",
        "å‚è€ƒã«ã—ã¦ãã ã•ã„",
        "ãœã²ã”è¦§ãã ã•ã„",
        "ãœã²å‚è€ƒã«",
        "ãœã²ãƒã‚§ãƒƒã‚¯",
        "ãœã²ã¨ã‚‚",
        "ãœã²",
        # ä¸»èªå‘¼ã³ã‹ã‘ç³»ï¼ˆå¾Œç¶šã®ã€Œã¯ã€ã”ã¨é™¤å»ï¼‰
        "æ°—ã«ãªã‚‹æ–¹ã¯",
        "çŸ¥ã‚ŠãŸã„æ–¹ã¯",
        "è©³ã—ãçŸ¥ã‚ŠãŸã„æ–¹ã¯",
    ]
    for s in STOP_PHRASES:
        text = text.replace(s, "")
    # ã‚ˆãå‡ºã‚‹æ–‡æ³•å´©ã‚Œã®è£œæ­£
    text = re.sub(r"\s+", " ", text)              # é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹
    text = re.sub(r"(ã¯|ãŒ)\s*ã«ã¤ã„ã¦", "ã«ã¤ã„ã¦", text)  # ã€Œã€œã¯/ãŒ ã«ã¤ã„ã¦ã€â†’ã€Œã«ã¤ã„ã¦ã€
    text = re.sub(r"ã«\s*ã¤ã„ã¦", "ã«ã¤ã„ã¦", text)        # å…¨åŠè§’ã‚†ã‚‰ã
    text = re.sub(r"ã¯ã©ã†ãªã£ã¦ã„ã‚‹ã®ã‹", "ã®æ¦‚è¦", text)   # å†—é•·ç–‘å•å½¢ã®ç°¡ç´„
    # å¤šé‡ã‚¹ãƒšãƒ¼ã‚¹æ•´ç†
    text = re.sub(r"\s{2,}", " ", text)
    return text.strip()

def _iseo_tok(s: str) -> int:
    return int(len(s or "") / 1.8)

def _iseo_chat(msgs: List[Dict[str, str]], max_t: int, temp: float, user_id: Optional[int] = None) -> str:
    """
    å†…éƒ¨SEOç”¨ã®è»½ãƒ©ãƒƒãƒ‘ã€‚è¨˜äº‹ç”Ÿæˆã®æŒ™å‹•ã‚’ç°¡ç•¥åŒ–ã—ã¦è¸è¥²ã€‚
    TokenUsageLog è¨˜éŒ²ã¯ user_id ãŒç„¡ã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ã€‚
    """
    if _OPENAI_CLIENT is None:
        raise RuntimeError("OpenAI client is not available")
    used = sum(_iseo_tok(m.get("content", "")) for m in msgs)
    available = ISEO_CTX_LIMIT - used - 16
    max_t = max(1, min(max_t, available))

    def _call(m: int) -> str:
        res = _OPENAI_CLIENT.chat.completions.create(
            model=ISEO_ANCHOR_MODEL,
            messages=msgs,
            max_tokens=m,
            temperature=temp,
            top_p=ISEO_ANCHOR_TOP_P,
            timeout=60,
        )
        # TokenUsageLogï¼ˆä»»æ„ï¼‰
        try:
            if hasattr(res, "usage") and user_id:
                from app.models import TokenUsageLog
                usage = res.usage
                log = TokenUsageLog(
                    user_id=user_id,
                    prompt_tokens=getattr(usage, "prompt_tokens", 0),
                    completion_tokens=getattr(usage, "completion_tokens", 0),
                    total_tokens=getattr(usage, "total_tokens", 0),
                )
                db.session.add(log)
                db.session.commit()
        except Exception as _e:
            logger.warning(f"[ISEO TokenLog warn] { _e }")
        content = (res.choices[0].message.content or "").strip()
        return _clean_gpt_output(content)

    try:
        return _call(max_t)
    except _BadRequestError as e:
        if "max_tokens" in str(e):
            retry_t = max(1, int(max_t * ISEO_SHRINK))
            return _call(retry_t)
        raise

def _generate_anchor_text_via_llm(
    dst_title: str,
    dst_keywords: List[str] | Tuple[str, ...] | None,
    src_hint: str = "",
    user_id: Optional[int] = None,
) -> str:
    """
    LLMã§ã€ŒKWã‚’1ã€œ2èªå«ã‚€ãƒ»40ã€œ60å­—ãƒ»â€¦ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©ã€æ–‡ã‚’ç”Ÿæˆã€‚
    æœ€ä½é™ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã€è¦ä»¶ã‚’æº€ãŸã•ãªã‘ã‚Œã°ãƒ†ãƒ³ãƒ—ãƒ¬ã§è£œæ­£ã€‚
    """
    kw_csv = ", ".join([k for k in (dst_keywords or []) if k]) if isinstance(dst_keywords, (list, tuple)) else (dst_keywords or "")
    sys = ANCHOR_SYSTEM_PROMPT
    usr = ANCHOR_USER_PROMPT_TEMPLATE.format(
        dst_title=(dst_title or "")[:200],
        dst_keywords=(kw_csv or "")[:200],
        src_hint=(src_hint or "")[:200],
    )
    out = _iseo_chat(
        [{"role": "system", "content": sys}, {"role": "user", "content": usr}],
        ISEO_MAX_TOKENS,
        ISEO_ANCHOR_TEMPERATURE,
        user_id=user_id,
    )
    # æœ€çµ‚æ­£è¦åŒ–ï¼ˆ1è¡Œãƒ»è£…é£¾ãªã—ï¼‰
    out = _clean_gpt_output(out)
    # --- è¿½åŠ ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå³æ ¼ç‰ˆï¼‰ ---
    text = out
    # èªå°¾ã‚’å¼·åˆ¶æ•´å½¢ï¼šæœ«å°¾å¥ç‚¹ãƒ»èª­ç‚¹ãƒ»ç©ºç™½ã‚’é™¤å»
    text = re.sub(r"[ã€ã€‚ï¼.\s]+$", "", text)
    # è¦å®šã®çµ‚æ­¢å¥ã§çµ‚ã‚ã‚‰ã›ã‚‹ï¼ˆé‡è¤‡é˜²æ­¢ã®ãŸã‚ä¸€æ—¦å‰Šã‚‹ï¼‰
    text = re.sub(r"(ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©)$", r"\1", text)
    if not text.endswith("ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"):
        # æœ«å°¾ãŒåˆ¥è¡¨ç¾ãªã‚‰ç½®æ›
        text = re.sub(r"(ã«ã¤ã„ã¦.*)$", "ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©", text)
        if not text.endswith("ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"):
            # ã©ã†ã—ã¦ã‚‚æ•´ã‚ãªã„å ´åˆã¯ãƒ†ãƒ³ãƒ—ãƒ¬ã§ä½œã‚Šç›´ã—
            first_kw = ""
            if isinstance(dst_keywords, (list, tuple)) and dst_keywords:
                first_kw = str(dst_keywords[0]).strip()
            if not first_kw:
                try:
                    toks = title_tokens(dst_title or "") or []
                    first_kw = toks[0] if toks else ""
                except Exception:
                    first_kw = ""
            base = (first_kw or (dst_title or "")[:20]).strip()
            text = f"{base}ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"
    # æ–‡å­—æ•°ã‚’æœ€çµ‚èª¿æ•´ï¼ˆè¶…éã¯ä¸¸ã‚ã€æœ«å°¾ã®èª­ç‚¹é¡ã¯é™¤å»ï¼‰
    if len(text) > ISEO_ANCHOR_MAX_CHARS:
        text = text[:ISEO_ANCHOR_MAX_CHARS]
        text = re.sub(r"[ã€ã€‚ï¼.\s]+$", "", text)
        # è¶…éä¸¸ã‚ã§çµ‚æ­¢å¥ãŒæ¬ ã‘ãŸå ´åˆã¯ãƒ†ãƒ³ãƒ—ãƒ¬ã§å¾©å…ƒ
        if not text.endswith("ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"):
            first_kw = ""
            if isinstance(dst_keywords, (list, tuple)) and dst_keywords:
                first_kw = str(dst_keywords[0]).strip()
            if not first_kw:
                try:
                    toks = title_tokens(dst_title or "") or []
                    first_kw = toks[0] if toks else ""
                except Exception:
                    first_kw = ""
            text = f"{first_kw or (dst_title or '')[:20]}ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"
    # --- ã“ã“ã‹ã‚‰ è¿½åŠ ã®å³æ ¼ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ ---
    text = _clean_gpt_output(text)
    text = re.sub(r"[ã€ã€‚ï¼.\s]+$", "", text)

    # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å€™è£œï¼ˆdst_keywordså„ªå…ˆâ†’ç„¡ã‘ã‚Œã°ã‚¿ã‚¤ãƒˆãƒ«ä¸»è¦ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
    kw_candidates: List[str] = []
    if isinstance(dst_keywords, (list, tuple)):
        kw_candidates = [str(k).strip() for k in dst_keywords if str(k).strip()]
    if not kw_candidates:
        try:
            kw_candidates = [w for w in (title_tokens(dst_title or "") or []) if w][:6]
        except Exception:
            kw_candidates = []

    def _norm(s: str) -> str:
        return nfkc_norm((s or "").strip()).lower()

    ntext = _norm(text)
    nkeys = [_norm(k) for k in kw_candidates if _norm(k)]

    # 1) å‡ºåŠ›ã«ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒ1èªã‚‚å«ã¾ã‚Œãªã„ â†’ ãƒ†ãƒ³ãƒ—ãƒ¬ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    has_kw = any((nk in ntext) for nk in nkeys) if nkeys else False

    # 2) å†’é ­ã®å“è³ª: åŠ©è©ãƒ»é€£ç”¨å½¢ã ã‘ç­‰ã®å¼±ã„å§‹ã¾ã‚Šã‚’å¼¾ãï¼ˆã‚ˆãå‡ºã‚‹NGã®ç°¡æ˜“æ¤œçŸ¥ï¼‰
    BAD_START_PAT = re.compile(r"^(?:ã«ã¤ã„ã¦|ã«ã‚ˆã‚Š|ã«é–¢ã—ã¦|ã«ãŠã‘ã‚‹|ã«ã‚ˆã£ã¦|ã«å¯¾ã—ã¦|å‘ä¸Šã•ã›ã‚‹ãŸã‚|æˆåŠŸã•ã›ã‚‹ãŸã‚|é¸ã¶ãŸã‚|çŸ¥ã£ã¦ãŠãã¹ãã“ã¨|æ´»ç”¨æ³•|æ–¹æ³•|ãƒã‚¤ãƒ³ãƒˆ)")
    bad_start = bool(BAD_START_PAT.search(nfkc_norm(text)))

    if (not has_kw) or bad_start:
        first_kw = ""
        if nkeys:
            # ãªã‚‹ã¹ãé•·ã„èªã‚’å„ªå…ˆ
            nkeys_sorted = sorted(nkeys, key=len, reverse=True)
            first_kw = nkeys_sorted[0]
        if not first_kw:
            try:
                tks = [w for w in (title_tokens(dst_title or "") or []) if w]
                first_kw = _norm(tks[0]) if tks else ""
            except Exception:
                first_kw = ""
        base = next((k for k in (kw_candidates or []) if _norm(k) == first_kw), "") or (dst_title or "")[:20]
        text = f"{base}ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"
        text = re.sub(r"[ã€ã€‚ï¼.\s]+$", "", text)

    return text

def _postprocess_anchor_text(text: str) -> str:
    """
    ç”Ÿæˆå¾Œã®æ—¥æœ¬èªã‚’è»½ãæ•´å½¢:
      - ã€Œå¾—ã‚‰ã‚Œã¾ã™ã«ã¤ã„ã¦ã€â†’ã€Œã«ã¤ã„ã¦ã€
      - ã€Œç•™å­¦inã€‡ã€‡ã€â†’ã€Œã€‡ã€‡ç•™å­¦ã€
      - æœ«å°¾ã®å¥èª­ç‚¹ãƒ»ç©ºç™½é™¤å»
    """
    s = _clean_gpt_output(text)
    s = re.sub(r"å¾—ã‚‰ã‚Œã¾ã™ã«ã¤ã„ã¦", "ã«ã¤ã„ã¦", s)
    s = re.sub(r"å¾—ã‚‰ã‚Œã‚‹?ã«ã¤ã„ã¦", "ã«ã¤ã„ã¦", s)
    # ç•™å­¦inã‚¹ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ³ â†’ ã‚¹ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ³ç•™å­¦ï¼ˆä¸€èˆ¬åŒ–: ç•™å­¦inXXXX â†’ XXXXç•™å­¦ï¼‰
    s = re.sub(r"ç•™å­¦in([ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9ãƒ¼]+)", r"\1ç•™å­¦", s)
    s = re.sub(r"[ã€ã€‚ï¼.\s]+$", "", s)
    # çµ‚æ­¢å¥ã®çµ±ä¸€ï¼ˆå£Šã‚Œã¦ã„ãŸã‚‰ä»˜ã‘ç›´ã™ï¼‰
    if not s.endswith("ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"):
        s = re.sub(r"(ã«ã¤ã„ã¦.*)$", "ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©", s)
        if not s.endswith("ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"):
            s = s + "ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"
    return s

def _safe_anchor_from_keywords(dst_kw_list: List[str], dst_title: str) -> str:
    """
    NGæ™‚ã®ã‚»ãƒ¼ãƒ•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã«ã€ä¸»è¦KWã‹ã‚‰å®‰å…¨ãƒ†ãƒ³ãƒ—ãƒ¬ã‚’ç”Ÿæˆã€‚
    ãƒ»KWãŒç„¡ã„å ´åˆã¯ã‚¿ã‚¤ãƒˆãƒ«ä¸»è¦èª
    ãƒ»ã€Œç•™å­¦inâ—¯â—¯ã€â†’ã€Œâ—¯â—¯ç•™å­¦ã€ã«æ­£è¦åŒ–
    """
    base = ""
    pool = [k for k in (dst_kw_list or []) if k]
    if not pool:
        try:
            pool = [w for w in (title_tokens(dst_title or "") or []) if w]
        except Exception:
            pool = []
    if pool:
        # ã‚ˆã‚Šä¸€èˆ¬çš„ã§é•·ã‚ã®èªã‚’å„ªå…ˆ
        pool = sorted(set(pool), key=len, reverse=True)
        base = pool[0]
    base = re.sub(r"ç•™å­¦in([ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9ãƒ¼]+)", r"\\1ç•™å­¦", base or "")
    base = (base or (dst_title or "")[:20]).strip()
    return f"{base}ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"

def _is_anchor_quality_ok(text: str, dst_keywords: List[str], dst_title: str) -> bool:
    """æœ€ä½å“è³ªãƒã‚§ãƒƒã‚¯ï¼šåè©ç³»KWã‚’1èªä»¥ä¸Šå«ã‚€ï¼æ–‡é ­ãŒè¿°èªã ã‘ã«ãªã‚‰ãªã„ï¼é•·ã•"""
    if not text:
        return False
    # æœ«å°¾ã¯æ—¢ã«ã€Œã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©ã€ã§æ­£è¦åŒ–æ¸ˆã¿å‰æ
    body = re.sub(r"ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©$", "", text).strip()
    if not (24 <= len(text) <= ISEO_ANCHOR_MAX_CHARS):
        return False
    # å…ˆé ­ãŒåŠ©è©ãƒ»è£œåŠ©å‹•è©ãƒ»å‹•è©èªå¹¹ã£ã½ã„å§‹ã¾ã‚Šã¯NGï¼ˆç°¡æ˜“ï¼‰
    if re.match(r"^(ã«ã¤ã„ã¦|ã«ã‚ˆã‚Š|ã«å‘ã‘|ã®ãŸã‚|ãŸã‚ã«|å‘ä¸Šã•ã›ã‚‹ãŸã‚|æ”¹å–„ã™ã‚‹ãŸã‚|é¸ã¶ãŸã‚|çŸ¥ã‚‹ãŸã‚)", body):
        return False
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¿…ãš1èªä»¥ä¸Šå«ã‚€ï¼ˆtitle_tokensã‹ã‚‰ã‚‚è£œå®Œï¼‰
    kw_pool = set(k for k in (dst_keywords or []) if k)
    try:
        kw_pool.update(title_tokens(dst_title or "") or [])
    except Exception:
        pass
    # 2æ–‡å­—ä»¥ä¸Šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é™å®šã—ã¦å«æœ‰åˆ¤å®š
    kw_pool = {k for k in kw_pool if len(k) >= 2}
    if not kw_pool:
        return True  # ã©ã†ã—ã¦ã‚‚ç„¡ã„å ´åˆã¯é€šã™ï¼ˆä¸Šä½ã§ãƒ†ãƒ³ãƒ—ãƒ¬ã«è½ã¨ã™ãŸã‚ç·©ã‚ï¼‰
    normalized = body
    hit = any(k in normalized for k in kw_pool)
    return bool(hit)

def _emit_anchor_html(href: str, text: str) -> str:
    text_safe = _TAG_STRIP.sub(" ", unescape(text or "")).strip()
    # æ§‹é€ ã¯ç¶­æŒï¼ˆ<a href ... title ...>ï¼‰ã€‚ç‰ˆæƒ…å ±ã¯ç›´å‰ã‚³ãƒ¡ãƒ³ãƒˆã§è¡¨ç¾ã€‚
    return f'{INTERNAL_SEO_SPEC_MARK}<a href="{href}" title="{text_safe}">{text_safe}</a>'

def _emit_recommend_box() -> str:
    """
    ã€Œé–¢é€£ãƒ»æ³¨ç›®è¨˜äº‹ã€ãƒ©ãƒ™ãƒ«ï¼ˆFont Awesome ã®ãƒªã‚¹ãƒˆã‚¢ã‚¤ã‚³ãƒ³ä»˜ãï¼‰ã‚’å‡ºåŠ›ã€‚
    ãƒ»ãƒœãƒƒã‚¯ã‚¹ã® margin/padding/radius ã‚‚å«ã‚å…¨ä½“ã‚’ç´„1/2ã«ç¸®å°
    ãƒ»èƒŒæ™¯è‰²ã‚’ #6fba2cã€æ–‡å­—è‰²ã‚’ç™½ã«
    ãƒ»é»’ãƒ•ãƒã® border ã‚’å»ƒæ­¢
    ãƒ»ã‚¢ã‚¤ã‚³ãƒ³ï¼‹æ–‡å­—ã‚’æ¨ªä¸¦ã³ä¸­å¤®æƒãˆ
    ãƒ»æ–‡å­—ã‚µã‚¤ã‚ºã¯å¾“æ¥ã®åŠåˆ†ã«ç¸®å°
    """
    return (
        f'<p>{ILINK_BOX_MARK}</p>'
        '<div class="ai-relbox" '
        'style="margin:0.45em auto 0.15em; padding:2px 3px; border-radius:3px; '
        'background:#6fba2c !important; color:#fff !important; '
        'font-weight:400; display:inline-flex; align-items:center; gap:3px; '
        'font-size:0.85em;">'
        '<i class="fa-solid fa-list" aria-hidden="true" '
        'style="display:inline-block;"></i>'
        '<span>ğŸ“„é–¢é€£ãƒ»æ³¨ç›®è¨˜äº‹</span>'
        '</div>'
    )


def _rejoin_paragraphs(paragraphs: List[str]) -> str:
    return "</p>".join(paragraphs)

def _html_to_text(s: str) -> str:
    if not s:
        return ""
    return _TAG_STRIP.sub(" ", unescape(s)).strip()

def _is_internal_url(site_url: str, href: str) -> bool:
    if not href:
        return False
    return href.startswith(site_url.rstrip("/"))

def _extract_links(html: str) -> List[Tuple[str, str]]:
    return [(m.group(1) or "", _html_to_text(m.group(2) or "")) for m in _A_TAG.finditer(html or "")]

def _extract_anchor_text_set(html: str) -> set[str]:
    """è¨˜äº‹ä¸­ã«æ—¢ã«å­˜åœ¨ã™ã‚‹ <a>â€¦</a> ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ­£è¦åŒ–æ¸ˆã¿ï¼‰é›†åˆ"""
    out = set()
    for _, atext in _extract_links(html or ""):
        key = nfkc_norm((atext or "").strip()).lower()
        if key:
            out.add(key)
    return out

def _collect_internal_hrefs(site: Site, html: str) -> set[str]:
    """è¨˜äº‹å…¨ä½“ã®å†…éƒ¨ãƒªãƒ³ã‚¯ href ã‚»ãƒƒãƒˆï¼ˆè¨˜äº‹å˜ä½ã®é‡è¤‡æŠ‘æ­¢ã«ä½¿ç”¨ï¼‰"""
    site_prefix = (site.url or "").rstrip("/")
    hrefs = set()
    for h, _ in _extract_links(html or ""):
        if h and site_prefix and h.startswith(site_prefix):
            hrefs.add(h)
    return hrefs

_A_OPEN = re.compile(r"<a\b[^>]*>", re.IGNORECASE)
_A_CLOSE = re.compile(r"</a\s*>", re.IGNORECASE)

def _mask_existing_anchors(html: str) -> Tuple[str, Dict[str, str]]:
    """
    æ—¢å­˜<a> ... </a> ã‚’ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã«ç½®ãæ›ãˆã¦ä¿è­·ã™ã‚‹ã€‚
    ç½®æ›å¾Œãƒ†ã‚­ã‚¹ãƒˆã§ç”Ÿã®èªå¥ç½®æ›ã‚’è¡Œã£ã¦ã‚‚æ—¢å­˜ãƒªãƒ³ã‚¯ã‚’å£Šã•ãªã„ãŸã‚ã€‚
    """
    placeholders: Dict[str, str] = {}
    out = []
    i = 0
    while i < len(html):
        m = _A_OPEN.search(html, i)
        if not m:
            out.append(html[i:])
            break
        # ç›´å‰ã¾ã§ã‚’è¿½åŠ 
        out.append(html[i:m.start()])
        # å¯¾å¿œã™ã‚‹ </a> ã‚’æ¢ã™
        mclose = _A_CLOSE.search(html, m.end())
        if not mclose:
            # ç•°å¸¸ç³»ã€‚ä»¥å¾Œã¯ãã®ã¾ã¾
            out.append(html[m.start():])
            break
        seg = html[m.start(): mclose.end()]
        key = f"__A_PLACEHOLDER_{len(placeholders)}__"
        placeholders[key] = seg
        out.append(key)
        i = mclose.end()
    return "".join(out), placeholders

def _unmask_existing_anchors(html: str, placeholders: Dict[str, str]) -> str:
    for k, v in placeholders.items():
        html = html.replace(k, v)
    return html

def _linkify_first_occurrence(
    para_html: str,
    anchor_text: str,
    href: str,
    tgt_title: Optional[str] = None,
) -> Optional[str]:
    """
    æ®µè½å†…ã®**æœªãƒªãƒ³ã‚¯é ˜åŸŸ**ã«ã‚ã‚‹ anchor_text ã®æœ€åˆã®å‡ºç¾ã‚’
    Wikipediaé¢¨ã® <a href="..." class="ai-ilink" title="...">anchor_text</a>
    ã«ç½®æ›ã™ã‚‹ã€‚è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° Noneã€‚
    è¦‹å‡ºã—/TOC ã‚’å«ã‚€ãƒ–ãƒ­ãƒƒã‚¯ã§ã¯å®Ÿè¡Œã—ãªã„ã€‚
    """
    if not (para_html and anchor_text and href):
        return None
    # è¦‹å‡ºã—ãƒ»TOCãƒ»STYLE ãƒ–ãƒ­ãƒƒã‚¯ã¯ä¸€å¾‹é™¤å¤–ï¼ˆæœ¬æ–‡ä»¥å¤–ã¯è§¦ã‚‰ãªã„ï¼‰
    if _H_TAG.search(para_html) or _TOC_HINT.search(para_html) or _STYLE_BLOCK.search(para_html):
        return None
    # NGã‚¢ãƒ³ã‚«ãƒ¼ã¯å³ä¸­æ­¢ï¼ˆã‚¿ã‚¤ãƒˆãƒ«é–¢é€£æ€§ã‚‚è€ƒæ…®ï¼‰
    if is_ng_anchor(anchor_text, tgt_title):
        return None
    masked, ph = _mask_existing_anchors(para_html)
    # ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã§æœ€åˆã®ä¸€è‡´ã‚’æ¢ã™ï¼ˆHTMLã‚¿ã‚°ã¯æ®‹ã‚‹ãŒ <a> ã¯ãƒã‚¹ã‚¯æ¸ˆã¿ï¼‰
    idx = masked.find(anchor_text)
    if idx == -1:
        return None
    # ç°¡æ˜“â€œèªã®å¢ƒç•Œâ€ãƒã‚§ãƒƒã‚¯ï¼š
    # - æ—¢å­˜<a>ã¯ãƒã‚¹ã‚¯æ¸ˆã¿ãªã®ã§ã€ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆé€£çµã®ã¿åˆ¤å®š
    # - å‰å¾ŒãŒã€Œèªæ–‡å­—ã€ã§ã‚‚ã€æ—¥æœ¬èªã®åŠ©è©ãªã‚‰â€œæŸ”ã‚‰ã‹ã„å¢ƒç•Œâ€ã¨ã—ã¦è¨±å®¹ã™ã‚‹
    def _is_word_char(ch: str) -> bool:
        return bool(re.match(r"[A-Za-z0-9ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³ãƒ¼]", ch))
    SOFT_BOUNDARIES = set(list("ã§ã‚’ã«ãŒã¯ã¨ã‚‚ã¸ã‚„ã®"))  # æ—¥æœ¬èªã®ä¸»è¦åŠ©è©
    before = masked[idx - 1] if idx > 0 else ""
    after  = masked[idx + len(anchor_text)] if (idx + len(anchor_text)) < len(masked) else ""
    # â€œä¸¡å´ãŒèªæ–‡å­—ã‹ã¤åŠ©è©ã§ã‚‚ãªã„â€ã¨ãã ã‘ä¸è‡ªç„¶ã¨ã—ã¦æ‹’å¦
    if (before and _is_word_char(before) and before not in SOFT_BOUNDARIES) and \
       (after  and _is_word_char(after)  and after  not in SOFT_BOUNDARIES):
        return None
    # Wikipediaé¢¨ï¼šhref + titleï¼ˆclass/style ãªã—ï¼‰ã€‚ç‰ˆæƒ…å ±ã¯ç›´å‰ã‚³ãƒ¡ãƒ³ãƒˆã§è¡¨ç¾ã€‚
    linked = f'{INTERNAL_SEO_SPEC_MARK}<a href="{href}" title="{anchor_text}">{anchor_text}</a>'
    masked = masked.replace(anchor_text, linked, 1)
    return _unmask_existing_anchors(masked, ph)



def _ensure_inline_underline_style(site: Site, html: str) -> str:
    """
    ãƒ†ãƒ¼ãƒCSSã‚’è§¦ã‚‰ãšã€ãƒªãƒ³ã‚¯è¦ç´ ã«ã‚‚ style ã‚’æ›¸ã‹ãšã«ä¸‹ç·šã¨è‰²ã‚’åŠ¹ã‹ã›ã‚‹ãŸã‚ã€
    è¨˜äº‹å…ˆé ­ã« 1åº¦ã ã‘æœ€å°ã® <style> ã‚’æŒ¿å…¥ã™ã‚‹ã€‚
      å¯¾è±¡: ã‚µã‚¤ãƒˆå†…URLã¸å‘ã a è¦ç´ ï¼ˆWikipedia ã¨åŒã˜ãã€Œå†…éƒ¨ãƒªãƒ³ã‚¯ã¯ä¸‹ç·šï¼‹é’ã€ï¼‰
    """
    if not html:
        return html
    
    # â˜…è¿½åŠ ï¼šç’°å¢ƒå¤‰æ•°ã§æ³¨å…¥ã‚’ç„¡åŠ¹åŒ–ï¼ˆWPãŒ<style>ã‚’å‰¥ãŒã™å ´åˆã«æœ‰åŠ¹ï¼‰
    import os
    if os.getenv("INTERNAL_SEO_EMBED_STYLE", "1") == "0":
        return html
    # ------- æ—¢å­˜ã®â€œä¸‹ç·šç”¨ã‚¹ã‚¿ã‚¤ãƒ«â€é–¢é€£ã‚¬ãƒ™ãƒ¼ã‚¸ã‚’å¾¹åº•é™¤å» -------
    site_url = site.url.rstrip("/")
    site_url_re = re.escape(site_url)

    # â‘  <p><!-- â€¦ --></p> ã®ç›´å¾Œã« <style>â€¦</style> ãŒç¶šãï¼ˆWP ãŒã‚³ãƒ¡ãƒ³ãƒˆã‚’ <p> ã«åŒ…ã‚€ã‚±ãƒ¼ã‚¹ï¼‰
    html = re.sub(
        r'<p>\s*<!--\s*ai-internal-link-style:v\d+\s*-->\s*</p>\s*<style\b[^>]*>.*?</style\s*>',
        '',
        html, flags=re.IGNORECASE | re.DOTALL
    )
    # â‘¡ ã‚³ãƒ¡ãƒ³ãƒˆç›´å¾Œã« <style> ãŒç¶šãç´ ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    html = re.sub(
        r'<!--\s*ai-internal-link-style:v\d+\s*-->\s*<style\b[^>]*>.*?</style\s*>',
        '',
        html, flags=re.IGNORECASE | re.DOTALL
    )
    # â‘¢ ã‚³ãƒ¡ãƒ³ãƒˆå˜ä½“ï¼ˆã©ã“ã«ã‚ã£ã¦ã‚‚ï¼‰ã‚’é™¤å»
    html = re.sub(
        r'<!--\s*ai-internal-link-style:v\d+\s*-->',
        '',
        html, flags=re.IGNORECASE
    )
    # â‘£ ã‚³ãƒ¡ãƒ³ãƒˆã ã‘ã‚’å«ã‚€ <p> ã‚‚é™¤å»ï¼ˆç©ºç™½è¡Œã®ç™ºç”Ÿã‚’é˜²ãï¼‰
    html = re.sub(
        r'<p>\s*</p>',
        '',
        html, flags=re.IGNORECASE | re.DOTALL
    )
    # â‘¤ ç©ºã® <style></style> ã‚’å…¨åŸŸã§é™¤å»
    html = re.sub(
        r'<style\b[^>]*>\s*</style\s*>',
        '',
        html, flags=re.IGNORECASE | re.DOTALL
    )
    # â‘¥ ãƒãƒ¼ã‚«ãƒ¼ãŒå‰¥ãŒã‚Œâ€œä¸­èº«ãŒæˆ‘ã€…ã®CSSâ€ãª <style> ã‚’ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã§é™¤å»
    #    æ¡ä»¶: a[href^="{site_url}"] ã‚’å«ã¿ã€0645ad ã‚’å«ã¿ã€.entry-content ç­‰ã®æœ¬æ–‡ã‚»ãƒ¬ã‚¯ã‚¿ã‚’å«ã‚€
    pattern_ours = rf'<style\b[^>]*>(?:(?:(?!</style).))*:where\([^)]*\)\s*a\[href\^\="{site_url_re}"\][^<]*0645ad(?:(?:(?!</style).))*</style\s*>'
    html = re.sub(pattern_ours, '', html, flags=re.IGNORECASE | re.DOTALL)
    # â‘¦ å…ˆé ­ã«æºœã¾ã£ãŸç©ºè¦ç´ ï¼ˆç©º<p>/ç©º<style>ï¼‰ã®æŸã‚’ã¾ã¨ã‚ã¦å‰¥ãŒã™
    html = re.sub(
        r'^(?:\s*(?:<p>\s*</p>|<style\b[^>]*>\s*</style\s*>))+',
        '',
        html, flags=re.IGNORECASE | re.DOTALL
    )
    

    css = f'''{_AI_STYLE_MARK}<style>
/* æœ¬æ–‡ã«é™å®šï¼šå†…éƒ¨ãƒªãƒ³ã‚¯ã¯ä¸‹ç·šï¼‹é’(#0645ad)ï¼ˆãƒ†ãƒ¼ãƒã«å‹ã¦ã‚‹ã‚ˆã† !importantï¼‰ */
:where(.ai-content,.entry-content,.post-content,article,.content) a[href^="{site_url}"] {{
  text-decoration: underline !important;
  color: #0645ad !important;
}}
/* è¦‹å‡ºã—ã¯é™¤å¤–ï¼ˆä¸‹ç·š/è‰²ã¨ã‚‚ç¶™æ‰¿ã§ä¸Šæ›¸ãï¼‰ */
:where(.ai-content,.entry-content,.post-content,article,.content) h1 a[href^="{site_url}"],
:where(.ai-content,.entry-content,.post-content,article,.content) h2 a[href^="{site_url}"],
:where(.ai-content,.entry-content,.post-content,article,.content) h3 a[href^="{site_url}"],
:where(.ai-content,.entry-content,.post-content,article,.content) h4 a[href^="{site_url}"],
:where(.ai-content,.entry-content,.post-content,article,.content) h5 a[href^="{site_url}"],
:where(.ai-content,.entry-content,.post-content,article,.content) h6 a[href^="{site_url}"] {{
  text-decoration: none !important;
  color: inherit !important;
}}
/* ä»£è¡¨çš„ãª TOC ã‚’é™¤å¤–ï¼ˆez-toc / #toc / .toctitleï¼‰ */
.toctitle a, .toc a, #toc a, .ez-toc a {{
  text-decoration: none !important;
  color: inherit !important;
}}
</style>'''
    return css + html


def _normalize_existing_internal_links(html: str) -> str:
    """
    æ—¢å­˜ã® ai-ilink / inline-style ã‚’ Wikipedia é¢¨ã«æ­£è¦åŒ–:
      <a href="..." class="ai-ilink" style="text-decoration:underline;">TEXT</a>
        â†’ <a href="..." title="TEXT">TEXT</a>
    """
    if not html:
        return html
    # a ã‚¿ã‚°ã‚’ã™ã¹ã¦åˆ—æŒ™ã—ã¦é‡è¤‡ã‚’æ•´ç†
    pat = re.compile(
        r'<a\b([^>]*)\bhref=["\']([^"\']+)["\']([^>]*)>(.*?)</a\s*>',
        re.IGNORECASE | re.DOTALL
    )
    seen_hrefs: set[str] = set()
    def _repl(m: re.Match) -> str:
        attrs_all = (m.group(1) or "") + (m.group(3) or "")
        href      = (m.group(2) or "").strip()
        inner     = m.group(4) or ""
        attrs_lc  = attrs_all.lower()
        # æ—¢ã«åŒã˜ href ãŒç™»å ´ã—ã¦ã„ãŸã‚‰å‰Šé™¤ï¼ˆç©ºæ–‡å­—è¿”ã™ï¼‰
        if href in seen_hrefs:
            return ""
        seen_hrefs.add(href)
        # æ­£è¦åŒ–ï¼šai-ilinkã‚„styleä»˜ãã¯å‰Šé™¤ã—ã¦ Wikipedia é¢¨ã«
        if ("ai-ilink" in attrs_lc) or ("style=" in attrs_lc):
            text = _TAG_STRIP.sub(" ", unescape(inner)).strip()
            return f'<a href="{href}" title="{text}">{inner}</a>'
        return m.group(0)
    return pat.sub(_repl, html)

def _add_attrs_to_first_anchor_with_href(html: str, href: str) -> str:
    """
    swap ãªã©ã§ href ã‚’å·®ã—æ›¿ãˆãŸæœ€åˆã® <a ... href="href"> ã‚’ Wikipedia é¢¨ã«æ­£è¦åŒ–ã™ã‚‹ã€‚
    - class/style ã‚’é™¤å»
    - title ãŒç„¡ã‘ã‚Œã°ç©ºã§ä»˜ä¸ï¼ˆå¾Œç¶šã®æ­£è¦åŒ–ã§æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã«åŒæœŸï¼‰
    """
    if not html or not href:
        return html
    pat = re.compile(
        rf'(<a\b[^>]*href=["\']{re.escape(href)}["\'][^>]*)(>)',
        re.IGNORECASE
    )
    def _repl(m):
        start, end = m.group(1), m.group(2)
        # class/style ã‚’ä¸¸ã”ã¨é™¤å»
        start = re.sub(r'\sclass=["\'][^"\']*["\']', '', start, flags=re.IGNORECASE)
        start = re.sub(r'\sstyle=["\'][^"\']*["\']', '', start, flags=re.IGNORECASE)
        # title ãŒç„¡ã‘ã‚Œã°è¿½åŠ 
        if not re.search(r'\btitle=["\']', start, re.IGNORECASE):
            start += ' title=""'
            # data-iseo ã¯ä½¿ç”¨ã—ãªã„ï¼ˆå±æ€§ã¯è¿½åŠ ã—ãªã„ï¼‰ã€‚ã‚³ãƒ¡ãƒ³ãƒˆãƒãƒ¼ã‚¯ã¯åˆ¥ã§æ‰±ã†ã€‚
        return start + end
    # æœ€åˆã®1ä»¶ã ã‘æ³¨å…¥
    return pat.sub(_repl, html, count=1)

# ---- ãƒ‡ãƒ¼ã‚¿å–å¾— ----

def _post_url(site_id: int, wp_post_id: int) -> Optional[str]:
    row = (
        ContentIndex.query
        .with_entities(ContentIndex.url)
        .filter_by(site_id=site_id, wp_post_id=wp_post_id)
        .one_or_none()
    )
    return row[0] if row else None

def _action_targets_meta(site_id: int, actions: List[InternalLinkAction]) -> Dict[int, Tuple[str, str]]:
    need_ids = list({a.target_post_id for a in actions})
    if not need_ids:
        return {}
    rows = (
        ContentIndex.query
        .with_entities(ContentIndex.wp_post_id, ContentIndex.url, ContentIndex.title)
        .filter(ContentIndex.site_id == site_id)
        .filter(ContentIndex.wp_post_id.in_(need_ids))
        .all()
    )
    # return: {pid: (url, title)}
    return {int(pid): ((url or ""), (title or "")) for (pid, url, title) in rows}

def _existing_internal_links_count(site: Site, html: str) -> int:
    site_url = site.url.rstrip("/")
    links = _extract_links(html)
    return sum(1 for (href, _) in links if _is_internal_url(site_url, href))


def _all_url_to_title_map(site_id: int) -> Dict[str, str]:
    """
    ã‚µã‚¤ãƒˆå†…ã™ã¹ã¦ã®å…¬é–‹è¨˜äº‹ã«ã¤ã„ã¦ URLâ†’ã‚¿ã‚¤ãƒˆãƒ« ã®è¾æ›¸ã‚’è¿”ã™ã€‚
    æ—§ä»•æ§˜å‰Šé™¤åˆ¤å®šã«åˆ©ç”¨ã€‚
    """
    rows = (
        ContentIndex.query
        .with_entities(ContentIndex.url, ContentIndex.title)
        .filter_by(site_id=site_id, status="publish")
        .filter(ContentIndex.wp_post_id.isnot(None))
        .all()
    )
    return { (u or ""): (t or "") for (u, t) in rows if u }

def _all_url_to_pid_map(site_id: int) -> Dict[str, int]:
    """
    ã‚µã‚¤ãƒˆå†…ã™ã¹ã¦ã®å…¬é–‹è¨˜äº‹ã«ã¤ã„ã¦ URLâ†’wp_post_id ã®è¾æ›¸ã‚’è¿”ã™ã€‚
    æ—§ä»•æ§˜å‰Šé™¤ãƒ­ã‚°ã§ target_post_id ã‚’ç´ã¥ã‘ã‚‹ç”¨é€”ã€‚
    """
    rows = (
        ContentIndex.query
        .with_entities(ContentIndex.url, ContentIndex.wp_post_id)
        .filter_by(site_id=site_id, status="publish")
        .filter(ContentIndex.wp_post_id.isnot(None))
        .all()
    )
    out: Dict[str, int] = {}
    for (u, pid) in rows:
        if u and pid is not None:
            out[u] = int(pid)
    return out


# ---- å·®åˆ†ä½œæˆ ----

@dataclass
class ApplyResult:
    applied: int = 0
    swapped: int = 0
    skipped: int = 0
    legacy_deleted: int = 0
    message: str = ""

@dataclass
class PreviewItem:
    position: str
    anchor_text: str
    target_post_id: int
    target_url: str
    paragraph_index: int
    paragraph_excerpt_before: str
    paragraph_excerpt_after: str

def preview_apply_for_post(site_id: int, src_post_id: int) -> Tuple[str, ApplyResult, List[PreviewItem]]:
    """
    **å‰¯ä½œç”¨ãªã—**ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€‚
    - pending actions ã‚’èª­ã¿è¾¼ã¿ã€_apply_plan_to_html ã‚’ä½¿ã£ã¦â€œä»®é©ç”¨â€ã—ãŸHTMLã‚’ä½œã‚‹
    - DBæ›´æ–°ã‚‚WPæ›´æ–°ã‚‚è¡Œã‚ãªã„
    - ã©ã®ã‚¢ãƒ³ã‚«ãƒ¼ãŒæ¡ç”¨ã•ã‚ŒãŸã‹ï¼ˆä½ç½®ãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»URLãƒ»å‰å¾ŒæŠœç²‹ï¼‰ã‚’è¿”ã™
    """
    cfg = InternalSeoConfig.query.filter_by(site_id=site_id).one_or_none()
    if not cfg:
        cfg = InternalSeoConfig(site_id=site_id)
        db.session.add(cfg)
        db.session.commit()

    site = db.session.get(Site, site_id)
    # â–¼ topic ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§ã‚‚è§¦ã‚‰ãªã„ï¼‰
    try:
        if os.getenv("INTERNAL_SEO_SKIP_TOPIC", "1") != "0":
            src_url_row = (
                ContentIndex.query
                .with_entities(ContentIndex.url)
                .filter_by(site_id=site_id, wp_post_id=src_post_id)
                .one_or_none()
            )
            src_url = (src_url_row[0] or "") if src_url_row else ""
            if is_topic_url(src_url):
                return "", ApplyResult(message="skip-topic-page"), []
    except Exception:
        pass
    wp_post = fetch_single_post(site, src_post_id)
    if not wp_post:
        return "", ApplyResult(message="fetch-failed-or-excluded"), []
    
    # 1) æ—§ä»•æ§˜å‰Šé™¤ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼šDBã¯è§¦ã‚‰ãªã„ï¼‰â€” æ–°ã‚·ã‚°ãƒãƒãƒ£å„ªå…ˆã€æœªå¯¾å¿œç’°å¢ƒã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    url_title_map = _all_url_to_title_map(site_id)
    try:
        cleaned_html, deletions = find_and_remove_legacy_links(
            wp_post.content_html or "", url_title_map, spec_version=INTERNAL_SEO_SPEC_VERSION
        )
    except TypeError:
        # cleaner æœªæ›´æ–°ç’°å¢ƒã§ã‚‚å‹•ãã‚ˆã†å¾Œæ–¹äº’æ›
        cleaned_html, deletions = find_and_remove_legacy_links(wp_post.content_html or "", url_title_map)

    # 2) æ–°ä»•æ§˜ã® pending ã‚’å–å¾—
    actions = (
        InternalLinkAction.query
        .filter_by(site_id=site_id, post_id=src_post_id, status="pending")
        .order_by(InternalLinkAction.created_at.asc())
        .all()
    )

    meta_map = _action_targets_meta(site_id, actions)

    original_paras = _split_paragraphs(wp_post.content_html or "")
    # 3) æ—§ä»•æ§˜ã‚’é™¤å»ã—ãŸæœ¬æ–‡ã‚’ãƒ™ãƒ¼ã‚¹ã«æ–°ä»•æ§˜ã‚’ä»®é©ç”¨
    base_html = cleaned_html if cleaned_html is not None else (wp_post.content_html or "")
    new_html, res = _apply_plan_to_html(site, src_post_id, base_html, actions, cfg, meta_map)
    res.legacy_deleted = len(deletions or [])
    new_paras = _split_paragraphs(new_html)

    previews: List[PreviewItem] = []
    # _apply_plan_to_html å†…ã§ in-memory ã« a.status="applied" ã‚’ç«‹ã¦ã‚‹ãŒã€ã“ã“ã§ã¯commitã—ãªã„
    for a in actions:
        if a.status == "applied":
            try:
                pidx = int(a.position.split(":")[1]) if a.position and a.position.startswith("p:") else -1
            except Exception:
                pidx = -1
            before_snip = _html_to_text(original_paras[pidx])[:120] if (0 <= pidx < len(original_paras)) else ""
            after_snip  = _html_to_text(new_paras[pidx])[:120] if (0 <= pidx < len(new_paras)) else ""
            # meta ã¯ä¸€åº¦ã ã‘å–å¾—
            meta_obj = getattr(a, "meta", None)
            meta_dict = meta_obj if isinstance(meta_obj, dict) else {}
            _dst_url_from_meta = (meta_dict.get("dst_url") or "").strip()
            previews.append(PreviewItem(
                position=a.position or "",
                anchor_text=a.anchor_text or "",
                target_post_id=int(a.target_post_id),
                target_url=_dst_url_from_meta or (meta_map.get(a.target_post_id, ("",""))[0]) or "",
                paragraph_index=pidx,
                paragraph_excerpt_before=before_snip,
                paragraph_excerpt_after=after_snip,
            ))
    # pending ãŒç„¡ãã¦ã‚‚ã€æ—§ä»•æ§˜å‰Šé™¤ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯è¿”ã™ï¼ˆmessage ã‚’èª¿æ•´ï¼‰
    if not actions and res.legacy_deleted > 0:
        res.message = "legacy-clean-only"
    elif not actions:
        res.message = "no-pending"
    return new_html, res, previews   

def _apply_plan_to_html(
    site: Site,
    src_post_id: int,
    html: str,
    actions: List[InternalLinkAction],
    cfg: InternalSeoConfig,
    target_meta_map: Dict[int, Tuple[str, str]],  # pid -> (url, title)
) -> Tuple[str, ApplyResult]:
    """
    è¨ˆç”»ï¼ˆplan/swap_candidateï¼‰ã®ã†ã¡ã€æœ¬æ–‡å†…ã®æŒ¿å…¥ã¨ç½®æ›ã‚’è¡Œã†ã€‚
    - æœ¬æ–‡å†…ã®å†…éƒ¨ãƒªãƒ³ã‚¯ç·æ•°ãŒ min~max ã«åã¾ã‚‹ã‚ˆã†èª¿æ•´
    """
    res = ApplyResult()
    if not html:
        res.message = "empty-html"
        return html, res

    # ä»¥é™ã€H2æœ«å°¾ã«ç›´æ¥æŒ¿å…¥ã™ã‚‹ãŸã‚ã€æ®µè½åˆ†å‰²ã¯å¾Œæ®µï¼ˆswapç­‰ï¼‰ã§éƒ½åº¦å†è¨ˆç®—ã™ã‚‹
    paragraphs = _split_paragraphs(html) or [html]

    site_url = site.url.rstrip("/")
    # ä½œæ¥­ç”¨ã®æœ¬æ–‡
    html_work = html
    # æ—¢å­˜å†…éƒ¨ãƒªãƒ³ã‚¯ã®å€‹æ•°
    existing_internal = _existing_internal_links_count(site, html_work)
    # è¨˜äº‹å…¨ä½“ã§æ—¢ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹å†…éƒ¨hrefï¼ˆé‡è¤‡æŠ‘æ­¢ç”¨ï¼‰
    article_href_set = _collect_internal_hrefs(site, html_work)
    # è¨˜äº‹å…¨ä½“ã§æ—¢ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼ˆé‡è¤‡æŠ‘æ­¢ç”¨ï¼‰
    existing_anchor_text_set = _extract_anchor_text_set(html_work)
    # åŒä¸€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆPIDã¯è¨˜äº‹å†…ã§1å›ã¾ã§
    used_target_pids: set[int] = set()

    # æ—¢å®šã‚’ 2ã€œ4 ã«å¤‰æ›´ï¼ˆã‚µã‚¤ãƒˆè¨­å®šãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆï¼‰
    need_min = max(2, int(cfg.min_links_per_post or 2))
    need_max = min(4, int(cfg.max_links_per_post or 4))

    # 1) ã¾ãšã¯ reason='plan' ã‚’å„ªå…ˆã—ã¦æŒ¿å…¥
    def _reason_prefix(a):
        return (a.reason or "").split(":", 1)[0]
    plan_actions = [a for a in actions if _reason_prefix(a) in ("plan", "review_approved")]
    swaps = [a for a in actions if _reason_prefix(a) == "swap_candidate"]

    inserted = 0
    # è¨˜äº‹å†…ã§åŒä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã¯1å›ã¾ã§
    seen_anchor_keys = set()
    # --- æ–°ä»•æ§˜: H2æœ«å°¾ã«æŒ¿å…¥ã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†é›¢ ---
    h2_actions = [a for a in plan_actions if (a.position or "").startswith("h2:")]
    p_actions  = [a for a in plan_actions if (a.position or "").startswith("p:")]

    # 1-A) H2æœ«å°¾ã¸ã®æŒ¿å…¥ï¼ˆåŒã˜è¨˜äº‹å†…ã§ã®ä½ç½®ãšã‚Œã‚’é¿ã‘ã‚‹ãŸã‚ â€œæœ«å°¾åº§æ¨™ã®é™é †â€ ã§å‡¦ç†ï¼‰
    if h2_actions:
        # ç¾åœ¨ã®æœ¬æ–‡ã‹ã‚‰ H2 ã‚»ã‚¯ã‚·ãƒ§ãƒ³åº§æ¨™ã‚’å–å¾—
        sections = extract_h2_sections(html_work)
        # (tail_pos, act, h2_idx) ã‚’ä½œã‚‹ï¼ˆ-1 ã¯æœ¬æ–‡æœ«å°¾ï¼‰
        h2_plan: List[Tuple[int, InternalLinkAction, int]] = []
        for act in h2_actions:
            try:
                h2_idx = int((act.position or "h2:-1").split(":")[1])
            except Exception:
                h2_idx = -1
            if h2_idx >= 0 and sections and 0 <= h2_idx < len(sections):
                tail_pos = int(sections[h2_idx]["tail_insert_pos"])
            else:
                tail_pos = len(html_work)
                h2_idx = -1
            h2_plan.append((tail_pos, act, h2_idx))
        # æœ«å°¾ã‹ã‚‰å‡¦ç†ï¼ˆæŒ¿å…¥ã«ã‚ˆã‚‹ä»¥å¾Œä½ç½®ã®ã‚·ãƒ•ãƒˆã‚’å›é¿ï¼‰
        h2_plan.sort(key=lambda x: x[0], reverse=True)

        for tail_pos, act, h2_idx in h2_plan:
            if existing_internal + inserted >= need_max:
                break
            # --- metaå„ªå…ˆã§ãƒªãƒ³ã‚¯å…ˆæƒ…å ±ã‚’å–å¾— ---
            meta_obj = getattr(act, "meta", None)
            _meta: dict = meta_obj if isinstance(meta_obj, dict) else {}
            href0, tgt_title0 = target_meta_map.get(act.target_post_id, ("", ""))
            href_meta  = (_meta.get("dst_url") or "").strip()
            title_meta = (_meta.get("dst_title") or "").strip()
            kw_meta = _meta.get("dst_keywords")
            if isinstance(kw_meta, str):
                kw_meta_list = [k.strip() for k in kw_meta.split(",") if k.strip()]
            elif isinstance(kw_meta, (list, tuple)):
                kw_meta_list = [str(k).strip() for k in kw_meta if str(k).strip()]
            else:
                kw_meta_list = []
            href = href_meta or href0
            tgt_title = title_meta or tgt_title0
            if not href:
                act.status = "skipped"
                act.updated_at = datetime.now(UTC)
                res.skipped += 1
                continue
            # è‡ªå·±ãƒªãƒ³ã‚¯ç¦æ­¢
            try:
                if int(act.target_post_id) == int(src_post_id):
                    act.status = "skipped"
                    act.reason = "self-link"
                    act.updated_at = datetime.now(UTC)
                    res.skipped += 1
                    continue
            except Exception:
                pass
            # åŒä¸€URLé‡è¤‡ãƒ»åŒä¸€PIDé‡è¤‡
            if href in article_href_set:
                act.status = "skipped"; act.reason = "duplicate-href-in-article"
                act.updated_at = datetime.now(UTC); res.skipped += 1; continue
            try:
                if int(act.target_post_id) in used_target_pids:
                    act.status = "skipped"; act.reason = "duplicate-target-in-article"
                    act.updated_at = datetime.now(UTC); res.skipped += 1; continue
            except Exception:
                pass

            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³æœ¬æ–‡æœ«å°¾è¿‘å‚ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ’ãƒ³ãƒˆã« LLM ç”Ÿæˆ
            if h2_idx >= 0 and sections and 0 <= h2_idx < len(sections):
                s = sections[h2_idx]
                ctx = _html_to_text(html_work[s["h2_end"]:s["tail_insert_pos"]])[:120]
            else:
                ctx = _html_to_text(html_work[-4000:])[:120]
            try:
                dst_kw_list = kw_meta_list or [w for w in (title_tokens(tgt_title or "") or []) if w][:6]
            except Exception:
                dst_kw_list = kw_meta_list or []
            try:
                if ANCHOR_STYLE == "template":
                    base = (dst_kw_list[0] if dst_kw_list else (tgt_title or "")[:20]).strip()
                    anchor_text = f"{base}ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"
                else:
                    anchor_text = _generate_anchor_text_via_llm(
                        dst_title=tgt_title or "", dst_keywords=dst_kw_list, src_hint=ctx, user_id=None
                    )
                anchor_text = _postprocess_anchor_text(anchor_text)
            except Exception as e:
                logger.warning(f"[GEN-ANCHOR:H2] LLM failed: {e}")
                key = (tgt_title or "").strip()
                anchor_text = (f"{key}ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©")[:58] if key else "å†…éƒ¨ãƒªãƒ³ã‚¯ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"

            # æœ€ä½å“è³ª/NGãƒã‚§ãƒƒã‚¯ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if _is_ng_anchor_generated_line(anchor_text, tgt_title):
                fb = _postprocess_anchor_text(_safe_anchor_from_keywords(dst_kw_list, tgt_title or ""))
                if len(fb) > ISEO_ANCHOR_MAX_CHARS:
                    fb = re.sub(r"[ã€ã€‚ï¼.\s]+$", "", fb[:ISEO_ANCHOR_MAX_CHARS])
                    if not fb.endswith("ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"):
                        fb = _safe_anchor_from_keywords(dst_kw_list, tgt_title or "")
                        fb = _postprocess_anchor_text(fb)
                if _is_ng_anchor_generated_line(fb, tgt_title):
                    act.status = "skipped"; act.reason = "ng-anchor"
                    act.updated_at = datetime.now(UTC); res.skipped += 1; continue
                anchor_text = fb

            # ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆé‡è¤‡æŠ‘æ­¢
            anchor_key = nfkc_norm(anchor_text).lower()
            if anchor_key and (anchor_key in seen_anchor_keys or anchor_key in existing_anchor_text_set):
                act.status = "skipped"; act.reason = "duplicate-anchor-in-article"
                act.updated_at = datetime.now(UTC); res.skipped += 1; continue

            # ç”ŸæˆHTMLï¼ˆå›²ã¿ãƒœãƒƒã‚¯ã‚¹ â†’ æ”¹è¡Œ â†’ <a>ï¼‰
            box_html = _emit_recommend_box()
            a_html   = _emit_anchor_html(href, anchor_text)
            insert_html = box_html + "<br>" + a_html

            # å®ŸæŒ¿å…¥
            html_work = html_work[:tail_pos] + insert_html + html_work[tail_pos:]
            # è¿½è·¡ã‚»ãƒƒãƒˆæ›´æ–°
            article_href_set.add(href)
            if anchor_key:
                seen_anchor_keys.add(anchor_key)
                existing_anchor_text_set.add(anchor_key)
            try:
                used_target_pids.add(int(act.target_post_id))
            except Exception:
                pass
            # çŠ¶æ…‹æ›´æ–°
            act.anchor_text = anchor_text
            act.status = "applied"
            act.applied_at = datetime.now(UTC)
            res.applied += 1
            inserted += 1

            # æ¬¡ã®H2ä½ç½®è¨ˆç®—ãŒãšã‚Œãªã„ã‚ˆã†ã€å¿…è¦ãªã‚‰å†æŠ½å‡º
            sections = extract_h2_sections(html_work)

    # 1-B) æ—§äº’æ›ï¼šp:{idx} æŒ‡å®šãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã¯å¾“æ¥ã©ãŠã‚Šæ®µè½æœ«ã« <br><a> ã§è¿½åŠ 
    paragraphs = _split_paragraphs(html_work) or [html_work]
    for act in p_actions:
        if existing_internal + inserted >= need_max:
            break
        # ä½ç½®æŒ‡å®š 'p:{idx}'
        try:
            if not act.position.startswith("p:"):
                res.skipped += 1
                continue
            idx = int(act.position.split(":")[1])
        except Exception:
            res.skipped += 1
            continue
        if idx < 0 or idx >= len(paragraphs):
            res.skipped += 1
            continue
        # --- metaå„ªå…ˆã§ãƒªãƒ³ã‚¯å…ˆæƒ…å ±ã‚’å–å¾—ï¼ˆfallbackã¯ContentIndexç”±æ¥ã®target_meta_mapï¼‰ ---
        # meta ã¯ä¸€åº¦ã ã‘å–å¾—
        meta_obj = getattr(act, "meta", None)
        _meta: dict = meta_obj if isinstance(meta_obj, dict) else {}
        href0, tgt_title0 = target_meta_map.get(act.target_post_id, ("", ""))
        href_meta = (_meta.get("dst_url") or "").strip()
        title_meta = (_meta.get("dst_title") or "").strip()
        # dst_keywords ã¯ list/tuple/str ã„ãšã‚Œã«ã‚‚å¯¾å¿œï¼ˆæœ€çµ‚çš„ã«liståŒ–ï¼‰
        kw_meta = _meta.get("dst_keywords")
        if isinstance(kw_meta, str):
            kw_meta_list = [k.strip() for k in kw_meta.split(",") if k.strip()]
        elif isinstance(kw_meta, (list, tuple)):
            kw_meta_list = [str(k).strip() for k in kw_meta if str(k).strip()]
        else:
            kw_meta_list = []
        href = href_meta or href0
        tgt_title = title_meta or tgt_title0
        if not href:
            res.skipped += 1
            continue

        # --- è‡ªå·±ãƒªãƒ³ã‚¯ç¦æ­¢ï¼ˆsrc_post_id == target_post_id ã®å ´åˆï¼‰ ---
        try:
            if int(act.target_post_id) == int(src_post_id):
                act.status = "skipped"
                act.reason = "self-link"
                act.updated_at = datetime.now(UTC)
                res.skipped += 1
                continue
        except Exception:
            pass

        # ï¼ˆç·©å’Œï¼‰planner ç”±æ¥ã® anchor_text ã¯ä½¿ã‚ãšã€generated_line ã§ã¯ LLM ã§æ¯å›ç”Ÿæˆã™ã‚‹

        # --- åŒä¸€URLã¸ã®é‡è¤‡ã‚¢ãƒ³ã‚«ãƒ¼ç¦æ­¢ï¼ˆæ—¢ã«åŒã˜hrefã‚’åˆ¥ã‚¢ãƒ³ã‚«ãƒ¼ã§ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆï¼‰ ---
        existing_links = [h for (h, atext) in _extract_links(_rejoin_paragraphs(paragraphs))]
        if href in existing_links:
            # ãŸã ã—åŒä¸€ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆãªã‚‰æ—¢å­˜ã‚’ç½®æ›å¯¾è±¡ã«å›ã™ã®ã§OK
            if act.anchor_text and not any(atext == act.anchor_text and h == href for (h, atext) in _extract_links(_rejoin_paragraphs(paragraphs))):
                act.status = "skipped"
                act.reason = "duplicate-href-anchor"
                act.updated_at = datetime.now(UTC)
                res.skipped += 1
                continue
        # ---- æ–°æ–¹å¼ / æ—§æ–¹å¼ã®åˆ†å² ----
        if ANCHOR_MODE == "generated_line":
            # æ®µè½æœ¬æ–‡ï¼ˆå®‰å…¨ã‚¬ãƒ¼ãƒ‰/ç¦æ­¢é ˜åŸŸãƒã‚§ãƒƒã‚¯ï¼‰
            para_html = paragraphs[idx]
            if _H_TAG.search(para_html) or _TOC_HINT.search(para_html):
                res.skipped += 1
                continue
            # è¨˜äº‹å…¨ä½“ã§åŒä¸€hrefãŒæ—¢ã«å­˜åœ¨ã—ãŸã‚‰ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ®µè½ã‚’ã¾ãŸã„ã é‡è¤‡é˜²æ­¢ï¼‰
            if href in article_href_set:
                act.status = "skipped"
                act.reason = "duplicate-href-in-article"
                act.updated_at = datetime.now(UTC)
                res.skipped += 1
                continue
            # åŒä¸€target_post_idã¯è¨˜äº‹å†…ã§1å›ã¾ã§
            try:
                if int(act.target_post_id) in used_target_pids:
                    act.status = "skipped"
                    act.reason = "duplicate-target-in-article"
                    act.updated_at = datetime.now(UTC)
                    res.skipped += 1
                    continue
            except Exception:
                pass
            # åŒä¸€æ®µè½ã«åŒã˜hrefãŒæ—¢ã«ã‚ã‚‹ãªã‚‰å¤šé‡ãƒªãƒ³ã‚¯å›é¿
            if href in [h for (h, _) in _extract_links(para_html)]:
                res.skipped += 1
                continue

            # --- æƒ¹å¥ãƒ†ã‚­ã‚¹ãƒˆï¼šLLM ç”Ÿæˆï¼ˆç·©å’Œç‰ˆï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–æ‹ ãƒ»æœ€ä½é™ã®æ•´å½¢ã®ã¿ï¼‰ ---
            try:
                src_hint = _html_to_text(para_html)[:120]
                dst_kw_list = kw_meta_list or []
                if not dst_kw_list:
                    try:
                        dst_kw_list = [w for w in (title_tokens(tgt_title or "") or []) if w][:6]
                    except Exception:
                        dst_kw_list = []
                if ANCHOR_STYLE == "template":
                    base = (dst_kw_list[0] if dst_kw_list else (tgt_title or "")[:20]).strip()
                    anchor_text = f"{base}ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"
                else:
                    anchor_text = _generate_anchor_text_via_llm(
                        dst_title=tgt_title or "",
                        dst_keywords=dst_kw_list,
                        src_hint=src_hint,
                        user_id=None,
                    )
                # äº‹å¾Œæ•´å½¢ï¼ˆæ—¥æœ¬èªã®ä¸è‡ªç„¶ã•ã‚’è»½æ¸›ï¼‰
                anchor_text = _postprocess_anchor_text(anchor_text)
                if len(anchor_text) > ISEO_ANCHOR_MAX_CHARS:
                    anchor_text = anchor_text[:ISEO_ANCHOR_MAX_CHARS]
                    anchor_text = re.sub(r"[ã€ã€‚ï¼.\s]+$", "", anchor_text)
                    if not anchor_text.endswith("ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"):
                        base = (dst_kw_list[0] if dst_kw_list else (tgt_title or "")[:20]).strip()
                        anchor_text = f"{base}ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"
            except Exception as e:
                logger.warning(f"[GEN-ANCHOR] LLM failed: {e}")
                key = (tgt_title or "").strip()
                anchor_text = (f"{key}ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©")[:ISEO_ANCHOR_MAX_CHARS] if key else "å†…éƒ¨ãƒªãƒ³ã‚¯ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"

            # NGã‚¢ãƒ³ã‚«ãƒ¼æœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½é™ï¼‰
            if _is_ng_anchor_generated_line(anchor_text, tgt_title):
                # --- â˜…ãƒªã‚«ãƒãƒª1ï¼šå®‰å…¨ãƒ†ãƒ³ãƒ—ãƒ¬å†æ§‹æˆ â†’ æ—¥æœ¬èªè£œæ­£ â†’ å†åˆ¤å®š
                fallback = _safe_anchor_from_keywords(dst_kw_list, tgt_title or "")
                fallback = _postprocess_anchor_text(fallback)
                # é•·ã™ãã‚‹å ´åˆã®ä¸¸ã‚ï¼ˆèªå°¾ã‚’ä¿ã¤ï¼‰
                if len(fallback) > ISEO_ANCHOR_MAX_CHARS:
                    fallback = fallback[:ISEO_ANCHOR_MAX_CHARS]
                    fallback = re.sub(r"[ã€ã€‚ï¼.\\s]+$", "", fallback)
                    if not fallback.endswith("ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"):
                        fallback = _safe_anchor_from_keywords(dst_kw_list, tgt_title or "")
                if not _is_ng_anchor_generated_line(fallback, tgt_title):
                    anchor_text = fallback
                else:
                    act.status = "skipped"
                    act.reason = "ng-anchor"
                    act.updated_at = datetime.now(UTC)
                    res.skipped += 1
                    continue

            # ç›´å‰æ®µè½ãŒã™ã§ã«å†…éƒ¨ãƒªãƒ³ã‚¯ã§çµ‚ã‚ã£ã¦ã„ã‚Œã°ï¼ˆç‰ˆãƒãƒ¼ã‚¯ã‚ã‚Šï¼‰é€£ç¶šè¡Œã‚’å›é¿
            if idx - 1 >= 0:
                prev_tail = paragraphs[idx - 1][-200:]
                if INTERNAL_SEO_SPEC_MARK in prev_tail:
                    act.status = "skipped"
                    act.reason = "avoid-consecutive-link-paragraphs"
                    act.updated_at = datetime.now(UTC)
                    res.skipped += 1
                    continue

            # è¨˜äº‹å†…ã§ã®ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆé‡è¤‡æŠ‘æ­¢ï¼ˆæ—¢å­˜ï¼‹ä»Šå›å®Ÿè¡Œå†…ï¼‰
            anchor_key = nfkc_norm(anchor_text).lower()
            if anchor_key and anchor_key in seen_anchor_keys:
                act.status = "skipped"
                act.reason = "duplicate-anchor-in-article"
                act.updated_at = datetime.now(UTC)
                res.skipped += 1
                continue

            if anchor_key and anchor_key in existing_anchor_text_set:
                act.status = "skipped"
                act.reason = "duplicate-anchor-existing"
                act.updated_at = datetime.now(UTC)
                res.skipped += 1
                continue

            anchor_html = _emit_anchor_html(href, anchor_text)
            # p: ã§ã¯å›²ã¿ãƒœãƒƒã‚¯ã‚¹ã¯ä½¿ã‚ãªã„ï¼ˆæ–°ä»•æ§˜ã¯ h2: ã®ã¿ï¼‰
            paragraphs[idx] = para_html + "<br>" + anchor_html
            article_href_set.add(href)
            try:
                used_target_pids.add(int(act.target_post_id))
            except Exception:
                pass
            act.anchor_text = anchor_text
            act.status = "applied"
            act.applied_at = datetime.now(UTC)
            res.applied += 1
            inserted += 1
            if anchor_key:
                seen_anchor_keys.add(anchor_key)
                existing_anchor_text_set.add(anchor_key)
            logger.info(f"[GEN-ANCHOR] p={idx} text='{anchor_text}' -> {href}")
            continue
        # åŒä¸€target_post_idã¯è¨˜äº‹å†…ã§1å›ã¾ã§
        try:
            if int(act.target_post_id) in used_target_pids:
                act.status = "skipped"
                act.reason = "duplicate-target-in-article"
                act.updated_at = datetime.now(UTC)
                res.skipped += 1
                continue
        except Exception:
            pass
            # åŒã˜URLãŒæ®µè½å†…ã«æ—¢ã«ã‚ã‚‹ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå¤šé‡ãƒªãƒ³ã‚¯å›é¿ï¼‰
            if href in [h for (h, _) in _extract_links(para_html)]:
                res.skipped += 1
                continue

            # æƒ¹å¥ãƒ†ã‚­ã‚¹ãƒˆï¼šgenerated_line ãƒ¢ãƒ¼ãƒ‰ã§ã¯ **æ¯å› LLM ç”Ÿæˆ**ï¼ˆplannerã® anchor_text ã¯ä½¿ã‚ãªã„ï¼‰
            try:
                # æ–‡è„ˆãƒ’ãƒ³ãƒˆï¼šå½“è©²æ®µè½ã®ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠœç²‹
                src_hint = _html_to_text(para_html)[:120]
                # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼šplannerã®meta(dst_keywords)ã‚’æœ€å„ªå…ˆã€ç„¡ã‘ã‚Œã°ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æŠ½å‡º
                dst_kw_list = kw_meta_list
                if not dst_kw_list:
                    try:
                        dst_kw_list = [w for w in (title_tokens(tgt_title or "") or []) if w][:6]
                    except Exception:
                        dst_kw_list = []
                # ç”Ÿæˆ â†’ å“è³ªãƒã‚§ãƒƒã‚¯ â†’ æœ€å¤§2å›ã¾ã§å†ç”Ÿæˆã€æœ€å¾Œã¯ãƒ†ãƒ³ãƒ—ãƒ¬ã§ç¢ºå®š
                def _gen_once() -> str:
                    if ANCHOR_STYLE == "template":
                        base = (dst_kw_list[0] if dst_kw_list else (tgt_title or "")[:20]).strip()
                        return f"{base}ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"
                    return _generate_anchor_text_via_llm(
                        dst_title=tgt_title or "",
                        dst_keywords=dst_kw_list,
                        src_hint=src_hint,
                        user_id=None,
                    )
                anchor_text = _gen_once()
                tries = 0
                while tries < 2 and not _is_anchor_quality_ok(anchor_text, dst_kw_list, tgt_title or ""):
                    anchor_text = _gen_once()
                    tries += 1
                if not _is_anchor_quality_ok(anchor_text, dst_kw_list, tgt_title or ""):
                    base = (dst_kw_list[0] if dst_kw_list else (tgt_title or "")[:20]).strip()
                    anchor_text = f"{base}ã«ã¤ã„ã¦è©³ã—ã„è§£èª¬ã¯ã‚³ãƒãƒ©"
            except Exception as e:
                logger.warning(f"[GEN-ANCHOR] LLM failed: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®å®šå‹ï¼ˆè»½ã„CTA + ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸»è¦èªï¼‰
                key = (tgt_title or "").strip()
                anchor_text = (f"{key}ã®è©³ã—ã„è§£èª¬ã¯ã“ã¡ã‚‰ã€‚")[:80] if key else "è©³ã—ã„è§£èª¬ã¯ã“ã¡ã‚‰ã€‚"
            # NGã‚¢ãƒ³ã‚«ãƒ¼æœ€çµ‚ãƒã‚§ãƒƒã‚¯
            if is_ng_anchor(anchor_text, tgt_title):
                act.status = "skipped"
                act.reason = "ng-anchor"
                act.updated_at = datetime.now(UTC)
                res.skipped += 1
                continue

            # è¨˜äº‹å†…ã§ã®é‡è¤‡æŠ‘æ­¢
            anchor_key = nfkc_norm(anchor_text).lower()
            if anchor_key and anchor_key in seen_anchor_keys:
                act.status = "skipped"
                act.reason = "duplicate-anchor-in-article"
                act.updated_at = datetime.now(UTC)
                res.skipped += 1
                continue

            anchor_html = _emit_anchor_html(href, anchor_text)
            # è¦æ±‚æ§‹é€ ç¶­æŒï¼š<br><a ...> ã®å½¢ã ãŒã€ç‰ˆãƒãƒ¼ã‚¯ã¯ç›´å‰ã‚³ãƒ¡ãƒ³ãƒˆã¨ã—ã¦ <a> ã®ç›´å‰ã«ç½®ã
            paragraphs[idx] = para_html + "<br>" + anchor_html
            # è¨˜äº‹ãƒ¬ãƒ™ãƒ«ã®é‡è¤‡æŠ‘æ­¢ã‚»ãƒƒãƒˆã‚’æ›´æ–°
            article_href_set.add(href)
            try:
                used_target_pids.add(int(act.target_post_id))
            except Exception:
                pass

            # çŠ¶æ…‹æ›´æ–°
            act.anchor_text = anchor_text  # ç”Ÿæˆçµæœã‚’ä¿å­˜ï¼ˆå†è©¦è¡Œæ™‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚‚ãªã‚‹ï¼‰
            act.status = "applied"
            act.updated_at = datetime.now(UTC)
            res.applied += 1
            inserted += 1
            if anchor_key:
                seen_anchor_keys.add(anchor_key)
            logger.info(f"[GEN-ANCHOR] p={idx} text='{anchor_text}' -> {href}")

        else:
            # ---- æ—§æ–¹å¼ï¼šèªå¥ã®æœ€åˆã®å‡ºç¾ã‚’ãƒªãƒ³ã‚¯åŒ–ï¼ˆå¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒï¼‰ ----
            # â˜…å®‰å…¨ã‚¬ãƒ¼ãƒ‰ï¼ˆç·©å’Œç‰ˆï¼‰ï¼šã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¿ã‚¤ãƒˆãƒ«ã¸ã®éƒ¨åˆ†ä¸€è‡´
            if nfkc_norm(act.anchor_text) not in nfkc_norm(tgt_title or ""):
                act.status = "skipped"
                act.reason = "skipped:anchor-not-in-target-title"
                act.updated_at = datetime.now(UTC)
                res.skipped += 1
                continue

            # é‡è¤‡æŠ‘æ­¢
            anchor_key = nfkc_norm((act.anchor_text or "").strip()).lower()
            if anchor_key and anchor_key in seen_anchor_keys:
                act.status = "skipped"
                act.reason = "duplicate-anchor-in-article"
                act.updated_at = datetime.now(UTC)
                res.skipped += 1
                continue

            para_html = paragraphs[idx]
            if _H_TAG.search(para_html) or _TOC_HINT.search(para_html):
                res.skipped += 1
                continue
            if href in [h for (h, _) in _extract_links(para_html)]:
                res.skipped += 1
                continue
            if is_ng_anchor(act.anchor_text, tgt_title):
                act.status = "skipped"
                act.reason = "ng-anchor"
                act.updated_at = datetime.now(UTC)
                res.skipped += 1
                continue
            new_para = _linkify_first_occurrence(para_html, act.anchor_text, href, tgt_title)
            if not new_para:
                res.skipped += 1
                continue
            paragraphs[idx] = new_para
            article_href_set.add(href)
            try:
                used_target_pids.add(int(act.target_post_id))
            except Exception:
                pass
            act.status = "applied"
            act.applied_at = datetime.now(UTC)
            res.applied += 1
            inserted += 1
            if anchor_key:
                seen_anchor_keys.add(anchor_key)

    # h2/p é©ç”¨å¾Œã®æœ¬æ–‡ã‚’é€£çµ
    new_html_mid = _rejoin_paragraphs(paragraphs)
    #   ï¼ˆç°¡æ˜“ãƒ«ãƒ¼ãƒ«ï¼šscoreã®ä½ãã†ãªæ—¢å­˜ãƒªãƒ³ã‚¯ã‚’ã²ã¨ã¤ã ã‘å·®ã—æ›¿ãˆï¼‰
    # 2) swapå€™è£œï¼šæ—¢å­˜å†…éƒ¨ãƒªãƒ³ã‚¯ãŒã‚ã‚‹ & ã¾ã ä½™è£•ãŒãªã„å ´åˆã«ç½®æ›ã‚’è©¦ã¿ã‚‹
    if swaps and (existing_internal + inserted) >= need_min:
        # æ—¢å­˜ãƒªãƒ³ã‚¯åˆ—æŒ™
        existing = _extract_links(new_html_mid)
        # æ—¢å­˜ internal ã®ã†ã¡ã‚¹ã‚³ã‚¢ãŒä½ã„ã‚‚ã®ã‚’ç‰¹å®š
        # URL -> post_id
        url_to_pid = {}
        rows = (
            ContentIndex.query
            .with_entities(ContentIndex.url, ContentIndex.wp_post_id)
            .filter(ContentIndex.site_id == site.id)
            .filter(ContentIndex.url.in_([u for (u, _) in existing]))
            .all()
        )
        for u, pid in rows:
            url_to_pid[u] = int(pid) if pid else None

        # ãã®post_idã®ã‚¹ã‚³ã‚¢ã‚’å–ã‚Šå‡ºã—ã€æœ€å°ã‚¹ã‚³ã‚¢ã®ãƒªãƒ³ã‚¯ã‚’äº¤æ›å¯¾è±¡ã«
        def score_of(dst_pid: Optional[int]) -> float:
            if not dst_pid:
                return 0.0
            row = (
                InternalLinkGraph.query
                .with_entities(InternalLinkGraph.score)
                .filter_by(site_id=site.id, source_post_id=src_post_id, target_post_id=dst_pid)
                .one_or_none()
            )
            return float(row[0]) if row and row[0] is not None else 0.0

        worst_url = None
        worst_score = 999.0
        for (href, _) in existing:
            if not _is_internal_url(site_url, href):
                continue
            sc = score_of(url_to_pid.get(href))
            if sc < worst_score:
                worst_score = sc
                worst_url = href

        if worst_url:
            # æœ€ã‚‚ã‚¹ã‚³ã‚¢ã®é«˜ã„ swap å€™è£œã‚’1ã¤
            swaps_sorted = sorted(swaps, key=lambda a: a.id)  # å®‰å®š
            best_swap = None
            best_sc = -1.0
            for s in swaps_sorted:
                href, _t = target_meta_map.get(s.target_post_id, ("",""))
                if not href:
                    continue
                row = (
                    InternalLinkGraph.query
                    .with_entities(InternalLinkGraph.score)
                    .filter_by(site_id=site.id, source_post_id=src_post_id, target_post_id=s.target_post_id)
                    .one_or_none()
                )
                sc = float(row[0]) if row and row[0] is not None else 0.0
                if sc > best_sc:
                    best_sc = sc
                    best_swap = (s, href)

            if best_swap and best_sc > worst_score + 0.10:  # ãƒãƒ¼ã‚¸ãƒ³
                s_act, new_href = best_swap
                # æ®µè½å˜ä½ã§ç½®æ›ã—ã€è¦‹å‡ºã—/TOC æ®µè½ã¯é™¤å¤–
                replaced = False
                for i, para in enumerate(paragraphs):
                    if _H_TAG.search(para) or _TOC_HINT.search(para):
                        continue
                    idx = para.find(f'href="{worst_url}"')
                    if idx == -1:
                        continue
                    new_para = para.replace(f'href="{worst_url}"', f'href="{new_href}"', 1)
                    # class/style ã‚’å–ã‚Šé™¤ã Wikipedia é¢¨ã«ï¼ˆtitle ã¯ _normalize ã§æ•´ã†ï¼‰
                    new_para = _add_attrs_to_first_anchor_with_href(new_para, new_href)
                    paragraphs[i] = new_para
                    replaced = True
                    break
                if replaced:                    
                    s_act.status = "applied"
                    s_act.reason = "swap"  # æ¡ç”¨ã•ã‚ŒãŸç½®æ›
                    s_act.applied_at = datetime.now(UTC)
                    res.swapped += 1

    # æœ¬æ–‡ã‚’é€£çµ â†’ æ—¢å­˜ã® ai-ilink / inline-style ã‚’ Wikipedia é¢¨ã«æ­£è¦åŒ–
    new_html = _rejoin_paragraphs(paragraphs)
    new_html = _normalize_existing_internal_links(new_html)
    # æ­£è¦åŒ–ã§ a ã‚’ã„ã˜ã£ã¦ã‚‚ã€ç›´å‰ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆINTERNAL_SEO_SPEC_MARKï¼‰ã¯ HTML ã¨ç‹¬ç«‹ã§æ®‹ã‚‹æƒ³å®š
    return new_html, res

# ---- ãƒ‘ãƒ–ãƒªãƒƒã‚¯API ----

def _update_with_retry(site: Site, post_id: int, new_html: str) -> bool:
    """
    WP æ›´æ–°ã®ãƒªãƒˆãƒ©ã‚¤ãƒ©ãƒƒãƒ‘ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰
    ç’°å¢ƒå¤‰æ•°:
      ISEO_WP_RETRY_MAX      ... æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ï¼ˆæ—¢å®š 3ï¼‰
      ISEO_WP_RETRY_BASE_MS  ... åˆæœŸå¾…æ©ŸãƒŸãƒªç§’ï¼ˆæ—¢å®š 400ï¼‰
    """
    try:
        max_retry = int(os.getenv("ISEO_WP_RETRY_MAX", "3"))
        base_ms   = int(os.getenv("ISEO_WP_RETRY_BASE_MS", "400"))
    except Exception:
        max_retry, base_ms = 3, 400

    attempt = 0
    while True:
        ok = update_post_content(site, post_id, new_html)
        if ok:
            return True
        if attempt >= max_retry:
            return False
        # 2^attempt * base_ms ï¼ˆÂ±20% ã®ã‚†ã‚‰ãï¼‰
        delay = (base_ms * (2 ** attempt)) / 1000.0
        delay *= random.uniform(0.8, 1.2)
        time.sleep(delay)
        attempt += 1

def apply_actions_for_post(site_id: int, src_post_id: int, dry_run: bool = False) -> ApplyResult:
    """
    1è¨˜äº‹åˆ†ã® pending ã‚’èª­ã¿è¾¼ã‚“ã§å·®åˆ†é©ç”¨ï¼ˆWPæ›´æ–°ï¼‰ã€‚
    - dry_run=True ã®å ´åˆã¯WPæ›´æ–°ã›ãšã€çµæœã ã‘è¿”ã™
    """
    cfg = InternalSeoConfig.query.filter_by(site_id=site_id).one_or_none()
    if not cfg:
        cfg = InternalSeoConfig(site_id=site_id)
        db.session.add(cfg)
        db.session.commit()

    site = db.session.get(Site, site_id)
    # â–¼ topic ã‚¹ã‚­ãƒƒãƒ—ï¼ˆè¨˜äº‹URLãŒ topic ã®ã¨ãã¯â€œé©ç”¨ãƒ­ã‚¸ãƒƒã‚¯å…¨ä½“â€ã‚’åœæ­¢ï¼‰
    try:
        if os.getenv("INTERNAL_SEO_SKIP_TOPIC", "1") != "0":
            src_url = _post_url(site_id, src_post_id) or ""
            if is_topic_url(src_url):
                return ApplyResult(message="skip-topic-page")
    except Exception:
        pass
    wp_post = fetch_single_post(site, src_post_id)
    if not wp_post:
        return ApplyResult(message="fetch-failed-or-excluded")

    # 1) æ—§ä»•æ§˜å‰Šé™¤ï¼ˆapplyï¼šå¾Œã§å‰Šé™¤ãƒ­ã‚°ã‚’ä¿å­˜ï¼‰â€” æ–°ã‚·ã‚°ãƒãƒãƒ£å„ªå…ˆã€æœªå¯¾å¿œç’°å¢ƒã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    url_title_map = _all_url_to_title_map(site_id)
    url_pid_map   = _all_url_to_pid_map(site_id)
    try:
        cleaned_html, deletions = find_and_remove_legacy_links(
            wp_post.content_html or "", url_title_map, spec_version=INTERNAL_SEO_SPEC_VERSION
        )
    except TypeError:
        cleaned_html, deletions = find_and_remove_legacy_links(wp_post.content_html or "", url_title_map)

    # 2) å¯¾è±¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆplan / swap_candidateï¼‰
    actions = (
        InternalLinkAction.query
        .filter_by(site_id=site_id, post_id=src_post_id, status="pending")
        .order_by(InternalLinkAction.created_at.asc())
        .all()
    )

    # --- â˜…åŒè¨˜äº‹å†…ãƒªãƒ“ãƒ«ãƒ‰æ™‚ã®æ—§ç‰ˆã‚¯ãƒ­ãƒ¼ã‚ºï¼ˆç›£æŸ»ãƒ­ã‚°ã®æ•´åˆæ€§ç¢ºä¿ï¼‰ ---
    # ãƒ«ãƒ¼ãƒ«:
    #  - ä»Šå›é©ç”¨å¯¾è±¡ï¼ˆpendingï¼‰ã® link_version ã‚’åŸºæº–ã¨ã™ã‚‹ï¼ˆæœªè¨­å®šã¯ spec ã®æ•´æ•°ç‰ˆï¼‰
    #  - ã™ã§ã« 'applied' ã®è¡Œã§ã€åŸºæº–ã‚ˆã‚Šå¤ã„ link_version ã¯ 'superseded' ã«æ›´æ–°
    #  - HTML ã®å‰Šé™¤ã¯è¡Œã‚ãªã„ï¼ˆæ—¢å­˜ã®é‡è¤‡æŠ‘æ­¢ã§äºŒé‡é©ç”¨ã¯å›é¿ï¼‰
    if actions:
        try:
            # pending ã«å«ã¾ã‚Œã‚‹æœ€å¤§ link_versionï¼ˆç„¡ã‘ã‚Œã° spec ç”±æ¥ã«çµ±ä¸€ï¼‰
            pending_versions = [int(getattr(a, "link_version") or 0) for a in actions]
            target_link_version = max(max(pending_versions), _link_version_int())
        except Exception:
            target_link_version = _link_version_int()

        if not dry_run:
            now = datetime.now(UTC)
            # åŒä¸€è¨˜äº‹ã§ â€œå¤ã„ç‰ˆâ€ ã® applied ã‚’ä¸€æ‹¬ã‚¯ãƒ­ãƒ¼ã‚º
            old_applied = (
                InternalLinkAction.query
                .filter(
                    and_(
                        InternalLinkAction.site_id == site_id,
                        InternalLinkAction.post_id == src_post_id,
                        InternalLinkAction.status == "applied",
                        InternalLinkAction.link_version < target_link_version,
                    )
                ).all()
            )
            for oa in old_applied:
                oa.status = "superseded"
                oa.reverted_at = now
                oa.reason = (oa.reason or "rebuild")  # ç”±æ¥ã‚’ç°¡æ˜“ãƒãƒ¼ã‚¯
                oa.updated_at = now
    else:
        target_link_version = _link_version_int()

    meta_map = _action_targets_meta(site_id, actions)

    # 3) å·®åˆ†ä½œæˆï¼ˆæ—§ä»•æ§˜å‰Šé™¤æ¸ˆã¿ã®æœ¬æ–‡ã«æ–°ä»•æ§˜ã‚’é©ç”¨ï¼‰
    base_html = cleaned_html if cleaned_html is not None else (wp_post.content_html or "")
    # å…¥åŠ›HTMLã‚’ãã®ã¾ã¾ç”¨ã„ã¦æ–°ä»•æ§˜ã‚’é©ç”¨ï¼ˆè¦‹å‡ºã—å†…ãƒªãƒ³ã‚¯ã¯ä¸å¤‰æ›´ï¼‰
    new_html, res = _apply_plan_to_html(site, src_post_id, base_html, actions, cfg, meta_map)
    # è¨˜äº‹å…ˆé ­ã« 1å›ã ã‘ä¸‹ç·šCSSã‚’æ³¨å…¥ï¼ˆãƒ†ãƒ¼ãƒéä¾å­˜ï¼‰
    new_html = _ensure_inline_underline_style(site, new_html)
    res.legacy_deleted = len(deletions or [])

    if dry_run:
        # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼šDBã‚’ä¸€åˆ‡å¤‰æ›´ã—ãªã„ï¼ˆæ—§ä»•æ§˜å‰Šé™¤ä»¶æ•°ã ã‘åæ˜ ï¼‰
        if not actions and res.legacy_deleted > 0:
            res.message = "legacy-clean-only"
        elif not actions:
            res.message = "no-pending"
        return res

    # ç›£æŸ»ç”¨ã®æŠœç²‹
    before_excerpt = _html_to_text(wp_post.content_html)[:280]
    after_excerpt = _html_to_text(new_html)[:280]

    #    æ–°ã—ãé©ç”¨ã•ã‚ŒãŸ pending ã¯ã€ä»Šå›ã® target_link_version ã«çµ±ä¸€ã—ã¦è¨˜éŒ²
    for a in actions:
        if a.status == "applied":
            try:
                a.link_version = int(target_link_version)
            except Exception:
                a.link_version = _link_version_int()
            a.diff_before_excerpt = before_excerpt
            a.diff_after_excerpt = after_excerpt
        else:
            # é©ç”¨ã•ã‚Œãªã‹ã£ãŸ pending ã¯ã‚¹ã‚­ãƒƒãƒ—ã¸
            a.status = a.status if a.status == "applied" else "skipped"
            a.updated_at = datetime.now(UTC)

    # 5) æ—§ä»•æ§˜å‰Šé™¤ãƒ­ã‚°ã‚’ä¿å­˜ï¼ˆ1å‰Šé™¤=1è¡Œã€status='legacy_deleted'ï¼‰
    if deletions:
        now = datetime.now(UTC)
        for d in deletions:
            anchor_text = (d.get("anchor_text") or "")
            href        = (d.get("href") or "")
            position    = (d.get("position") or "")
            # target_post_id ã¯ cleaner ãŒè¿”ã™ã‹ã€URLâ†’PID ã§æ¨å®š
            tpid = d.get("target_post_id")
            if not tpid and href:
                tpid = url_pid_map.get(href)
            try:
                tpid_int = int(tpid) if tpid is not None else None
            except Exception:
                tpid_int = None
            ila = InternalLinkAction(
                site_id=site_id,
                post_id=src_post_id,
                target_post_id=tpid_int,
                anchor_text=anchor_text,
                position=position,
                reason="legacy_cleanup",
                status="legacy_deleted",
                link_version=_link_version_int(),
                created_at=now,
                updated_at=now,
                diff_before_excerpt=before_excerpt,
                diff_after_excerpt=after_excerpt,
                # link_version ã¯æ—¢ã« _link_version_int() ã‚’æ˜ç¤ºã‚»ãƒƒãƒˆ
            )
            db.session.add(ila)        
    db.session.commit()

    # WPã¸åæ˜ 
    ok = update_post_content(site, src_post_id, new_html)
    # --- No-Op æ—©æœŸçµ‚äº†ï¼šæœ¬æ–‡ã«å®Ÿè³ªå·®åˆ†ãŒç„¡ã„ & ä½•ã‚‚é©ç”¨ã—ã¦ã„ãªã„å ´åˆã¯ WP æ›´æ–°ã‚’çœç•¥ ---
    #   â€» æ—§ä»•æ§˜ã®å‰Šé™¤ã‚‚ãªãã€applied / swapped ãŒ 0 ã®ã¨ãã ã‘ã‚¹ã‚­ãƒƒãƒ—
    if (res.applied == 0 and res.swapped == 0 and res.legacy_deleted == 0):
        orig_norm = (wp_post.content_html or "").strip()
        new_norm  = (new_html or "").strip()
        if orig_norm == new_norm:
            logger.info("[Applier] no-op skip (no diff & no actions) site=%s post=%s", site_id, src_post_id)
            return res

    # WPã¸åæ˜ ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
    ok = _update_with_retry(site, src_post_id, new_html)
    if not ok:
        # åæ˜ å¤±æ•—æ™‚ã¯ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰±ã„ã«ã—ã¦ãŠãï¼ˆappliedâ†’skippedï¼‰
        for a in actions:
            if a.status == "applied":
                a.status = "pending"  # å†è©¦è¡Œã§ãã‚‹ã‚ˆã† pending ã«æˆ»ã™
                a.applied_at = None
        db.session.commit()
        return ApplyResult(message="wp-update-failed")

    return res

def apply_actions_for_site(site_id: int, limit_posts: Optional[int] = 50, dry_run: bool = False) -> Dict[str, int]:
    """
    ã‚µã‚¤ãƒˆå…¨ä½“ã§ pending ã‚’æœ¬æ–‡ã«åæ˜ ã€‚limit_posts ã§åˆ»ã‚“ã§å®‰å…¨ã«ã€‚
    """
    q = (
        InternalLinkAction.query
        .with_entities(InternalLinkAction.post_id)
        .filter_by(site_id=site_id, status="pending")
        .group_by(InternalLinkAction.post_id)
        .order_by(db.func.min(InternalLinkAction.created_at).asc())
    )
    if limit_posts:
        q = q.limit(limit_posts)
    src_ids = [int(pid) for (pid,) in q.all()]
    total = {"applied": 0, "swapped": 0, "skipped": 0, "processed_posts": 0}

    for src_post_id in src_ids:
        res = apply_actions_for_post(site_id, src_post_id, dry_run=dry_run)
        total["applied"] += res.applied
        total["swapped"] += res.swapped
        total["skipped"] += res.skipped
        total["processed_posts"] += 1

        
        # --- ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡ï¼šWP REST API è² è·ä¿è­· ---
        if not dry_run:
            try:
                # 1åˆ†ã‚ãŸã‚Šã®æœ€å¤§ä»¶æ•°ï¼ˆä¾‹: 120 â†’ 0.5ç§’é–“éš”ï¼‰
                per_min = int(os.getenv("INTERNAL_SEO_RATE_LIMIT_PER_MIN", "0"))
                if per_min > 0:
                    base_sleep = 60.0 / max(1, per_min)
                else:
                    base_sleep = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœ€ä½500ms

                # 200ã€œ500ms ã¯æœ€ä½ä¿è¨¼
                base_sleep = max(base_sleep, 0.2)

                # Â±30% ã®ãƒ©ãƒ³ãƒ€ãƒ æºã‚‰ãã‚’åŠ ãˆã‚‹
                sleep_time = base_sleep * random.uniform(0.7, 1.3)
                time.sleep(sleep_time)
            except Exception as e:
                logger.warning(f"[Applier] rate-limit sleep skipped due to error: {e}")

    logger.info("[Applier] site=%s result=%s", site_id, total)
    return total

# ==== è¿½åŠ ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼å˜ä½ã§ã‚µã‚¤ãƒˆæ¨ªæ–­é©ç”¨ ====
def apply_actions_for_user(user_id: int, limit_posts: int = 50, dry_run: bool = False) -> Dict[str, object]:
    """
    æŒ‡å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç´ã¥ãå…¨ã‚µã‚¤ãƒˆã‚’å¯¾è±¡ã«ã€pending ã®å†…éƒ¨ãƒªãƒ³ã‚¯é©ç”¨ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    - `limit_posts`: ã“ã®å‘¼ã³å‡ºã—ï¼ˆ1 tickï¼‰ã§å‡¦ç†ã™ã‚‹ã€Œè¨˜äº‹æ•°ã€ã®ç·äºˆç®—
    - äºˆç®—ã¯ã‚µã‚¤ãƒˆã”ã¨ã® pending ä»¶æ•°ã‚’è¦‹ã¦ã€Œæ°´å‰²ã‚Šï¼ˆå‡ç­‰ï¼‹ä½™ã‚Šå‰å¯„ã›ï¼‰ã€ã§é…åˆ†

    æˆ»ã‚Šå€¤:
      {
        "applied": int,
        "swapped": int,
        "skipped": int,
        "processed_posts": int,
        "pending_total": int,         # é–‹å§‹æ™‚ç‚¹ã®ã€Œæœªå‡¦ç†è¨˜äº‹ã€ç·æ•°ï¼ˆdistinct post_idï¼‰
        "site_breakdown": [           # ã‚µã‚¤ãƒˆã”ã¨ã®å®Ÿè¡Œçµæœã‚µãƒãƒª
          {
            "site_id": int,
            "allocated_posts": int,   # ä»Šå›å‰²ã‚Šå½“ã¦ãŸè¨˜äº‹æ•°
            "pending_posts": int,     # é–‹å§‹æ™‚ç‚¹ã®ã‚µã‚¤ãƒˆå†… pending è¨˜äº‹æ•°
            "result": {"applied":..,"swapped":..,"skipped":..,"processed_posts":..}
          },
          ...
        ]
      }
    """
    # 1) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚µã‚¤ãƒˆæŠ½å‡º
    sites = (
        Site.query
        .with_entities(Site.id)
        .filter(Site.user_id == user_id)
        .all()
    )
    site_ids = [int(sid) for (sid,) in sites]
    if not site_ids:
        return {
            "applied": 0, "swapped": 0, "skipped": 0,
            "processed_posts": 0, "pending_total": 0,
            "site_breakdown": []
        }

    # 2) ã‚µã‚¤ãƒˆã”ã¨ã®ã€Œpending ã®ã‚ã‚‹æŠ•ç¨¿æ•°ï¼ˆdistinct post_idï¼‰ã€ã‚’é›†è¨ˆ
    pending_rows = (
        db.session.query(
            InternalLinkAction.site_id,
            db.func.count(db.func.distinct(InternalLinkAction.post_id)).label("pending_posts")
        )
        .filter(InternalLinkAction.site_id.in_(site_ids),
                InternalLinkAction.status == "pending")
        .group_by(InternalLinkAction.site_id)        
        .all()
    )
    pending_map = {int(sid): int(cnt) for (sid, cnt) in pending_rows}

    # pending ãŒã‚¼ãƒ­ãªã‚‰ä½•ã‚‚ã—ãªã„
    pending_total = sum(pending_map.values())
    if pending_total == 0 or (limit_posts or 0) <= 0:
        return {
            "applied": 0, "swapped": 0, "skipped": 0,
            "processed_posts": 0, "pending_total": pending_total,
            "site_breakdown": [
                {"site_id": sid, "allocated_posts": 0, "pending_posts": pending_map.get(sid, 0), "result": {"applied":0,"swapped":0,"skipped":0,"processed_posts":0}}
                for sid in site_ids
            ]
        }

    # 3) äºˆç®—é…åˆ†ï¼ˆå‡ç­‰å‰²ã‚Šï¼‹ä½™ã‚Šã‚’ pending ãŒå¤šã„ã‚µã‚¤ãƒˆã‹ã‚‰åŠ ç®—ï¼‰
    targets = [sid for sid in site_ids if pending_map.get(sid, 0) > 0]
    if not targets:
        targets = []  # å¿µã®ãŸã‚
    n = len(targets)
    budget = max(0, int(limit_posts or 0))
    base = budget // n if n else 0
    rem  = budget % n if n else 0

    # ä½™ã‚Šã¯ pending å¤šã„é †ã« +1
    targets_sorted = sorted(targets, key=lambda sid: pending_map.get(sid, 0), reverse=True)
    allocation = {sid: 0 for sid in site_ids}
    for sid in targets_sorted:
        allocation[sid] = base
    for i in range(rem):
        allocation[targets_sorted[i]] += 1

    # 4) ã‚µã‚¤ãƒˆã”ã¨ã«å®Ÿè¡Œï¼ˆä¸Šé™ã¯ pending_posts ã‚’è¶…ãˆãªã„ï¼‰
    total = {"applied": 0, "swapped": 0, "skipped": 0, "processed_posts": 0}
    breakdown: List[Dict[str, object]] = []
    for sid in site_ids:
        pending_posts = pending_map.get(sid, 0)
        alloc = min(allocation.get(sid, 0), pending_posts)
        if alloc <= 0:
            breakdown.append({
                "site_id": sid,
                "allocated_posts": 0,
                "pending_posts": pending_posts,
                "result": {"applied":0,"swapped":0,"skipped":0,"processed_posts":0}
            })
            continue
        res = apply_actions_for_site(sid, limit_posts=alloc, dry_run=dry_run)
        # é›†è¨ˆ
        total["applied"] += int(res.get("applied", 0))
        total["swapped"] += int(res.get("swapped", 0))
        total["skipped"] += int(res.get("skipped", 0))
        total["processed_posts"] += int(res.get("processed_posts", 0))
        breakdown.append({
            "site_id": sid,
            "allocated_posts": alloc,
            "pending_posts": pending_posts,
            "result": res
        })

    total.update({"pending_total": pending_total, "site_breakdown": breakdown})
    return total