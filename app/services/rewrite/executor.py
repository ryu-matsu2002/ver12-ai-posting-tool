# app/services/rewrite/executor.py
# ãƒªãƒ©ã‚¤ãƒˆå®Ÿè¡Œã®å¸ä»¤å¡”ï¼ˆå®‰å…¨è¨­è¨ˆï¼šãƒªãƒ³ã‚¯å®Œå…¨ä¿è­· + ç›£æŸ»ãƒ­ã‚° + ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³æ—¢å®šï¼‰

import os
import re
import json
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import statistics

from flask import current_app
from sqlalchemy import select, text
from sqlalchemy.orm import joinedload, selectinload
from urllib.parse import urlparse
from openai import OpenAI, BadRequestError

from app import db
from app.models import (
    Article,
    Site,
    ArticleRewritePlan,
    ArticleRewriteLog,
    GSCUrlStatus,
    GSCMetric,
    SerpOutlineCache,
    TokenUsageLog,
)
from app.wp_client import (
    fetch_single_post,
    update_post_content,
    resolve_wp_post_id,
    update_post_meta,
)

# === OpenAI è¨­å®šï¼ˆarticle_generator.py ã¨åŒã˜æµå„€ï¼‰ ===
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY", ""))
MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

TOKENS = {
    "policy": 1200,     # æ–¹é‡ãƒ†ã‚­ã‚¹ãƒˆ
    "rewrite": 3600,    # æœ¬æ–‡ãƒªãƒ©ã‚¤ãƒˆ
    "summary": 400,     # diff æ¦‚è¦
}
TEMP = {
    "policy": 0.4,
    "rewrite": 0.5,
    "summary": 0.2,
}
TOP_P = 0.9
CTX_LIMIT = 12000
SHRINK = 0.85

META_MAX = 180  # ãƒ¡ã‚¿èª¬æ˜æœ€å¤§é•·ï¼ˆwp_clientã®ãƒãƒªã‚·ãƒ¼ã¨æ•´åˆï¼‰

# ========== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆarticle_generator.py ã¨åŒç³»ã®æŒ¯ã‚‹èˆã„ï¼‰ ==========

def _tok(s: str) -> int:
    return int(len(s) / 1.8)

def _chat(msgs: List[Dict[str, str]], max_t: int, temp: float, user_id: Optional[int] = None) -> str:
    used = sum(_tok(m.get("content", "")) for m in msgs)
    available = CTX_LIMIT - used - 16
    max_t = min(max_t, max(1, available))
    if max_t < 1:
        raise ValueError("Calculated max_tokens is below minimum.")

    def _call(m: int) -> str:
        res = client.chat.completions.create(
            model=MODEL,
            messages=msgs,
            max_tokens=m,
            temperature=temp,
            top_p=TOP_P,
            timeout=120,
        )

        # TokenUsageLogï¼ˆå¯èƒ½ãªã‚‰ä¿å­˜ï¼‰
        try:
            if hasattr(res, "usage") and user_id:
                usage = res.usage
                log = TokenUsageLog(
                    user_id=user_id,
                    prompt_tokens=getattr(usage, "prompt_tokens", 0),
                    completion_tokens=getattr(usage, "completion_tokens", 0),
                    total_tokens=getattr(usage, "total_tokens", 0),
                )
                db.session.add(log)
                db.session.commit()
        except Exception as e:
            logging.warning(f"[rewrite/_chat] ãƒˆãƒ¼ã‚¯ãƒ³ãƒ­ã‚°ä¿å­˜å¤±æ•—: {e}")

        content = (res.choices[0].message.content or "").strip()
        finish = res.choices[0].finish_reason
        if finish == "length":
            logging.warning("âš ï¸ OpenAI response was cut off due to max_tokens.")
        return content

    try:
        return _call(max_t)
    except BadRequestError as e:
        if "max_tokens" in str(e):
            retry_t = max(1, int(max_t * SHRINK))
            return _call(retry_t)
        raise
def _safe_json_loads(s: str):
    try:
        return json.loads(s)
    except Exception:
        return None

def _unique_urls(outlines: List[Dict], limit: int = 10) -> List[str]:
    seen = set()
    results: List[str] = []
    for o in outlines or []:
        u = (o or {}).get("url")
        if not u:
            continue
        if u in seen:
            continue
        seen.add(u)
        results.append(u)
        if len(results) >= limit:
            break
    return results    

def _take_sources_with_titles(outlines: List[Dict], limit: int = 8) -> List[Dict]:
    """
    å‚ç…§å…ƒã‚’äººé–“å¯èª­ã«ï¼ˆurl, title, snippetï¼‰ã§æœ€å¤§ limit ä»¶ã€‚
    title/snippet ãŒç„¡ã„ã‚‚ã®ã¯å¾Œæ–¹äº’æ›ã¨ã—ã¦ url ã®ã¿ã§å‡ºã™ã€‚
    """
    out: List[Dict] = []
    seen = set()
    for o in outlines or []:
        u = (o or {}).get("url")
        if not u or u in seen:
            continue
        seen.add(u)
        out.append({
            "url": u,
            "title": (o or {}).get("title") or "",
            "snippet": (o or {}).get("snippet") or ""
        })
        if len(out) >= limit:
            break
    return out


# ========== åé›†ãƒ•ã‚§ãƒ¼ã‚ºï¼šææ–™é›†ã‚ ==========

def _collect_wp_html(site: Site, article: Article) -> Tuple[Optional[int], Optional[str]]:
    """
    WPä¸Šã®æœ€æ–°æœ¬æ–‡HTMLã‚’å–å¾—ã€‚æˆ»ã‚Šå€¤: (wp_post_id, content_html or None)
    """
    wp_id = article.wp_post_id
    if not wp_id:
        wp_id = resolve_wp_post_id(site, article, save=True)

    if not wp_id:
        return None, None

    post = fetch_single_post(site, wp_id)
    if post and post.content_html:
        return wp_id, post.content_html
    return wp_id, None


def _collect_gsc_snapshot(site_id: int, article: Article) -> Dict:
    """
    GSCã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ³ã¨æœ€è¿‘ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è»½ãã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã€‚
    ç„¡ã‘ã‚Œã°ç©ºæ§‹é€ ã‚’è¿”ã™ï¼ˆLLMã«â€œç„¡ã„â€ã“ã¨ã‚’ä¼ãˆã‚‹ï¼‰ã€‚
    """
    snap: Dict = {"url_status": None, "metrics_recent": []}
    try:
        # URL Inspection ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆæœ€æ–°1ä»¶ï¼‰
        if article.posted_url:
            s = (
                db.session.query(GSCUrlStatus)
                .filter(GSCUrlStatus.site_id == site_id, GSCUrlStatus.url == article.posted_url)
                .order_by(GSCUrlStatus.updated_at.desc())
                .first()
            )
            if s:
                snap["url_status"] = {
                    "indexed": s.indexed,
                    "coverage_state": s.coverage_state,
                    "verdict": s.verdict,
                    "last_crawl_time": s.last_crawl_time.isoformat() if s.last_crawl_time else None,
                    "robots_txt_state": s.robots_txt_state,
                    "page_fetch_state": s.page_fetch_state,
                    "last_inspected_at": s.last_inspected_at.isoformat() if s.last_inspected_at else None,
                }

        # ç›´è¿‘ã®GSCãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆãã®è¨˜äº‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿‘å‚ã§æŠ½å‡ºâ€¦æœ€ä½é™ã¯åŒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
        if article.keyword:
            rows = (
                db.session.query(GSCMetric)
                .filter(GSCMetric.site_id == site_id, GSCMetric.user_id == article.user_id, GSCMetric.query == article.keyword)
                .order_by(GSCMetric.date.desc())
                .limit(28)
                .all()
            )
            for r in rows:
                snap["metrics_recent"].append({
                    "date": r.date.isoformat(),
                    "impressions": r.impressions,
                    "clicks": r.clicks,
                    "ctr": r.ctr,
                    "position": r.position,
                })
    except Exception as e:
        logging.info(f"[rewrite/_collect_gsc_snapshot] skipped: {e}")
    return snap


def _collect_serp_outline(article: Article) -> List[Dict]:
    """
    ç«¶åˆè¦‹å‡ºã—ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ã‚’å–ã‚Šå‡ºã™ã€‚ç„¡ã‘ã‚Œã°ç©ºé…åˆ—ã€‚
    """
    try:
        q = (
            db.session.query(SerpOutlineCache)
            .filter(SerpOutlineCache.article_id == article.id)
            .order_by(SerpOutlineCache.fetched_at.desc())
        )
        rec = q.first()
        if rec and rec.outlines:
            return rec.outlines
    except Exception as e:
        logging.info(f"[rewrite/_collect_serp_outline] skipped: {e}")
    return []


# ========== ç«¶åˆæ§‹é€ ã‚·ã‚°ãƒŠãƒ«ã®è¦ç´„ï¼ˆFAQ/HowTo/è¡¨/èªæ•°ãªã©ï¼‰ ==========

def _summarize_serp_signals(outlines: List[Dict]) -> Dict:
    """
    serp_collector ãŒä¿å­˜ã—ãŸ signals/schema/intro ã‚’é›†è¨ˆã—ã¦ã€
    - must_add_sections_suggested: ["FAQ","HowTo","Table"] ã®ã‚ˆã†ãªâ€œå‹â€ææ¡ˆ
    - word_count_stats: {"median": x, "p75": y}
    - estimated_length_range: "2500-3500" ã®ã‚ˆã†ãªæ–‡å­—æ•°ãƒ¬ãƒ³ã‚¸ä»®èª¬
    ã‚’è¿”ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãŒç„¡ã‘ã‚Œã°ç©ºãƒ™ãƒ¼ã‚¹ã€‚
    """
    if not outlines:
        return {
            "must_add_sections_suggested": [],
            "word_count_stats": {},
            "estimated_length_range": None
        }
    has_faq = 0
    has_howto = 0
    has_table = 0
    wcounts: List[int] = []
    for o in outlines:
        schema = (o or {}).get("schema") or []
        sig = (o or {}).get("signals") or {}
        if "FAQ" in schema or sig.get("has_faq"):
            has_faq += 1
        if "HowTo" in schema or sig.get("has_howto"):
            has_howto += 1
        if sig.get("has_table"):
            has_table += 1
        wc = sig.get("word_count")
        if isinstance(wc, int) and wc > 0:
            wcounts.append(wc)
    n = max(1, len(outlines))
    suggest: List[str] = []
    # â€œåŠæ•°ä»¥ä¸ŠãŒæ¡ç”¨ã—ã¦ã„ã‚‹å‹â€ã¯ç©æ¥µææ¡ˆ
    if has_faq >= (n // 2 + n % 2):
        suggest.append("FAQ")
    if has_howto >= (n // 2 + n % 2):
        suggest.append("HowTo")
    if has_table >= (n // 2 + n % 2):
        suggest.append("Table")
    stats = {}
    est = None
    try:
        if wcounts:
            med = int(statistics.median(wcounts))
            p75 = int(statistics.quantiles(wcounts, n=4)[2]) if len(wcounts) >= 4 else med
            stats = {"median": med, "p75": p75}
            # æ—¥æœ¬èªã®ã ã„ãŸã„ã®æ–‡å­—æ•°=å˜èªæ•°Ã—1.8ï¼ˆ_tokã®é€†è¿‘ä¼¼ï¼‰ã‚’ä½¿ã„ã€ç¯„å›²ã«ä¸¸ã‚ã‚‹
            def _chars(words: int) -> int:
                return int(words * 1.8)
            low = max(1200, int(_chars(med) * 0.85))
            high = int(_chars(p75) * 1.15)
            # 500åˆ»ã¿ç¨‹åº¦ã«ä¸¸ã‚ã¦è¦‹ã‚„ã™ã
            def _round_500(x: int) -> int:
                return int(round(x / 500.0) * 500)
            est = f"{_round_500(low)}-{_round_500(high)}"
    except Exception:
        pass
    return {
        "must_add_sections_suggested": suggest,
        "word_count_stats": stats,
        "estimated_length_range": est
    }

# ========== ç«¶åˆã¨ã®ã‚®ãƒ£ãƒƒãƒ—åˆ†æ & ãƒ­ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿æ•´å½¢ ==========

_H2_RE = re.compile(r"<h2[^>]*>(.*?)</h2>", flags=re.IGNORECASE | re.DOTALL)
_H3_RE = re.compile(r"<h3[^>]*>(.*?)</h3>", flags=re.IGNORECASE | re.DOTALL)

def _strip_tags(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"<[^>]+>", "", s)
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def _extract_my_headings(html: str) -> Dict[str, List[str]]:
    """
    è‡ªè¨˜äº‹ã®H2/H3ã‚’æŠ½å‡ºï¼ˆãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼‰ã€‚
    """
    h2s = [_strip_tags(m.group(1)) for m in _H2_RE.finditer(html or "")]
    h3s = [_strip_tags(m.group(1)) for m in _H3_RE.finditer(html or "")]
    # ç©ºæ–‡å­—ã‚’é™¤å»
    h2s = [x for x in h2s if x]
    h3s = [x for x in h3s if x]
    return {"h2": h2s, "h3": h3s}

def _analyze_gaps(original_html: str, outlines: List[Dict]) -> Tuple[Dict, List[str], Dict]:
    """
    ç«¶åˆã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ï¼ˆ[{url, h:[...], notes?...}]ï¼‰ã‹ã‚‰
    - å‚ç…§URLä¸€è¦§
    - è‡ªè¨˜äº‹ã«ä¸è¶³ã—ã¦ã„ãã†ãªè¦‹å‡ºã—å€™è£œ
    - è£œåŠ©çµ±è¨ˆï¼ˆé »å‡ºä¸Šä½ãƒ†ãƒ¼ãƒãªã©ï¼‰
    ã‚’è¿”ã™ã€‚outlines ãŒç©ºãªã‚‰ç©ºã®çµæœã‚’è¿”ã™ã€‚
    """
    referenced_urls = []
    comp_h2_counts = {}
    comp_h3_counts = {}

    for o in outlines or []:
        url = o.get("url")
        if url:
            referenced_urls.append(url)
        hs = o.get("h") or []
        for h in hs:
            t = _strip_tags(h or "")
            # H2/H3ã£ã½ã„ç²’åº¦ã ã‘ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆH1ã‚„é›‘å¤šã¯é™¤å¤–ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰
            # æ—¢ã«æ§‹é€ åŒ–æ¸ˆã¿ãªã‚‰ "H2: xxx" å½¢å¼ã‚’æƒ³å®šã€ãƒ—ãƒ¬ãƒ¼ãƒ³ãªã‚‰ãã®ã¾ã¾æ‰±ã†
            key = t
            if not key:
                continue
            # ç°¡æ˜“ã«â€œé•·ã‚ã®è¦‹å‡ºã—â€ã‚’å„ªå…ˆã—ã¦å­¦ç¿’ï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰
            if len(key) < 2:
                continue
            comp_h2_counts[key] = comp_h2_counts.get(key, 0) + 1

    mine = _extract_my_headings(original_html or "")
    my_set = set(mine["h2"] + mine["h3"])

    # é »åº¦é †ã§â€œè‡ªè¨˜äº‹ã«ç„¡ã„è¦‹å‡ºã—â€ã‚’å€™è£œã«
    sorted_h2 = sorted(comp_h2_counts.items(), key=lambda x: (-x[1], x[0]))[:30]
    missing = [h for h, c in sorted_h2 if h not in my_set][:15]

    stats = {
        "top_competitor_headings": [{"heading": h, "freq": c} for h, c in sorted_h2[:10]],
        "my_h2": mine["h2"][:20],
        "my_h3": mine["h3"][:20],
    }
    return stats, referenced_urls, {"missing_headings": missing}

#ï¼ˆé‡è¤‡å®šç¾©ãŒå¾Œã‚ã«ã‚ã‚‹ãŸã‚ã€ã“ã®ç‰ˆã¯å‰Šé™¤ï¼‰

# ========== ã‚®ãƒ£ãƒƒãƒ—åˆ†æï¼ˆSERP Ã— ç¾æœ¬æ–‡ â†’ è¿½åŠ ã™ã¹ãé …ç›®ã®æ§‹é€ åŒ–ï¼‰ ==========

def _build_gap_analysis(article: Article, original_html: str, outlines: List[Dict], gsc_snapshot: Dict) -> Tuple[Dict, str]:
    """
    å‚ç…§SERPï¼ˆè¦‹å‡ºã—éª¨å­ï¼‰ã¨ç¾è¡Œæœ¬æ–‡ã‚’æ¯”è¼ƒã—ã¦ã€â€œä¸è¶³/æ”¹å–„â€ã‚’æ§‹é€ åŒ–JSON + ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã§è¿”ã™ã€‚
    - æˆ»ã‚Š: (gap_summary_json, checklist_text)
      gap_summary_json ä¾‹:
        {
          "missing_topics": ["æ–™é‡‘æ¯”è¼ƒ", "åŠ¹æœã®ç›®å®‰(æœŸé–“ãƒ»å›æ•°)"],
          "must_add_sections": ["FAQ", "ä½“é¨“è«‡/äº‹ä¾‹"],
          "quality_issues": ["å°å…¥ãŒæŠ½è±¡çš„", "çµè«–ãŒæ›–æ˜§"],
          "estimated_length_range": "2500-3500"
        }
    """
    # æœ¬æ–‡ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ï¼‰
    current_text = _strip_html_min(original_html)[:3500]
    # SERPè¦ç´„ï¼ˆå†—é•·å›é¿ã®ãŸã‚H2/H3ä¸­å¿ƒï¼‰ï¼‹ ç«¶åˆã®æ§‹é€ ã‚·ã‚°ãƒŠãƒ«
    compact_outlines = []
    for o in (outlines or [])[:8]:
        compact_outlines.append({
            "url": o.get("url"),
            "h": (o.get("h") or [])[:30],
            "notes": o.get("notes", "")[:200]
        })
    serp_signals = _summarize_serp_signals(outlines)

    sys = (
        "ã‚ãªãŸã¯æ—¥æœ¬èªSEOã®ç·¨é›†é•·ã§ã™ã€‚ä»¥ä¸‹ã®ææ–™ã‹ã‚‰â€œä½•ãŒä¸è¶³ã‹/ä½•ã‚’è¶³ã™ã¹ãã‹â€ã‚’æ§‹é€ åŒ–ã—ã¦è¿”ã—ã¦ãã ã•ã„ã€‚"
        "è¿”ç­”ã¯å³å¯†ãªJSONã®ã¿ï¼ˆå‰å¾Œã«ä½™è¨ˆãªæ–‡å­—ã‚’å…¥ã‚Œãªã„ï¼‰ã€‚"
        "ã‚­ãƒ¼ã¯ missing_topics(é…åˆ—), must_add_sections(é…åˆ—), quality_issues(é…åˆ—), estimated_length_range(æ–‡å­—åˆ—)ã€‚"
        "å†…éƒ¨ãƒªãƒ³ã‚¯ã‚„æ–°è¦ãƒªãƒ³ã‚¯ææ¡ˆã¯ä¸€åˆ‡ç¦æ­¢ã€‚"
    )
    usr = json.dumps({
        "article": {"id": article.id, "title": article.title, "keyword": article.keyword},
        "gsc": gsc_snapshot,
        "current_excerpt": current_text,
        "serp_outlines": compact_outlines,
        "serp_structure_signals": serp_signals
    }, ensure_ascii=False)
    raw = _chat(
        [{"role": "system", "content": sys}, {"role": "user", "content": usr}],
        TOKENS["policy"], TEMP["policy"], user_id=article.user_id
    )
    gap = _safe_json_loads(raw) or {}
    # LLMå‡ºåŠ›ã‚’â€œãƒ‡ãƒ¼ã‚¿é§†å‹•ã®ä»®èª¬â€ã§è£œå¼·ï¼ˆä¸è¶³ã—ã¦ã„ã‚Œã°åŸ‹ã‚ã‚‹ï¼é‡è¤‡ã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼‰
    try:
        ms = set((gap.get("must_add_sections") or []))
        for s in (serp_signals.get("must_add_sections_suggested") or []):
            ms.add(s)
        gap["must_add_sections"] = sorted(list(ms))
        if not gap.get("estimated_length_range") and serp_signals.get("estimated_length_range"):
            gap["estimated_length_range"] = serp_signals["estimated_length_range"]
    except Exception:
        pass
    # ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆæ–‡å­—åˆ—ã‚‚ï¼ˆUIè¡¨ç¤ºå‘ã‘ï¼‰
    checklist_lines: List[str] = []
    for k in ("missing_topics", "must_add_sections", "quality_issues"):
        vals = gap.get(k) or []
        if isinstance(vals, list) and vals:
            title = {
                "missing_topics": "ä¸è¶³ãƒˆãƒ”ãƒƒã‚¯",
                "must_add_sections": "è¿½åŠ å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³",
                "quality_issues": "å“è³ªèª²é¡Œ",
            }[k]
            checklist_lines.append(f"â– {title}")
            for v in vals:
                checklist_lines.append(f"- {v}")
    if gap.get("estimated_length_range"):
        checklist_lines.append(f"â– æ¨å¥¨æ–‡å­—é‡: {gap['estimated_length_range']}")
    checklist = "\n".join(checklist_lines) if checklist_lines else ""
    return gap, checklist

def _derive_templates_from_gsc(gsc_snapshot: Dict) -> List[str]:
    """
    GSCã®çŠ¶æ…‹ã‹ã‚‰ã€é©ç”¨ã—ãŸæ–½ç­–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®â€œã‚¹ãƒ©ã‚°â€ã‚’æ¨å®šã™ã‚‹è»½ã„é–¢æ•°ã€‚
    UIã§ã€ã©ã®æ–¹é‡ã‚’ä½¿ã£ãŸã‹ã€ã‚’è¦‹ã›ã‚‹ç›®çš„ã€‚
    """
    slugs: List[str] = []
    st = ((gsc_snapshot or {}).get("url_status") or {})
    cov = (st.get("coverage_state") or "").lower()
    if "discovered" in cov and "not indexed" in cov:
        slugs.append("coverage_discovered_not_indexed")
    if "crawled" in cov and "not indexed" in cov:
        slugs.append("coverage_crawled_not_indexed")
    if "alternate page" in cov:
        slugs.append("coverage_alternate_canonical")
    # CTR/é †ä½ãªã©ç°¡æ˜“æ¨å®š
    metrics = (gsc_snapshot or {}).get("metrics_recent") or []
    if metrics:
        # ã–ã£ãã‚Šæœ€è¿‘5ä»¶å¹³å‡
        last5 = metrics[:5]
        try:
            avg_pos = sum(m.get("position", 0) or 0 for m in last5) / max(1, len(last5))
            avg_ctr = sum(m.get("ctr", 0) or 0 for m in last5) / max(1, len(last5))
            if avg_pos <= 20 and avg_ctr < 0.01:
                slugs.append("low_ctr_ranked_20")
            if avg_pos > 30:
                slugs.append("low_visibility_ranked_30plus")
        except Exception:
            pass
    return slugs or None


# ========== ãƒªãƒ³ã‚¯å®Œå…¨ä¿è­·ï¼ˆç½®æ›â†’å¾©å…ƒï¼‰ ==========

_LINK_RE = re.compile(r"<a\b[^>]*>.*?</a>", flags=re.IGNORECASE | re.DOTALL)

def _mask_links(html: str) -> Tuple[str, Dict[str, str]]:
    """
    æœ¬æ–‡å†…ã® <a ...>...</a> ã‚’ [[LINK_i]] ã«ç½®æ›ã—ã€ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¿”ã™ã€‚
    å¾Œæ®µã®LLMã«ã¯ã€ã“ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸€åˆ‡æ”¹å¤‰ã—ãªã„ã‚ˆã†å³å‘½ã™ã‚‹ã€‚
    """
    mapping: Dict[str, str] = {}
    def _repl(m):
        idx = len(mapping)
        key = f"[[LINK_{idx}]]"
        mapping[key] = m.group(0)
        return key
    masked = _LINK_RE.sub(_repl, html or "")
    return masked, mapping

def _unmask_links(html: str, mapping: Dict[str, str]) -> str:
    if not html or not mapping:
        return html or ""
    out = html
    for k, v in mapping.items():
        out = out.replace(k, v)
    return out


# ========== ãƒ¡ã‚¿ç”Ÿæˆï¼ˆä»»æ„ãƒ»å®‰å…¨ãƒˆãƒªãƒ ï¼‰ ==========

def _strip_html_min(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"<(script|style)[^>]*>.*?</\1>", "", s, flags=re.I | re.S)
    s = re.sub(r"<[^>]+>", "", s)
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def _smart_truncate(s: str, limit: int = META_MAX) -> str:
    if not s:
        return ""
    if len(s) <= limit:
        return s.strip()
    cut = s[:limit]
    for sep in ["ã€‚", "ï¼", "ï¼", "ï¼Ÿ", "ã€", "ï¼Œ", " ", "ã€€"]:
        i = cut.rfind(sep)
        if i >= 60:
            cut = cut[:i]
            break
    return cut.strip()

def _gen_meta_from_body(title: str, body_html: str, user_id: Optional[int]) -> str:
    try:
        body_txt = _strip_html_min(body_html)[:1200]
        sys = "ã‚ãªãŸã¯æ—¥æœ¬èªã®SEOç·¨é›†è€…ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸè¨˜äº‹ã®è¦ç‚¹ã‚’ã€è‡ªç„¶ã§ã‚¯ãƒªãƒƒã‚¯ã‚’èª˜ç™ºã—ã‚„ã™ã„1æ–‡ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚èª‡å¼µã‚„æ–­å®šã¯é¿ã‘ã¾ã™ã€‚"
        usr = (
            f"åˆ¶ç´„:\n- {META_MAX}æ–‡å­—ä»¥å†…\n- æ–‡ä¸­ã§ä¸è‡ªç„¶ã«é€”åˆ‡ã‚Œãªã„\n- è¨˜å·è£…é£¾ã‚’ä½¿ã‚ãªã„\n\n"
            f"ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘\n{title}\n\nã€æœ¬æ–‡æŠœç²‹ã€‘\n{body_txt}\n"
        )
        meta = _chat(
            [{"role": "system", "content": sys}, {"role": "user", "content": usr}],
            TOKENS["summary"], TEMP["summary"], user_id=user_id
        )
        return _smart_truncate(_strip_html_min(meta), META_MAX)
    except Exception as e:
        logging.info(f"[rewrite/_gen_meta_from_body] skipped: {e}")
        return ""


# ========== æ–¹é‡ç”Ÿæˆ & æœ¬æ–‡ãƒªãƒ©ã‚¤ãƒˆ ==========

def _build_policy_text(article: Article, gsc: Dict, outlines: List[Dict], gap_summary: Optional[Dict] = None) -> str:
    """
    LLMã«ã€Œä½•ã‚’ãƒ»ã©ã“ã‚’ãƒ»ã©ã†ç›´ã™ã‹ã€ã®æ‰‹é †æ›¸ã‚’ä½œã‚‰ã›ã‚‹ã€‚
    â€» ã“ã“ã§ã¯HTMLã‚’æ›¸ã‹ã›ãªã„ã€‚ã‚ãã¾ã§â€œè¨­è¨ˆå›³â€ã€‚
    """
    sys = (
        "ã‚ãªãŸã¯æ—¥æœ¬èªSEOã®ç·¨é›†é•·ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸææ–™ï¼ˆGSCæŒ‡æ¨™ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ³ã€ç«¶åˆè¦‹å‡ºã—ï¼‰"
        "ã‹ã‚‰â€œãªãœä¼¸ã³ãªã„ã®ã‹â€ã‚’ä»®èª¬åŒ–ã—ã€ã©ã“ã‚’ã©ã†ç›´ã™ã‹ã®å®Ÿè¡Œæ‰‹é †ã‚’ä½œã£ã¦ãã ã•ã„ã€‚"
        "å‡ºåŠ›ã¯ç®‡æ¡æ›¸ããƒ™ãƒ¼ã‚¹ã§ã€è¦‹å‡ºã—æ§‹æˆãƒ»å°å…¥æ”¹å–„ãƒ»E-E-A-Tãƒ»FAQãƒ»ç”¨èªèª¬æ˜ãªã©å…·ä½“ç­–ã‚’å«ã‚ã¾ã™ã€‚"
        "å†…éƒ¨ãƒªãƒ³ã‚¯ã®è¿½åŠ ãƒ»å¤‰æ›´ãƒ»å‰Šé™¤ã¯ä¸€åˆ‡ææ¡ˆã—ãªã„ã§ãã ã•ã„ï¼ˆæ—¢å­˜ãƒªãƒ³ã‚¯ã¯å³ç¦ã§è§¦ã‚‰ãªã„ï¼‰ã€‚"
    )
    # ç«¶åˆã®æ§‹é€ å‚¾å‘ã‚’åŸºã«ã€å‡ºåŠ›ä»•æ§˜ã‚’æ¡ä»¶ä»˜ãã§æ˜ç¤º
    gap = gap_summary or {}
    must_sections = set((gap.get("must_add_sections") or []))
    length_hint = gap.get("estimated_length_range") or ""
    output_specs: List[str] = []
    if "FAQ" in must_sections:
        output_specs.append("FAQã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ï¼šH2é…ä¸‹ã§Qã‚’å¤ªå­—ã€Aã¯ç°¡æ½”ã€‚æœ€å¤§5å•ã€‚å†…éƒ¨ãƒªãƒ³ã‚¯ãƒ»å¤–éƒ¨ãƒªãƒ³ã‚¯ã¯è¿½åŠ ã—ãªã„ã€‚")
    if "HowTo" in must_sections:
        output_specs.append("HowToï¼ˆæ‰‹é †ï¼‰ã‚’è¿½åŠ ï¼šç•ªå·ä»˜ããƒªã‚¹ãƒˆã§3-7æ®µéšã€‚å„ã‚¹ãƒ†ãƒƒãƒ—1-2æ–‡ã€‚")
    if "Table" in must_sections:
        output_specs.append("æ¯”è¼ƒè¡¨ã‚’è¿½åŠ ï¼š<table>ã§åˆ—ã¯ã€é …ç›®/èª¬æ˜/ç›®å®‰ã€ã®3åˆ—ã‚’åŸºæœ¬ã€‚")
    if length_hint:
        output_specs.append(f"æœ¬æ–‡ã®ç·é‡ã¯æ¦‚ã­ {length_hint} æ–‡å­—å¸¯ã‚’ç›®å®‰ï¼ˆéåº¦ã«ç››ã‚‰ãªã„ï¼‰ã€‚")

    usr = json.dumps({
        "article": {"id": article.id, "title": article.title, "keyword": article.keyword, "url": article.posted_url},
        "gsc_snapshot": gsc,
        "serp_outlines": outlines[:8],  # å†—é•·å›é¿
        "rewrite_specs": output_specs
    }, ensure_ascii=False, indent=2)
    return _chat(
        [{"role": "system", "content": sys}, {"role": "user", "content": usr}],
        TOKENS["policy"], TEMP["policy"], user_id=article.user_id
    )

def _rewrite_html(original_html: str, policy_text: str, user_id: Optional[int]) -> str:
    """
    æœ¬æ–‡ãƒªãƒ©ã‚¤ãƒˆï¼ˆãƒªãƒ³ã‚¯å®Œå…¨ä¿è­·ï¼‰ã€‚<a> ã¯ã™ã¹ã¦ [[LINK_i]] ã«ç½®æ›ã—ã€LLMã¸ã€‚
    æˆ»ã‚Šã§ [[LINK_i]] ã‚’å³å¯†å¾©å…ƒã™ã‚‹ã€‚
    """
    masked, mapping = _mask_links(original_html or "")

    sys = (
        "ã‚ãªãŸã¯æ—¥æœ¬èªSEOã®ç·¨é›†è€…ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸâ€œä¿®æ­£æ–¹é‡â€ã«å¾“ã„ã€HTMLæœ¬æ–‡ã‚’ç·¨é›†ã—ç›´ã—ã¦ãã ã•ã„ã€‚"
        "é‡è¦: ä»¥ä¸‹ã‚’å³å®ˆã—ã¦ãã ã•ã„ã€‚\n"
        "1) [[LINK_i]] ã¨ã„ã†ãƒˆãƒ¼ã‚¯ãƒ³ã¯çµ¶å¯¾ã«å¤‰æ›´ãƒ»å‰Šé™¤ãƒ»é †åºå…¥æ›¿ã‚’ã—ãªã„ã“ã¨ï¼ˆãã®ã¾ã¾å‡ºåŠ›ã«æ®‹ã™ï¼‰\n"
        "2) å…ƒã®æœ¬æ–‡ã«å­˜åœ¨ã—ãªã„æ–°ã—ã„ãƒã‚¤ãƒ‘ãƒ¼ãƒªãƒ³ã‚¯ã‚’è¿½åŠ ã—ãªã„ã“ã¨\n"
        "3) æ—¢å­˜ã®è¦‹å‡ºã—éšå±¤ã¯æ¦‚ã­ç¶­æŒã—ã¤ã¤ã€å°å…¥ãƒ»ã¾ã¨ã‚ãƒ»FAQãªã©ã‚’æ”¹å–„ã—ã¦ã‚ˆã„\n"
        "4) äº‹å®Ÿã«åŸºã¥ãã€èª‡å¼µãƒ»æ–­å®šã‚’é¿ã‘ã‚‹\n"
        "5) å‡ºåŠ›ã¯HTMLæ–­ç‰‡ã®ã¿ã€‚<html>ã‚„<body>ã¯å«ã‚ãªã„\n"
    )
    usr = (
        "=== ä¿®æ­£æ–¹é‡ ===\n"
        f"{policy_text}\n\n"
        "=== ç·¨é›†å¯¾è±¡ï¼ˆãƒªãƒ³ã‚¯ã¯ [[LINK_i]] ã«ç½®æ›æ¸ˆã¿ï¼‰ ===\n"
        f"{masked}\n"
    )

    edited = _chat(
        [{"role": "system", "content": sys}, {"role": "user", "content": usr}],
        TOKENS["rewrite"], TEMP["rewrite"], user_id=user_id
    )

    # å¾©å…ƒ
    return _unmask_links(edited, mapping)

def _same_domain(site_url: str, posted_url: str) -> bool:
    """
    ã‚¶ãƒƒã‚¯ãƒªæ¯”è¼ƒï¼šãƒ›ã‚¹ãƒˆåã®æœ«å°¾ä¸€è‡´ã§åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨ã¿ãªã™ã€‚
    ä¾‹: roof-pilates.com ã¨ www.roof-pilates.com ã¯åŒä¸€æ‰±ã„ã€‚
    livedoor.blog ãªã©åˆ¥ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯ falseã€‚
    """
    if not site_url or not posted_url:
        return False
    try:
        s = urlparse(site_url).hostname or ""
        p = urlparse(posted_url).hostname or ""
        s = s.lower().lstrip("www.")
        p = p.lower().lstrip("www.")
        return p.endswith(s)
    except Exception:
        return False
# ========== ãƒ¡ã‚¤ãƒ³ï¼š1ä»¶å®Ÿè¡Œ ==========

def execute_one_plan(*, user_id: int, plan_id: Optional[int] = None, dry_run: bool = True) -> Dict:
    """
    1ä»¶ã®ãƒªãƒ©ã‚¤ãƒˆè¨ˆç”»ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    - dry_run=True: WPæ›´æ–°ã—ãªã„ï¼ˆæ–¹é‡ã¨å·®åˆ†ã®ç”Ÿæˆãƒ»ãƒ­ã‚°ã ã‘ï¼‰
    - dry_run=False: WPã«æ›´æ–°åæ˜ ã¾ã§è¡Œã†
    æˆ»ã‚Šå€¤ã¯çµæœã®ã‚µãƒãƒªãƒ¼è¾æ›¸ã€‚
    """
    app = current_app._get_current_object()
    with app.app_context():
        # 1) ã¾ãš ID ã ã‘ã‚’ FOR UPDATE SKIP LOCKED ã§å–å¾—ï¼ˆJOINã—ãªã„ï¼‰
        id_q = db.session.query(ArticleRewritePlan.id).filter(
            ArticleRewritePlan.user_id == user_id,
            ArticleRewritePlan.is_active.is_(True),
            ArticleRewritePlan.status == "queued",
        ).order_by(
            ArticleRewritePlan.priority_score.desc(),
            ArticleRewritePlan.created_at.asc(),
        )
        if plan_id:
            id_q = id_q.filter(ArticleRewritePlan.id == plan_id)

        # Postgresã«ã€Œã©ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ­ãƒƒã‚¯ã™ã‚‹ã‹ã€ã‚’æ˜ç¤º
        id_q = id_q.with_for_update(skip_locked=True, of=ArticleRewritePlan)

        target_id = id_q.limit(1).scalar()
        if not target_id:
            return {"status": "empty", "message": "å®Ÿè¡Œå¯èƒ½ãª queued è¨ˆç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}

        # 2) å–å¾—ã—ãŸIDã§é–¢é€£ã‚’åˆ¥ã‚¯ã‚¨ãƒªã§ãƒ­ãƒ¼ãƒ‰ï¼ˆã“ã®ã‚¯ã‚¨ãƒªã«ã¯ãƒ­ãƒƒã‚¯ä¸è¦ï¼‰
        plan = db.session.query(ArticleRewritePlan).options(
            selectinload(ArticleRewritePlan.article),
            selectinload(ArticleRewritePlan.site),
        ).get(target_id)
        if not plan:
            return {"status": "empty", "message": f"ID={target_id} ã®è¨ˆç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}

        plan.status = "running"
        plan.started_at = datetime.utcnow()
        plan.attempts = (plan.attempts or 0) + 1
        db.session.commit()

        article: Article = plan.article
        site: Site = plan.site

        # 2) ãƒ‰ãƒ¡ã‚¤ãƒ³ä¸ä¸€è‡´ã®å®‰å…¨ã‚¬ãƒ¼ãƒ‰
        #    ä¾‹: site.url=roof-pilates.com ã ãŒ posted_url ãŒ livedoor.blog â†’ WPæ›´æ–°å¯¾è±¡å¤–
        if article.posted_url and not _same_domain(site.url, article.posted_url):
            reason = f"domain_mismatch: site={site.url} posted={article.posted_url}"
            current_app.logger.info(f"[rewrite] skip plan_id={plan.id} ({reason})")
            # dry_run ã§ã‚‚ 'running' ã®ã¾ã¾ã«ã—ãªã„ã‚ˆã†çµ‚ç«¯åŒ–
            plan.finished_at = datetime.utcnow()
            if dry_run:
                plan.status = "done"  # æ—¢å­˜ã® dry_run çµ‚äº†ã¨åŒç­‰ã®æ‰±ã„ã«å¯„ã›ã‚‹
                db.session.commit()
                return {
                    "status": "skipped(dry)",
                    "reason": reason,
                    "plan_id": plan.id,
                    "article_id": article.id,
                    "wp_post_id": None,
                }
            else:
                # æœ¬å®Ÿè¡Œã§ã¯è¨ˆç”»ã‚’ã‚¨ãƒ©ãƒ¼çµ‚äº†ï¼ˆç„¡åŠ¹åŒ–ã¯ã“ã“ã§ã¯è¡Œã‚ãªã„ï¼šæ‰‹å‹•åˆ¤æ–­ã®ä½™åœ°ã‚’æ®‹ã™ï¼‰
                plan.status = "error"
                plan.last_error = reason
                db.session.commit()
                return {
                    "status": "skipped",
                    "reason": reason,
                    "plan_id": plan.id,
                    "article_id": article.id,
                    "wp_post_id": None,
                }

        # 3) ææ–™åé›†
        wp_post_id, wp_html = _collect_wp_html(site, article)
        original_html = wp_html or (article.body or "")
        if not original_html:
            plan.status = "error"
            plan.last_error = "æœ¬æ–‡ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆWP/DBã¨ã‚‚ç©ºï¼‰"
            plan.finished_at = datetime.utcnow()
            db.session.commit()
            return {"status": "error", "message": plan.last_error, "plan_id": plan.id}

        gsc_snap = _collect_gsc_snapshot(site.id, article)
        outlines = _collect_serp_outline(article)

        # 3.5) SERP Ã— ç¾æœ¬æ–‡ã®ã‚®ãƒ£ãƒƒãƒ—åˆ†æï¼ˆä¸è¶³/è¿½åŠ ã‚»ã‚¯ã‚·ãƒ§ãƒ³/å“è³ªèª²é¡Œ ãªã©ã‚’æ§‹é€ åŒ–ï¼‰
        gap_summary_json, policy_checklist = _build_gap_analysis(article, original_html, outlines, gsc_snap)
        used_templates = _derive_templates_from_gsc(gsc_snap)
        referenced_urls = _unique_urls(outlines, limit=10)
        referenced_count = len(referenced_urls)
        referenced_sources = _take_sources_with_titles(outlines, limit=8)

        # 4) æ–¹é‡ä½œæˆ
        policy_text = _build_policy_text(article, gsc_snap, outlines, gap_summary_json)
        # äººãŒä¸€è¦§ã§åˆ¤æ–­ã—ã‚„ã™ã„ã‚ˆã†ã€å‚ç…§ä»¶æ•°ãƒ»URLãƒ»ä¸è¶³ã®è¦ç‚¹ã‚’æ–¹é‡æœ«å°¾ã«è¿½è¨˜
        try:
            missing_topics = (gap_summary_json or {}).get("missing_topics") or []
            _mt_head = "ã€".join(missing_topics[:5])
            url_lines = "\n".join(referenced_urls[:8])
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚‚æ·»ãˆã¦äººé–“å¯èª­ã«ï¼ˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã¯ãƒã‚¤ã‚ºã‚’é¿ã‘ã¦çœç•¥/ä»»æ„ï¼‰
            title_lines = "\n".join(
                [f"- {s.get('title') or '(no title)'}\n  {s.get('url')}" for s in referenced_sources]
            )
            policy_text = (
                f"{policy_text}\n\n"
                f"---\n"
                f"ã€å‚ç…§SERPä»¶æ•°ã€‘{referenced_count}\n"
                f"ã€å‚ç…§URLã€‘\n{url_lines}\n"
                f"ã€å‚ç…§ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆæŠœç²‹ï¼‰ã€‘\n{title_lines}\n"
                f"ã€ä¸è¶³ãƒˆãƒ”ãƒƒã‚¯ï¼ˆè¦ç‚¹ï¼‰ã€‘{_mt_head if _mt_head else 'â€”'}\n"
            )
        except Exception:
            pass

        # 4.5)ï¼ˆå‰Šé™¤ï¼‰â€” ä»¥é™ã¯ _build_gap_analysis ã®çµæœã‚’ãã®ã¾ã¾ä½¿ã†

        # 5) æœ¬æ–‡ãƒªãƒ©ã‚¤ãƒˆï¼ˆãƒªãƒ³ã‚¯ä¿è­·ï¼‰
        edited_html = _rewrite_html(original_html, policy_text, user_id=article.user_id)

        # 6) ç›£æŸ»ãƒ­ã‚°ï¼ˆå·®åˆ†è¦ç´„ã‚’LLMã§è¦ç´„ï¼‰
        sys = "ã‚ãªãŸã¯æ—¥æœ¬èªã®ç·¨é›†è€…ã§ã™ã€‚ä¿®æ­£å‰å¾Œã®æœ¬æ–‡ã®é•ã„ã‚’ç®‡æ¡æ›¸ãã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚å…·ä½“çš„ã«ã€‚"
        usr = f"ã€ä¿®æ­£å‰ã€‘\n{_strip_html_min(original_html)[:3000]}\n\nã€ä¿®æ­£å¾Œã€‘\n{_strip_html_min(edited_html)[:3000]}"
        diff_summary = _chat(
            [{"role": "system", "content": sys}, {"role": "user", "content": usr}],
            TOKENS["summary"], TEMP["summary"], user_id=article.user_id
        )

        # 7) ãƒ­ã‚°ä¿å­˜ï¼ˆWPçµæœã¯å¾Œã§ä¸Šæ›¸ãï¼‰
        log = ArticleRewriteLog(
            user_id=article.user_id,
            site_id=site.id,
            article_id=article.id,
            plan_id=plan.id,
            policy_text=policy_text,
            diff_summary=diff_summary,
            snapshot_before=original_html,
            snapshot_after=edited_html if dry_run else None,  # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³æ™‚ã®ã¿â€œäºˆå®šâ€ã¨ã—ã¦æ®‹ã™
            wp_status="unknown",
            wp_post_id=wp_post_id,
            executed_at=datetime.utcnow(),
            # ğŸ”½ ä»Šå›è¿½åŠ ã®ç›£æŸ»æƒ…å ±
            referenced_count=referenced_count,
            referenced_urls=referenced_urls,
            gap_summary=gap_summary_json,
            policy_checklist=policy_checklist,
            used_templates=used_templates,
        )
        db.session.add(log)
        db.session.commit()

        wp_ok = False
        wp_err = None

        # 8) WPæ›´æ–°ï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã˜ã‚ƒãªã‘ã‚Œã°åæ˜ ï¼‰
        if not dry_run:
            try:
                # æ—¢å­˜ã® ai-content ãƒ©ãƒƒãƒ‘ãŒã‚ã‚‹å ´åˆã¯å°Šé‡ï¼ˆç„¡ã‘ã‚Œã°ãã®ã¾ã¾ï¼‰
                if '<div class="ai-content">' in edited_html:
                    new_html = edited_html
                else:
                    new_html = f'<div class="ai-content">{edited_html}</div>'

                wp_ok = update_post_content(site, wp_post_id, new_html) if wp_post_id else False

                # ä»»æ„ï¼šãƒ¡ã‚¿èª¬æ˜ã‚’å®‰å…¨ã«ç”Ÿæˆãƒ»æ›´æ–°ï¼ˆå†…éƒ¨ãƒªãƒ³ã‚¯ã¯ä¸€åˆ‡è§¦ã‚‰ãªã„ï¼‰
                if wp_ok:
                    meta = _gen_meta_from_body(article.title or "", edited_html, user_id=article.user_id)
                    if meta:
                        try:
                            update_post_meta(site, wp_post_id, meta)
                        except Exception as e:
                            logging.info(f"[rewrite/meta] meta push skipped: {e}")

                # æˆåŠŸãªã‚‰ãƒ­ã‚°ã‚’ç¢ºå®š
                if wp_ok:
                    log.wp_status = "success"
                    log.snapshot_after = edited_html
                else:
                    log.wp_status = "error"
                    log.error_message = "WPæ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ"
                db.session.commit()
            except Exception as e:
                wp_err = str(e)
                log.wp_status = "error"
                log.error_message = wp_err
                db.session.commit()

        # 9) ãƒ—ãƒ©ãƒ³ã®çµ‚äº†å‡¦ç†
        plan.finished_at = datetime.utcnow()
        if dry_run:
            plan.status = "done"
            db.session.commit()
            return {
                "status": "done(dry)",
                "plan_id": plan.id,
                "article_id": article.id,
                "wp_post_id": wp_post_id,
            }
        else:
            plan.status = "done" if wp_ok else "error"
            if not wp_ok and not plan.last_error:
                plan.last_error = wp_err or "WPæ›´æ–°ã«å¤±æ•—"
            db.session.commit()
            return {
                "status": "success" if wp_ok else "error",
                "plan_id": plan.id,
                "article_id": article.id,
                "wp_post_id": wp_post_id,
                "error": wp_err,
            }


# ========== CLI/ãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼è£œåŠ©ï¼ˆä»»æ„ï¼‰ ==========

def run_once(user_id: int, plan_id: Optional[int] = None, dry_run: bool = True) -> None:
    """
    python -c ã‹ã‚‰å‘¼ã³ã‚„ã™ã„è–„ã„ãƒ©ãƒƒãƒ‘
    """
    res = execute_one_plan(user_id=user_id, plan_id=plan_id, dry_run=dry_run)
    print(json.dumps(res, ensure_ascii=False, indent=2))
