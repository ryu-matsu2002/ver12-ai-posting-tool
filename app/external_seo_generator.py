# app/external_seo_generator.py

import random
import logging
from typing import List, Tuple, Set, Iterable, Optional
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin, urlparse

import requests
from flask import current_app
from xml.etree import ElementTree as ET

from concurrent.futures import ThreadPoolExecutor, as_completed  # â˜… ä¸¦åˆ—åŒ–
import html as _html  # â˜… ã‚¿ã‚¤ãƒˆãƒ«ã®ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ç”¨
import re as _re      # â˜… ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºã«ã‚‚åˆ©ç”¨ï¼ˆæ—¢å­˜ã¨è¡çªã—ãªã„ã‚ˆã†åˆ¥åã«åˆã‚ã›ã‚‹ï¼‰

from app import db
from app.models import Site, Keyword, Article, ExternalArticleSchedule
from app.google_client import (
    fetch_top_queries_for_site,   # impressionsé™é †ã®queryä¸Šä½å–å¾—ï¼ˆ40ä»¶ï¼‰
    fetch_top_pages_for_site,     # impressionsé™é †ã®pageä¸Šä½å–å¾—ï¼ˆä»»æ„ä»¶ï¼‰
)
from .article_generator import _chat, _compose_body, TOKENS, TEMP, _generate  # â˜… _generate ã‚’æµç”¨

# è¿½åŠ : ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆç”¨
def _fallback_title_from_keyword(kw: str) -> str:
    """ã‚¿ã‚¤ãƒˆãƒ«ãŒç©ºã®ã¨ãã«å¿…ãšè¿”ã™ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    kw = (kw or "").strip()
    if not kw:
        return "è‡ªå‹•ç”Ÿæˆè¨˜äº‹"
    base = kw[:60]
    tails = ["ã®å®Œå…¨ã‚¬ã‚¤ãƒ‰", "ã®åŸºç¤çŸ¥è­˜", "ã®å§‹ã‚æ–¹", "ã®ãƒã‚¤ãƒ³ãƒˆ", "ã§å¤±æ•—ã—ãªã„ã‚³ãƒ„"]
    return f"{base}{tails[sum(map(ord, base)) % len(tails)]}"

def _safe_title(proposed: Optional[str], kw: str) -> str:
    """å€™è£œãŒç©º/ç©ºç™½ãªã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ç”Ÿæˆã—ã€120æ–‡å­—ã«åã‚ã‚‹"""
    t = (proposed or "").strip()
    if not t:
        t = _fallback_title_from_keyword(kw)
    return t[:120]

# ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è¨­å®š
JST = timezone(timedelta(hours=9))

# ===============================
# å›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚¿ã‚¤ãƒˆãƒ« / æœ¬æ–‡ï¼‰
# ===============================
TITLE_PROMPT = """ã‚ãªãŸã¯SEOã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚

å…¥åŠ›ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦
WEBã‚µã‚¤ãƒˆã®Qï¼†Aè¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ä½¿ç”¨ã™ã‚‹ã€ŒQï¼†Aè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚’ã€Œ1å€‹ã€è€ƒãˆã¦ãã ã•ã„ã€‚

Qï¼†Aè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã«ã¯å¿…ãšå…¥åŠ›ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¨ã¦ä½¿ã£ã¦ãã ã•ã„
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é †ç•ªã¯å…¥ã‚Œæ›¿ãˆãªã„ã§ãã ã•ã„
æœ€å¾Œã¯ã€Œï¼Ÿã€ã§ç· ã‚ã¦ãã ã•ã„


###å…·ä½“ä¾‹###

ã€Œç”±å¸ƒé™¢ è¦³å…‰ ãŠã™ã™ã‚ ã‚¹ãƒãƒƒãƒˆã€ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¯¾ã™ã‚‹å‡ºåŠ›æ–‡
â†“â†“â†“
ç”±å¸ƒé™¢è¦³å…‰ã§ã‚«ãƒƒãƒ—ãƒ«ã«äººæ°—ã®ãŠã™ã™ã‚ã‚¹ãƒãƒƒãƒˆã¯ï¼Ÿ

ã€Œç”±å¸ƒé™¢ è¦³å…‰ ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ¼ã‚¹ã€ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¯¾ã™ã‚‹å‡ºåŠ›æ–‡
â†“â†“â†“
ç”±å¸ƒé™¢è¦³å…‰ã®ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ¼ã‚¹ã§ãŠã™ã™ã‚ã®ä¸€æ—¥ãƒ—ãƒ©ãƒ³ã¯ï¼Ÿ
"""

BODY_PROMPT = """ã‚ãªãŸã¯SEOã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®å°‚é–€ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚

ã“ã‚Œã‹ã‚‰ã€**Q&Aå½¢å¼**ã®è¨˜äº‹æœ¬æ–‡ã‚’ä½œæˆã—ã¦ã‚‚ã‚‰ã„ã¾ã™ã€‚
å¿…ãšã€ä»¥ä¸‹ã®ã€åŸ·ç­†ãƒ«ãƒ¼ãƒ«ã€‘ã¨ã€å‡ºåŠ›æ¡ä»¶ã€‘ã«**å³å¯†ã«å¾“ã£ã¦**ãã ã•ã„ã€‚

---

### âœ…ã€åŸ·ç­†ãƒ«ãƒ¼ãƒ«ã€‘

#### 1. æ–‡ç« æ§‹æˆï¼ˆè¨˜äº‹å…¨ä½“ã®æµã‚Œï¼‰
- è¨˜äº‹ã¯ã€Œå•é¡Œæèµ·ã€â†’ã€Œèª­è€…ã¸ã®å…±æ„Ÿã€â†’ã€Œè§£æ±ºç­–ã®æç¤ºã€ã®é †ç•ªã§æ§‹æˆã—ã¦ãã ã•ã„ã€‚
- ã‚‚ã—ãã¯ã€Œçµè«–ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã€â†’ã€Œèª­è€…ã¸ã®å…±æ„Ÿã€â†’ã€Œä½“é¨“è«‡ã€ã‚„ã€Œãƒ¬ãƒ“ãƒ¥ãƒ¼é¢¨ã€â†’ã€Œæ¨©å¨æ€§ï¼ˆè³‡æ ¼ãƒ»å®Ÿç¸¾ï¼‰ã‚„å°‚é–€æ€§ã€ã®é †ç•ªã§æ§‹æˆã—ã¦ãã ã•ã„ã€‚
- è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã®ç´ ã«ãªã£ãŸæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾ç‡ã‚’7%å‰å¾Œã«ã™ã‚‹

#### 2. èª­è€…è¦–ç‚¹
- èª­è€…ã¯ã€Q&Aã‚¿ã‚¤ãƒˆãƒ«ã«æ‚©ã‚“ã§æ¤œç´¢ã—ã¦ããŸã€Œ1äººã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€ã§ã™ã€‚
- å¿…ãšã€**èª­è€…ãŒæœ¬å½“ã«çŸ¥ã‚ŠãŸã„ã“ã¨**ã‚’ã‚ã‹ã‚Šã‚„ã™ãä¼ãˆã¦ãã ã•ã„ã€‚
- å‘¼ã³ã‹ã‘ã¯ã€Œã‚ãªãŸã€ã«çµ±ä¸€ã—ã¦ãã ã•ã„ï¼ˆã€Œçš†ã•ã‚“ã€ãªã©ã®è¤‡æ•°å½¢ã¯NGã§ã™ï¼‰ã€‚
- èªã‚Šå£ã¯ã€Œè¦ªå‹ã«è©±ã™ã‚ˆã†ãªè¦ªã—ã¿ã‚’è¾¼ã‚ãŸæ•¬èªã€ã§æ›¸ã„ã¦ãã ã•ã„ã€‚
- æ¤œç´¢æ„å›³ã«100%å¿œãˆã‚‹ã‚ˆã†ã«æ›¸ã„ã¦ãã ã•ã„

#### 3. æ–‡ç« ã‚¹ã‚¿ã‚¤ãƒ«
- **æ®µè½å†…ã§æ”¹è¡Œã—ãªã„ã§ãã ã•ã„**ï¼ˆ1æ–‡ã®ã¾ã¨ã¾ã‚Šï¼1ã¤ã®æ–‡ç« ãƒ–ãƒ­ãƒƒã‚¯ã«ã—ã¦ãã ã•ã„ï¼‰ã€‚
- 1ã¤ã®æ–‡ç« ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆã„ã‚ã‚†ã‚‹ã€Œæ–‡ç« ã®å³¶ã€ï¼‰ã¯**1ï½3è¡Œä»¥å†…**ã«åã‚ã¦ãã ã•ã„ã€‚
- å„æ–‡ç« ãƒ–ãƒ­ãƒƒã‚¯ã®é–“ã¯ã€**2è¡Œåˆ†ç©ºã‘ã¦**ãã ã•ã„ï¼ˆãƒ€ãƒ–ãƒ«æ”¹è¡Œï¼‰ã€‚

#### 4. æ–‡å­—æ•°
- è¨˜äº‹ã®æœ¬æ–‡ã¯ã€å¿…ãš**2,500ï½3,500æ–‡å­—**ã®ç¯„å›²ã§æ›¸ã„ã¦ãã ã•ã„ã€‚

#### 5. å°è¦‹å‡ºã—
- hã‚¿ã‚°ï¼ˆh2, h3ï¼‰ã‚’ä½¿ã£ã¦ã€å†…å®¹ã‚’é©åˆ‡ã«æ•´ç†ã—ã¦ãã ã•ã„ã€‚

#### 6. ã»ã‹ã®ã‚µã‚¤ãƒˆã¸ã®ãƒªãƒ³ã‚¯ã«ã¤ã„ã¦
- ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã¯ã‚¸ãƒ£ãƒ³ãƒ«ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åˆã‚ã›ã¦ãƒªãƒ³ã‚¯ã—ã¦ãã ã•ã„
- ä½•åº¦ã§ã‚‚ã»ã‹ã®ã‚µã‚¤ãƒˆã¸ãƒªãƒ³ã‚¯ã—ã¦OKã§ã™
- å•†å“ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç´¹ä»‹ã™ã‚‹å ´åˆã¯ã€Œä¸è‡ªç„¶ãªæŠ¼ã—å£²ã‚Šã€ã«ãªã‚‰ãªã„ã‚ˆã†ã«ã€**ã”ãè‡ªç„¶ã«ç´¹ä»‹**ã—ã¦ãã ã•ã„ã€‚

---

### âœ…ã€å‡ºåŠ›æ¡ä»¶ã€‘

- è¨˜äº‹å†’é ­ã«ã¯ã€**Q&Aã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤ºã—ãªã„ã§ãã ã•ã„**ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãªã—ã§æœ¬æ–‡ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã—ã¦ãã ã•ã„ï¼‰ã€‚
- ä½™è¨ˆãªå‰ç½®ãã‚„æŒ¨æ‹¶ã‚‚å…¥ã‚Œãªã„ã§ãã ã•ã„ã€‚ä¾‹ï¼šã€Œæ‰¿çŸ¥ã—ã¾ã—ãŸã€ã€Œã“ã®è¨˜äº‹ã§ã¯ï½ã€ãªã©ä¸è¦ã§ã™ã€‚
- ãã®ã¾ã¾ã‚³ãƒ”ãƒšã—ãŸã„ã®ã§ã€å¿…ãšã€ã™ãã«æœ¬æ–‡ã‚’æ›¸ãå§‹ã‚ã¦ãã ã•ã„ã€‚

---

### ğŸ”¥ã€ç‰¹ã«é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã¾ã¨ã‚ã€‘

- ã€Œã‚ãªãŸã€å‘¼ã³ã§çµ±ä¸€
- ã€Œè¦ªã—ã¿ã‚’è¾¼ã‚ãŸæ•¬èªã€ã§
- Qï¼†Aè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã®ç´ ã«ãªã£ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾é »åº¦ã¯7%å‰å¾Œã§ã€‚
- æ®µè½ä¸­ã«æ”¹è¡Œç¦æ­¢ã€æ–‡ç« ã®å³¶ã¯1ï½3è¡Œ
- å„å³¶ã¯**2è¡Œç©ºã‘**
- ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤ºãªã—ã€æœ¬æ–‡ã‹ã‚‰å³ã‚¹ã‚¿ãƒ¼ãƒˆ
"""

# ===============================
# æŠ•ç¨¿ã‚¹ãƒ­ãƒƒãƒˆï¼ˆJST 10:00ã€œ21:59ï¼‰
# ===============================
RANDOM_MINUTE_CHOICES = [3, 7, 11, 13, 17, 19, 23, 27, 31, 37, 41, 43, 47, 53]
RANDOM_SECOND_CHOICES = [5, 12, 17, 23, 35, 42, 49]


def _random_minutes(n: int) -> List[int]:
    """é‡è¤‡ãªã n å€‹ã®åˆ†ã‚’é¸ã¶ã€‚å€™è£œãŒè¶³ã‚Šãªã„å ´åˆã¯ãƒ—ãƒ¼ãƒ«ã‚’æ‹¡å¼µ"""
    if n <= len(RANDOM_MINUTE_CHOICES):
        return random.sample(RANDOM_MINUTE_CHOICES, n)
    pool = RANDOM_MINUTE_CHOICES[:]
    while len(pool) < n:
        for m in RANDOM_MINUTE_CHOICES:
            if len(pool) >= n:
                break
            if m not in pool:
                pool.append(m)
    return random.sample(pool, n)


def _daily_slots_jst(per_day: int) -> List[Tuple[int, int]]:
    """
    1æ—¥ã®æŠ•ç¨¿ã‚¹ãƒ­ãƒƒãƒˆï¼ˆJSTï¼‰ã‚’è¿”ã™ã€‚
    10:00ã€œ21:59 ã®å„â€œæ™‚â€ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€åˆ†ã¯â€œåˆ‡ã‚Šã®è‰¯ããªã„åˆ†â€ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã€‚
    â€» åŒä¸€ã®â€œæ™‚â€ã¯1æœ¬ã®ã¿ â†’ æœ€ä½1æ™‚é–“ä»¥ä¸Šé–“éš”ã‚’æ‹…ä¿
    """
    base_hours = list(range(10, 22))  # 10..21 ã®12æ™‚é–“
    hours = sorted(random.sample(base_hours, per_day))  # ä¾‹: 10æœ¬/æ—¥ â†’ 10æ™‚é–“ã‚’æŠ½é¸
    minutes = _random_minutes(per_day)
    return list(zip(hours, minutes))


def _ensure_http_url(u: str) -> str:
    return u.strip()

#
# ====== è¿½åŠ ï¼šãƒªãƒ³ã‚¯å…ˆã‚¿ã‚¤ãƒˆãƒ«å–å¾—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ======
#
_ANCHOR_UA = "ai-posting-tool/1.0 (+title-fetch)"

def _extract_html_title(text: str) -> Optional[str]:
    """HTMLæ–‡å­—åˆ—ã‹ã‚‰ <meta property='og:title'> ã‚‚ã—ãã¯ <title> ã‚’æŠ½å‡º"""
    if not text:
        return None
    # og:title å„ªå…ˆ
    m = _re.search(r'<meta[^>]+property=["\']og:title["\'][^>]*content=["\']([^"\']+)["\']', text, flags=_re.I)
    if m and m.group(1).strip():
        return m.group(1).strip()
    # ä¸€èˆ¬çš„ãª <title>
    m = _re.search(r'<title[^>]*>(.*?)</title\s*>', text, flags=_re.I | _re.S)
    if m:
        # æ”¹è¡Œãƒ»ä½™ç™½ã®æ•´ç†
        t = _re.sub(r'\s+', ' ', (m.group(1) or '').strip())
        return t or None
    return None

def _fallback_anchor_from_url(u: str) -> str:
    """
    ã‚¿ã‚¤ãƒˆãƒ«ãŒå–ã‚Œãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼š
      1) ãƒ‘ã‚¹æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚°ã£ã½ã„éƒ¨åˆ†ã‚’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã‚¿ã‚¤ãƒˆãƒ«åŒ–
      2) ãã‚Œã§ã‚‚ãƒ€ãƒ¡ãªã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³
      3) æœ€å¾Œã«URLå…¨ä½“
    """
    try:
        pu = urlparse(u)
        # ã‚¹ãƒ©ãƒƒã‚°å€™è£œ
        path = (pu.path or "").rstrip("/")
        slug = path.split("/")[-1] if path else ""
        slug = _re.sub(r'[-_]+', ' ', slug).strip()
        slug = slug.title() if slug else ""
        if slug:
            return slug[:120]
        if pu.netloc:
            return pu.netloc
    except Exception:
        pass
    return u

def _clean_anchor_text(url: str, title: str) -> str:
    """
    å–å¾—ã—ãŸãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã€Œ - ã‚µã‚¤ãƒˆåã€ã€Œï½œã‚µã‚¤ãƒˆåã€ã€Œ| ã‚µã‚¤ãƒˆåã€ã€Œï¼šã‚µã‚¤ãƒˆåã€ãªã©ã®
    æœ«å°¾ãƒ–ãƒ©ãƒ³ãƒ‰è¡¨è¨˜ã‚’é™¤å»ã—ã¦ã€ã‚¢ãƒ³ã‚«ãƒ¼ã«ä½¿ã†â€œè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã ã‘â€ã‚’è¿”ã™ã€‚
    - åŒºåˆ‡ã‚Šã¯ã€Œç©ºç™½ä»˜ããƒã‚¤ãƒ•ãƒ³ã€ã€Œãƒ‘ã‚¤ãƒ—ã€ã€Œå…¨è§’ç¸¦æ£’ã€ã€Œã‚³ãƒ­ãƒ³ã€ã‚’å¯¾è±¡
    - å·¦å³ã®ç©ºç™½ã‚’é™¤å»ã—ã€æœ€ã‚‚â€œé•·ã„â€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ¡ç”¨ï¼ˆå¤šãã®ã‚µã‚¤ãƒˆã§è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ãŒæœ€é•·ï¼‰
    """
    t = (title or "").strip()
    if not t:
        return t
    # åŒºåˆ‡ã‚Šã§æœ«å°¾ã®ã‚µã‚¤ãƒˆåã‚’å¤–ã™ï¼ˆä¾‹: "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ« - ã‚µã‚¤ãƒˆå" / "ã‚µã‚¤ãƒˆå | è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«" ãªã©ï¼‰
    parts = _re.split(r'(?:\s-\s|\sâ€“\s|\sâ€”\s|\s\|\s|ï½œ|ï¼š|:|Â»)', t)
    parts = [p.strip() for p in parts if p and p.strip()]
    if len(parts) >= 2:
        # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ãŒæœ€ã‚‚é•·ããªã‚‹ã‚±ãƒ¼ã‚¹ãŒå¤šã„ã®ã§é•·ã•å„ªå…ˆã§é¸ã¶
        t = max(parts, key=lambda s: len(s))
    return t[:120]

def _fetch_page_title(u: str, timeout: int = 8) -> Optional[str]:
    """URLã¸HTTP GETã—ã¦ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ï¼ˆçŸ­æ™‚é–“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ/è»½é‡UAï¼‰"""
    try:
        # http/https ã®ã¿å¯¾è±¡ï¼ˆmailto:, javascript: ç­‰ã¯é™¤å¤–ï¼‰
        pu = urlparse(u)
        if pu.scheme not in ("http", "https"):
            return None
        r = requests.get(u, timeout=timeout, headers={"User-Agent": _ANCHOR_UA})
        if r.status_code != 200:
            return None
        # HTMLä»¥å¤–ã¯é™¤å¤–
        ctype = (r.headers.get("Content-Type") or "").lower()
        if "text/html" not in ctype:
            return None
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯requestsãŒæ¨å®šã™ã‚‹ã€å¤±æ•—æ™‚ã¯textãŒç©ºã«ãªã‚‹å¯èƒ½æ€§ã‚ã‚Š
        return _extract_html_title(r.text or "")
    except Exception:
        return None

def _prefetch_anchor_texts(urls: List[str], max_workers: int = 8) -> dict:
    """
    æ¸¡ã•ã‚ŒãŸURLã®ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä¸¦åˆ—ã§äº‹å‰å–å¾—ã—ã¦ dict ã§è¿”ã™ã€‚
    å–å¾—å¤±æ•—æ™‚ã¯ dict ã«å…¥ã‚Œãªã„ï¼ˆå‘¼ã³å‡ºã—å´ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ã€‚
    """
    anchors: dict[str, str] = {}
    # ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ã—ã¦è² è·ã‚’æŠ‘åˆ¶
    uniq = list({u for u in urls if isinstance(u, str)})
    def _job(u: str):
        t = _fetch_page_title(u)
        if t and t.strip():
            anchors[u] = _clean_anchor_text(u, t.strip())
    try:
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            futs = [ex.submit(_job, u) for u in uniq]
            for _ in as_completed(futs):
                pass
    except Exception:
        logging.exception("[external_seo] ã‚¿ã‚¤ãƒˆãƒ«äº‹å‰å–å¾—ä¸­ã«ä¾‹å¤–")
    return anchors


def _base_and_topic(site: Site) -> Tuple[str, str]:
    base = site.url.rstrip("/")
    # topic ã®æœ«å°¾ã«å¿…ãšã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ä»˜ã‘ã‚‹
    return base, f"{base}/topic/"


# ===============================
# URLåé›†ï¼ˆå›ºå®š5ãƒªãƒ³ã‚¯ & ãƒ©ãƒ³ãƒ€ãƒ å€™è£œï¼‰
# ===============================
def _build_fixed_links(site: Site) -> List[str]:
    """
    å›ºå®š5ãƒªãƒ³ã‚¯ï¼š
      - baseï¼ˆã‚µã‚¤ãƒˆTOPï¼‰
      - base/topic
      - GSC page impressions ä¸Šä½3ä»¶
    """
    base, sales = _base_and_topic(site)
    fixed = [base, sales]

    try:
        top_pages = fetch_top_pages_for_site(site, days=28, limit=3) or []
        # è¿”ã‚Šå€¤ãŒ dict ã®é…åˆ—ã§ã‚‚ str é…åˆ—ã§ã‚‚å¸å
        def _page_to_url(p):
            if isinstance(p, str):
                return p
            if isinstance(p, dict):
                return p.get("page") or p.get("url")
            return None

        pages = []
        for p in top_pages:
            url = _page_to_url(p)
            if url and isinstance(url, str):
                pages.append(url.strip())
        # base, sales ã¨è¢«ã£ãŸã‚‰é™¤å¤–ï¼ˆè£œå……ã¯ã—ãªã„ï¼‰
        for u in pages:
            if u not in fixed:
                fixed.append(u)
    except Exception as e:
        logging.warning(f"[external_seo] å›ºå®šãƒªãƒ³ã‚¯: GSCä¸Šä½pageå–å¾—å¤±æ•—: {e}")

    # å›ºå®šã¯æœ€å¤§5æœ¬ã«ä¸¸ã‚ã‚‹ï¼ˆä¸è¶³ã¯ãã®ã¾ã¾ï¼‰
    fixed = fixed[:5]
    if len(fixed) < 5:
        logging.warning(f"[external_seo] å›ºå®šãƒªãƒ³ã‚¯ãŒ {len(fixed)} ä»¶ã—ã‹ç”¨æ„ã§ãã¾ã›ã‚“ï¼ˆæƒ³å®š:5ä»¶ï¼‰: {fixed}")
    return fixed


def _fetch_xml(url: str, timeout: int = 10) -> Optional[ET.Element]:
    try:
        resp = requests.get(url, timeout=timeout, headers={"User-Agent": "ai-posting-tool/1.0"})
        if resp.status_code != 200 or not resp.content:
            return None
        return ET.fromstring(resp.content)  # XMLå®£è¨€ã‚„charsetå·®ç•°ã«ã‚‚å¼·ã‚ã«
    except Exception:
        return None


def _extract_loc_values(root: ET.Element) -> Iterable[str]:
    # sitemapindex -> sitemap -> loc
    for sm in root.findall(".//{*}sitemap"):
        loc = sm.find("{*}loc")
        if loc is not None and loc.text:
            yield loc.text.strip()
    # urlset -> url -> loc
    for u in root.findall(".//{*}url"):
        loc = u.find("{*}loc")
        if loc is not None and loc.text:
            yield loc.text.strip()


def _collect_all_site_urls(site: Site, max_nested: int = 50, max_total: int = 5000) -> Set[str]:
    """
    ãªã‚‹ã¹ãå¤šãã®å†…éƒ¨URLã‚’åé›†ã™ã‚‹ã€‚
    1) sitemap_index.xmlï¼ˆã‚ã‚Œã°ï¼‰â†’å„sitemap
    2) sitemap.xml
    3) wp-sitemap.xmlï¼ˆWPæ¨™æº–ï¼‰
    4) ãã‚Œã§ã‚‚ä¸è¶³ãªã‚‰ GSC pageä¸Šä½ï¼ˆæœ€å¤§1000ï¼‰
    5) æœ€å¾Œã®ä¿é™ºã§ WP REST ã‚’å©ã
    """
    base = site.url.rstrip("/")
    candidates = set()

    sitemap_entries = [
        f"{base}/sitemap_index.xml",
        f"{base}/sitemap.xml",
        f"{base}/wp-sitemap.xml",
    ]

    for sm_url in sitemap_entries:
        root = _fetch_xml(sm_url)
        if not root:
            continue

        locs = list(_extract_loc_values(root))
        if any(tag in root.tag for tag in ("sitemapindex", "index")) and locs:
            # nested sitemaps
            for child_url in locs[:max_nested]:
                cr = _fetch_xml(child_url)
                if not cr:
                    continue
                for u in _extract_loc_values(cr):
                    if u.startswith(base):
                        candidates.add(_ensure_http_url(u))
                        if len(candidates) >= max_total:
                            break
                if len(candidates) >= max_total:
                    break
        else:
            for u in locs:
                if u.startswith(base):
                    candidates.add(_ensure_http_url(u))
                    if len(candidates) >= max_total:
                        break

        if len(candidates) >= max_total:
            break

    # GSC page ã§è£œå®Œ
    if len(candidates) < 50:
        try:
            gsc_pages = fetch_top_pages_for_site(site, days=180, limit=1000) or []
            def _page_to_url(p):
                if isinstance(p, str):
                    return p
                if isinstance(p, dict):
                    return p.get("page") or p.get("url")
                return None
            for p in gsc_pages:
                url = _page_to_url(p)
                if url and isinstance(url, str) and url.startswith(base):
                    candidates.add(url.strip())
        except Exception as e:
            logging.warning(f"[external_seo] GSC pageè£œå®Œã«å¤±æ•—: {e}")

    # WordPress RESTï¼ˆæœ€å¾Œã®ä¿é™ºï¼‰
    if len(candidates) < 50:
        try:
            page = 1
            while len(candidates) < 200:
                api = f"{base}/wp-json/wp/v2/posts?per_page=100&page={page}"
                r = requests.get(api, timeout=8, headers={"User-Agent": "ai-posting-tool/1.0"})
                if r.status_code != 200:
                    break
                arr = r.json()
                if not isinstance(arr, list) or not arr:
                    break
                for it in arr:
                    link = it.get("link")
                    if isinstance(link, str) and link.startswith(base):
                        candidates.add(link.strip())
                page += 1
        except Exception:
            pass

    return candidates


def _pick_random_unique(urls: Iterable[str], n: int, excluded: Iterable[str] = ()) -> List[str]:
    pool = list({u for u in urls if isinstance(u, str)})
    ex = set(excluded or [])
    pool = [u for u in pool if u not in ex]
    if len(pool) < n:
        logging.error(f"[external_seo] ãƒ©ãƒ³ãƒ€ãƒ ç”¨URLãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆå¿…è¦:{n}, å–å¾—:{len(pool)}ï¼‰ã€‚è¦ä»¶ã‚’æº€ãŸã›ã¾ã›ã‚“ã€‚")
        return random.sample(pool, len(pool)) if pool else []
    return random.sample(pool, n)


# ===============================
# å¤–éƒ¨SEOï¼šä¸¦åˆ—ç”Ÿæˆ + ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
# ===============================
def generate_and_schedule_external_articles(
    user_id: int,
    site_id: int,
    blog_account_id: int,
    count: int = 100,
    per_day: int = 10,
    start_day_jst: Optional[datetime] = None,
) -> int:
    """
    å¤–éƒ¨SEOè¨˜äº‹ã‚’ä¸€æ‹¬ç”Ÿæˆã—ã€1æ—¥ per_day æœ¬ã€JST 10:00-21:59 ã®ãƒ©ãƒ³ãƒ€ãƒ åˆ†ã§ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç™»éŒ²ã€‚
    å®Ÿè£…ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰:
      1) ã¾ãš 100 æœ¬åˆ†ã® Article(status="pending") ã¨ ExternalArticleSchedule ã‚’ä¸€æ°—ã«ä½œæˆ
      2) ãã®å¾Œã€æœ¬æ–‡ç”Ÿæˆã‚’ ThreadPoolExecutor(max_workers=4) ã§ä¸¦åˆ—å®Ÿè¡Œã—ã€
         æœ¬æ–‡æœ«å°¾ã«ãã®è¨˜äº‹å°‚ç”¨ãƒªãƒ³ã‚¯ã‚’è¿½è¨˜ã—ã¦ Article.status="done" ã«ã™ã‚‹
      3) æŠ•ç¨¿ã¯æ—¢å­˜ã® _run_external_post_job ãŒæ‹¾ã£ã¦è¡Œã†
    """
    app = current_app._get_current_object()
    site = Site.query.get(site_id)
    assert site, "Site not found"

    # === ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸Šä½40ä»¶ï¼ˆimpressionsé™é †ï¼‰ ===
    try:
        top40 = fetch_top_queries_for_site(site, days=28, limit=40) or []
        def _to_query(d):
            if isinstance(d, str):
                return d
            if isinstance(d, dict):
                return d.get("query")
            return None

        kw40 = []
        for d in top40:
            q = _to_query(d)
            if q and isinstance(q, str):
                kw40.append(q.strip())
        kw40 = kw40[:40]
    except Exception as e:
        logging.exception(f"[external_seo] GSCã‚¯ã‚¨ãƒªå–å¾—ã«å¤±æ•—: {e}")
        kw40 = []

    # è£œå®Œï¼ˆä¸è¶³æ™‚ï¼‰ï¼šæ—¢å­˜Keywordã‹ã‚‰æ–°ã—ã„é †
    if len(kw40) < 40:
        need = 40 - len(kw40)
        extra = (
            Keyword.query
            .filter(Keyword.site_id == site_id)
            .order_by(Keyword.id.desc())
            .limit(need)
            .all()
        )
        kw40 += [k.keyword for k in extra if isinstance(k.keyword, str)]
        kw40 = kw40[:40]

    if len(kw40) < 40:
        raise RuntimeError(f"[external_seo] ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒ40ä»¶ã«æº€ãŸãªã„ãŸã‚ä¸­æ–­ã—ã¾ã—ãŸï¼ˆå–å¾—:{len(kw40)}ä»¶ï¼‰ã€‚")

    # === 100æœ¬ã®é…åˆ† ===
    dist: List[Tuple[str, int]] = [(kw, 3) for kw in kw40[:20]] + [(kw, 2) for kw in kw40[20:40]]
    gen_queue: List[str] = [kw for (kw, n) in dist for _ in range(n)]
    assert len(gen_queue) == 100, f"é…åˆ†ã‚¨ãƒ©ãƒ¼: {len(gen_queue)} != 100"

    # === ãƒªãƒ³ã‚¯è¨ˆç”» ===
    fixed5 = _build_fixed_links(site)  # [base, base/sales, top_page1, top_page2, top_page3]
    if len(fixed5) < 2:
        raise RuntimeError("[external_seo] å›ºå®šãƒªãƒ³ã‚¯ï¼ˆbase, base/salesï¼‰ãŒç¢ºä¿ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µã‚¤ãƒˆURLã‚’ã”ç¢ºèªãã ã•ã„ã€‚")

    fixed50 = []
    for u in fixed5:
        fixed50 += [u] * 10
    fixed50 = fixed50[:50]  # å¿µã®ãŸã‚ä¸¸ã‚

    # ãƒ©ãƒ³ãƒ€ãƒ 50ï¼ˆå›ºå®š5ã¯é™¤å¤–ã€åŒä¸€URLå†åˆ©ç”¨ä¸å¯ï¼‰
    all_urls = _collect_all_site_urls(site)
    random50 = _pick_random_unique(all_urls, 50, excluded=set(fixed5))
    if len(random50) < 50:
        raise RuntimeError(f"[external_seo] ãƒ©ãƒ³ãƒ€ãƒ ãƒªãƒ³ã‚¯ãŒ50ä»¶ã«æº€ãŸãªã„ãŸã‚ä¸­æ–­ã—ã¾ã—ãŸï¼ˆ{len(random50)}ä»¶ï¼‰ã€‚ã‚µã‚¤ãƒˆã®URLåé›†è¨­å®šã‚’ã”ç¢ºèªãã ã•ã„ã€‚")

    link_plan: List[str] = fixed50 + random50
    assert len(link_plan) == 100

    # â˜… æ–°è¦ï¼šãƒªãƒ³ã‚¯å…ˆã®ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’äº‹å‰å–å¾—ï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆç”¨ï¼‰
    anchor_map = _prefetch_anchor_texts(link_plan)

    # è¨˜äº‹ã¨ãƒªãƒ³ã‚¯ã®å¯¾å¿œã‚’ãƒ©ãƒ³ãƒ€ãƒ åŒ–ï¼ˆå‡ç­‰æ€§æ‹…ä¿ã®ãŸã‚è¨˜äº‹å´ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼‰
    random.shuffle(gen_queue)

    # === ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«é–‹å§‹æ—¥ï¼ˆç¿Œæ—¥ï¼‰ ===
    base_jst = datetime.now(JST).replace(hour=0, minute=0, second=0, microsecond=0)
    start_day_jst = (start_day_jst or base_jst) + timedelta(days=1)

    created_cnt = 0
    day_offset = 0
    idx = 0

    # 1) ã¾ãšæ ã‚’ä½œæˆï¼ˆArticle: pending / ExternalArticleSchedule: pendingï¼‰
    article_ids: List[int] = []
    per_article_link: List[Tuple[str, str]] = []  # (url, anchor_text)

    with app.app_context():
        try:
            while idx < len(gen_queue):
                # ãã®æ—¥ã®ã‚¹ãƒ­ãƒƒãƒˆï¼ˆ1æ™‚é–“1æœ¬ï¼åˆ†ã¯â€œåˆ‡ã‚Šã®è‰¯ããªã„åˆ†â€ï¼‰
                slots = _daily_slots_jst(per_day)
                slots.sort()

                for h, m in slots:
                    if idx >= len(gen_queue):
                        break

                    kw_str = gen_queue[idx]
                    link = link_plan[idx]

                    # Keyword å–å¾— or ç”Ÿæˆï¼ˆexternal ã‚½ãƒ¼ã‚¹ï¼‰
                    kobj = (
                        Keyword.query
                        .filter_by(user_id=user_id, site_id=site_id, keyword=kw_str)
                        .first()
                    )
                    if not kobj:
                        kobj = Keyword(
                            user_id=user_id,
                            site_id=site_id,
                            keyword=kw_str,
                            created_at=datetime.now(JST),
                            source="external",
                            status="pending",
                            used=False,
                        )
                        db.session.add(kobj)
                        db.session.flush()

                    # JST â†’ UTC naive ã«å¤‰æ›ï¼ˆç§’ã¯ã°ã‚‰ã™ï¼‰
                    when_jst = (start_day_jst + timedelta(days=day_offset)).replace(
                        hour=h,
                        minute=m,
                        second=random.choice(RANDOM_SECOND_CHOICES),
                        microsecond=0,
                    )
                    when_utc = when_jst.astimezone(timezone.utc)
                    when_naive = when_utc.replace(tzinfo=None)

                    # â˜… ä¿®æ­£: title ã«å¿…ãšéç©ºã®ä»®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥ã‚Œã‚‹
                    placeholder_title = _safe_title(None, kw_str)

                    art = Article(
                        keyword=kw_str,
                        title=placeholder_title,  # â† ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’å®Ÿéš›ã«è¨­å®š
                        body="",
                        user_id=user_id,
                        site_id=site_id,
                        status="pending",
                        progress=0,
                        source="external",
                        scheduled_at=when_naive,
                    )
                    db.session.add(art)
                    db.session.flush()

                    sched = ExternalArticleSchedule(
                        blog_account_id=blog_account_id,
                        article_id=art.id,
                        keyword_id=kobj.id,
                        scheduled_date=when_naive,
                        status="pending",
                    )
                    db.session.add(sched)

                    article_ids.append(art.id)
                    # â˜… URLã«å¯¾å¿œã™ã‚‹ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã‚’ç”¨æ„
                    anchor_txt = anchor_map.get(link) or _fallback_anchor_from_url(link)
                    # å¿µã®ãŸã‚ã“ã“ã§ã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã¯å½±éŸ¿ãªã—ï¼‰
                    anchor_txt = _clean_anchor_text(link, anchor_txt)
                    # HTMLè¡¨ç¤ºç”¨ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãŠãï¼ˆæŒ¿å…¥ç®‡æ‰€ã§äºŒé‡ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ãªã„ã‚ˆã†ã“ã“ã§ï¼‰
                    safe_anchor_txt = _html.escape(anchor_txt, quote=True)[:120]

                    per_article_link.append((link, safe_anchor_txt))

                    created_cnt += 1
                    idx += 1

                day_offset += 1

            db.session.commit()
            logging.info(f"[external_seo] æ ä½œæˆå®Œäº†: {created_cnt} ä»¶ï¼ˆsite_id={site_id}ï¼‰")

        except Exception as e:
            db.session.rollback()
            logging.exception(f"[external_seo] æ ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            raise


        # æœ¬æ–‡ã®ã€Œä¸­é–“ã€ã«ãƒªãƒ³ã‚¯ãƒ–ãƒ­ãƒƒã‚¯ã‚’å·®ã—è¾¼ã‚€
    def _insert_link_mid(html: str, link_url: str, anchor_text: str) -> str:
        """
        ã€Œé–¢é€£æƒ…å ±ã¯ã“ã¡ã‚‰ï¼š{ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ}ã€ã®å½¢ã§æŒ¿å…¥ã€‚
        åˆ¥ã‚¿ãƒ–é·ç§»ï¼ˆtarget=_blankï¼‰ã€å®‰å…¨ã®ãŸã‚ rel ã‚’ä»˜ä¸ã€‚
        """
        safe_url = _html.escape(link_url, quote=True)
        # anchor_text ã¯äº‹å‰ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—æ¸ˆã¿
        snippet = (
            f"<p>é–¢é€£æƒ…å ±ã¯ã“ã¡ã‚‰ï¼š"
            f"<a href='{safe_url}' target='_blank' rel='nofollow noopener noreferrer'>{anchor_text}</a>"
            f"</p>"
        )

        if not html:
            return snippet

        # 1) </p> ã®ç›´å¾Œã«å…¥ã‚Œã‚‹ï¼šæ®µè½ã‚’ä¿ã¦ã‚‹ã®ã§æœ€å„ªå…ˆ
        closings = [m.end() for m in _re.finditer(r'</p\s*>', html, flags=_re.I)]
        if closings:
            mid = max(0, len(closings) // 2 - 1)  # çœŸã‚“ä¸­ã®æ®µè½çµ‚ç«¯ã®ç›´å¾Œ
            pos = closings[mid]
            return html[:pos] + snippet + html[pos:]

        # 2) æ®µè½ãŒç„¡ã‘ã‚Œã°ã€ãƒ€ãƒ–ãƒ«æ”¹è¡Œã§ãƒ–ãƒ­ãƒƒã‚¯åˆ†å‰²ã—ã¦ä¸­é–“ã«å…¥ã‚Œã‚‹
        parts = _re.split(r'\n{2,}', html)
        if len(parts) >= 2:
            mid = len(parts) // 2
            return '\n\n'.join(parts[:mid]) + '\n\n' + snippet + '\n\n' + '\n\n'.join(parts[mid:])

        # 3) ãã‚Œã‚‚å³ã—ã‘ã‚Œã°æœ«å°¾ã«å¿µã®ãŸã‚è¿½åŠ ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        return html + '\n\n' + snippet

    # 2) ä¸¦åˆ—ã§æœ¬æ–‡ç”Ÿæˆ â†’ æœ¬æ–‡æœ«å°¾ã«ãƒªãƒ³ã‚¯è¿½è¨˜ â†’ done
    # external_seo_generator.py å†… _gen_and_append ã‚ˆã‚Š

    from app.article_generator import _unique_title

    def _gen_and_append(aid: int, link_url: str, anchor_text: str):
        _generate(app, aid, TITLE_PROMPT, BODY_PROMPT,
                  format="html", self_review=False, user_id=user_id)

        with app.app_context():
            art = Article.query.get(aid)
            if not art:
                return

            # ğŸ”§ ã‚¿ã‚¤ãƒˆãƒ«ãŒç©ºãªã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not art.title or not art.title.strip():
                art.title = _fallback_title_from_keyword(art.keyword or "")

            # ğŸ”§ é¡ä¼¼ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹å ´åˆã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
            art.title = _unique_title(art.keyword, TITLE_PROMPT)

            # æœ¬æ–‡ã«ã€Œã‚¿ã‚¤ãƒˆãƒ«ä»˜ããƒªãƒ³ã‚¯ã€ã‚’å·®ã—è¾¼ã¿ï¼ˆåˆ¥ã‚¿ãƒ–ï¼‰
            art.body = _insert_link_mid(art.body or "", link_url, anchor_text)

            if art.status not in ("done", "gen"):
                art.status = "done"
            art.progress = 100
            art.updated_at = datetime.utcnow()
            db.session.commit()

    try:
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for aid, pair in zip(article_ids, per_article_link):
                url, anchor = pair
                futures.append(executor.submit(_gen_and_append, aid, url, anchor))
            for f in as_completed(futures):
                try:
                    f.result()
                except Exception as e:
                    logging.exception(f"[external_seo] ä¸¦åˆ—ç”Ÿæˆã§ä¾‹å¤–: {e}")
    except Exception:
        # ä¸¦åˆ—å®Ÿè¡Œè‡ªä½“ãŒè½ã¡ã¦ã‚‚ã€ä½œæˆæ¸ˆã¿æ ã¯æ®‹ã‚‹ï¼ˆå†å®Ÿè¡Œã‚„å†ç”Ÿæˆã®ä½™åœ°ã‚’æ®‹ã™ï¼‰
        logging.exception("[external_seo] ä¸¦åˆ—ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºã§ã‚¨ãƒ©ãƒ¼")

    logging.info(f"[external_seo] ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†ï¼ˆsite_id={site_id}, total={created_cnt}ï¼‰")
    return created_cnt
