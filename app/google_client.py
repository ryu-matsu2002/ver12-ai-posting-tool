import os
import requests
import logging
from datetime import datetime, date, timedelta

from google.oauth2 import service_account
from googleapiclient.discovery import build
from flask import current_app
from app.models import Site, GSCMetric  # âœ… è¿½åŠ 
from app import db

# â”€â”€â”€â”€â”€â”€ Service Account èªè¨¼æƒ…å ±ã®èª­ã¿è¾¼ã¿ â”€â”€â”€â”€â”€â”€
BASE_DIR = os.path.dirname(os.path.dirname(__file__))
KEY_PATH = os.path.join(BASE_DIR, "credentials", "service_account.json")
SCOPES = ["https://www.googleapis.com/auth/webmasters.readonly"]

def get_search_console_service():
    credentials = service_account.Credentials.from_service_account_file(
        KEY_PATH, scopes=SCOPES
    )
    service = build("searchconsole", "v1", credentials=credentials)
    return service

# â”€â”€â”€â”€â”€â”€ âœ…âœ…âœ… è¿½åŠ : GSCMetricã¨ã—ã¦ä¿å­˜ã™ã‚‹å‡¦ç† â”€â”€â”€â”€â”€â”€
def store_metrics_from_gsc_rows(rows, site, metric_date: date):
    for row in rows:
        query = row["keys"][0]
        impressions = row.get("impressions", 0)
        clicks = row.get("clicks", 0)
        ctr = row.get("ctr", 0.0)
        position = row.get("position", 0.0)

        metric = GSCMetric(
            site_id=site.id,
            user_id=site.user_id,
            date=metric_date,
            query=query,
            impressions=impressions,
            clicks=clicks,
            ctr=ctr,
            position=position,
        )
        db.session.add(metric)
    db.session.commit()
    logging.info(f"[GSCMetric] âœ… ä¿å­˜å®Œäº†: {site.name} ({len(rows)} ä»¶)")

# â”€â”€â”€â”€â”€â”€ ğŸ” Search Console ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾— â”€â”€â”€â”€â”€â”€
def fetch_search_queries_for_site(site: Site, days: int = 28, row_limit: int = 1000) -> list[str]:
    try:
        # âœ… ä¿®æ­£: URLæœ«å°¾ã« / ã‚’è£œå®Œï¼ˆGSC APIã¯å®Œå…¨ä¸€è‡´ãŒå¿…é ˆï¼‰
        site_url = site.url
        if not site_url.endswith("/"):
            site_url += "/"

        # âœ… ã‚¯ã‚¨ãƒªå–å¾—ãƒ­ã‚°ï¼ˆäº‹å‰ï¼‰
        logging.info(f"[GSC] ã‚¯ã‚¨ãƒªå–å¾—é–‹å§‹: {site_url}")

        service = get_search_console_service()
        end_date = date.today()
        start_date = end_date - timedelta(days=days)

        request = {
            "startDate": start_date.isoformat(),
            "endDate": end_date.isoformat(),
            "dimensions": ["query"],
            "rowLimit": row_limit
        }

        response = service.searchanalytics().query(siteUrl=site_url, body=request).execute()
        rows = response.get("rows", [])

        # âœ… è¿½åŠ : ã‚¯ã‚¨ãƒªå–å¾—çµæœã®ãƒ­ã‚°
        logging.info(f"[GSC] {len(rows)} ä»¶ã®ã‚¯ã‚¨ãƒªã‚’å–å¾—: {site_url}")
        if not rows:
            logging.warning(f"[GSC] ã‚¯ã‚¨ãƒªãŒ0ä»¶ï¼ˆç©ºï¼‰ã§è¿”å´ã•ã‚Œã¾ã—ãŸ: {site_url}")

        # âœ…âœ…âœ… GSCMetricã«ä¿å­˜ï¼ˆä»Šå›ã®æ–°æ©Ÿèƒ½ï¼‰
        store_metrics_from_gsc_rows(rows, site, end_date)

        # âœ… æ—¢å­˜æ©Ÿèƒ½: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆè¨˜äº‹ç”Ÿæˆç”¨ï¼‰
        return [row["keys"][0] for row in rows]

    except Exception as e:
        logging.error(f"[GSCå–å¾—å¤±æ•—] site: {site.url} â†’ {e}")
        return []

# â”€â”€â”€â”€â”€â”€ ğŸ”„ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ï¼ˆã‚¯ãƒªãƒƒã‚¯æ•°ãƒ»è¡¨ç¤ºå›æ•°ï¼‰ â”€â”€â”€â”€â”€â”€
def update_gsc_metrics(site: Site):
    if not site.gsc_connected:
        return

    try:
        # âœ… ä¿®æ­£: URLæœ«å°¾ã« / ã‚’è£œå®Œï¼ˆGSC APIã¯å®Œå…¨ä¸€è‡´ãŒå¿…é ˆï¼‰
        site_url = site.url
        if not site_url.endswith("/"):
            site_url += "/"

        service = get_search_console_service()
        today = date.today()
        start_date = today - timedelta(days=30)

        request = {
            "startDate": start_date.isoformat(),
            "endDate": today.isoformat(),
            "dimensions": ["query"],
            "rowLimit": 25000
        }

        response = service.searchanalytics().query(siteUrl=site_url, body=request).execute()
        rows = response.get("rows", [])

        clicks = sum(row.get("clicks", 0) for row in rows)
        impressions = sum(row.get("impressions", 0) for row in rows)

        site.clicks = clicks
        site.impressions = impressions
        db.session.commit()

        logging.info(f"[GSC] âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°å®Œäº† - site: {site_url} | Clicks: {clicks}, Impressions: {impressions}")

    except Exception as e:
        logging.error(f"[GSC] ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—å¤±æ•— - {site.url} - {e}")

# â”€â”€â”€â”€â”€â”€ ğŸ” å…¨æ¥ç¶šã‚µã‚¤ãƒˆã‚’ä¸€æ‹¬æ›´æ–°ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ç”¨ï¼‰â”€â”€â”€â”€â”€â”€
def update_all_gsc_sites():
    sites = Site.query.filter_by(gsc_connected=True).all()
    for site in sites:
        update_gsc_metrics(site)


# è¿½åŠ ï¼šå†…éƒ¨ãƒ˜ãƒ«ãƒ‘ãƒ¼
def _site_url_norm(site: Site) -> str:
    return site.url if site.url.endswith("/") else site.url + "/"

def _run_search_analytics(site: Site, days: int, dimensions: list[str], row_limit: int,
                          order_by_impressions: bool = False):
    service = get_search_console_service()
    end_date = date.today()
    start_date = end_date - timedelta(days=days)

    body = {
        "startDate": start_date.isoformat(),
        "endDate": end_date.isoformat(),
        "dimensions": dimensions,
        "rowLimit": row_limit
    }
    if order_by_impressions:
        body["orderBy"] = [{"field": "impressions", "descending": True}]

    site_url = _site_url_norm(site)
    logging.info(f"[GSC] query dims={dimensions} limit={row_limit} {site_url}")
    resp = service.searchanalytics().query(siteUrl=site_url, body=body).execute()
    rows = resp.get("rows", [])
    logging.info(f"[GSC] rows={len(rows)} dims={dimensions} {site_url}")
    return rows

# è¿½åŠ ï¼šä¸Šä½ã‚¯ã‚¨ãƒª40ä»¶ï¼ˆè¡¨ç¤ºå›æ•°é™é †ï¼‰
def fetch_top_queries_for_site(site: Site, days: int = 28, limit: int = 40) -> list[dict]:
    try:
        rows = _run_search_analytics(site, days, ["query"], limit, order_by_impressions=True)
        out = []
        for r in rows:
            out.append({
                "query": r["keys"][0],
                "impressions": int(r.get("impressions", 0)),
                "clicks": int(r.get("clicks", 0)),
                "ctr": float(r.get("ctr", 0.0)),
                "position": float(r.get("position", 0.0)),
            })
        return out
    except Exception as e:
        logging.error(f"[GSC] fetch_top_queries_for_site failed: {e}")
        return []

# è¿½åŠ ï¼šä¸Šä½ãƒšãƒ¼ã‚¸3ä»¶ï¼ˆè¡¨ç¤ºå›æ•°é™é †ï¼‰
def fetch_top_pages_for_site(site: Site, days: int = 28, limit: int = 3) -> list[str]:
    try:
        rows = _run_search_analytics(site, days, ["page"], limit, order_by_impressions=True)
        return [r["keys"][0] for r in rows if r.get("keys")]
    except Exception as e:
        logging.error(f"[GSC] fetch_top_pages_for_site failed: {e}")
        return []

# è¿½åŠ ï¼šã‚µã‚¤ãƒˆå†…ã®å…¨ãƒšãƒ¼ã‚¸ï¼ˆSearch Consoleã«å‡ºã¦ãã‚‹ç¯„å›²ï¼‰
def fetch_all_pages_for_site(site: Site, days: int = 180, limit: int = 25000) -> list[str]:
    try:
        rows = _run_search_analytics(site, days, ["page"], limit, order_by_impressions=False)
        # é‡è¤‡ã‚„ç©ºã‚’é™¤å»
        pages = [r["keys"][0] for r in rows if r.get("keys") and r["keys"][0]]
        # çµ¶å¯¾URLã®ã¿ã€æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã¯ãã®ã¾ã¾ï¼ˆãã®ãƒšãƒ¼ã‚¸URLã‚’å°Šé‡ï¼‰
        return list(dict.fromkeys(pages))  # é †åºç¶­æŒã®é‡è¤‡æ’é™¤
    except Exception as e:
        logging.error(f"[GSC] fetch_all_pages_for_site failed: {e}")
        return []
