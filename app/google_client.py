import os
import requests
import logging
from datetime import datetime, date, timedelta

from google.oauth2 import service_account
from googleapiclient.discovery import build
from flask import current_app
from app.models import Site
from app import db

# â”€â”€â”€â”€â”€â”€ Service Account èªè¨¼æƒ…å ±ã®èª­ã¿è¾¼ã¿ â”€â”€â”€â”€â”€â”€
BASE_DIR = os.path.dirname(os.path.dirname(__file__))
KEY_PATH = os.path.join(BASE_DIR, "credentials", "service_account.json")
SCOPES = ["https://www.googleapis.com/auth/webmasters.readonly"]

def get_search_console_service():
    credentials = service_account.Credentials.from_service_account_file(
        KEY_PATH, scopes=SCOPES
    )
    service = build("searchconsole", "v1", credentials=credentials)
    return service

# â”€â”€â”€â”€â”€â”€ ğŸ” Search Console ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾— â”€â”€â”€â”€â”€â”€
def fetch_search_queries_for_site(site_url: str, days: int = 28, row_limit: int = 1000) -> list[str]:
    try:
        # âœ… ä¿®æ­£: URLæœ«å°¾ã« / ã‚’è£œå®Œï¼ˆGSC APIã¯å®Œå…¨ä¸€è‡´ãŒå¿…é ˆï¼‰
        if not site_url.endswith("/"):
            site_url += "/"

        # âœ… è¿½åŠ : ã‚¯ã‚¨ãƒªå–å¾—ãƒ­ã‚°ï¼ˆäº‹å‰ï¼‰
        logging.info(f"[GSC] ã‚¯ã‚¨ãƒªå–å¾—é–‹å§‹: {site_url}")

        service = get_search_console_service()
        end_date = date.today()
        start_date = end_date - timedelta(days=days)

        request = {
            "startDate": start_date.isoformat(),
            "endDate": end_date.isoformat(),
            "dimensions": ["query"],
            "rowLimit": row_limit
        }

        response = service.searchanalytics().query(siteUrl=site_url, body=request).execute()
        rows = response.get("rows", [])

        # âœ… è¿½åŠ : ã‚¯ã‚¨ãƒªå–å¾—çµæœã®ãƒ­ã‚°
        logging.info(f"[GSC] {len(rows)} ä»¶ã®ã‚¯ã‚¨ãƒªã‚’å–å¾—: {site_url}")
        if not rows:
            logging.warning(f"[GSC] ã‚¯ã‚¨ãƒªãŒ0ä»¶ï¼ˆç©ºï¼‰ã§è¿”å´ã•ã‚Œã¾ã—ãŸ: {site_url}")

        return [row["keys"][0] for row in rows]

    except Exception as e:
        logging.error(f"[GSCå–å¾—å¤±æ•—] site: {site_url} â†’ {e}")
        return []

# â”€â”€â”€â”€â”€â”€ ğŸ”„ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ï¼ˆã‚¯ãƒªãƒƒã‚¯æ•°ãƒ»è¡¨ç¤ºå›æ•°ï¼‰ â”€â”€â”€â”€â”€â”€
def update_gsc_metrics(site: Site):
    if not site.gsc_connected:
        return

    try:
        # âœ… ä¿®æ­£: URLæœ«å°¾ã« / ã‚’è£œå®Œï¼ˆGSC APIã¯å®Œå…¨ä¸€è‡´ãŒå¿…é ˆï¼‰
        site_url = site.url
        if not site_url.endswith("/"):
            site_url += "/"

        service = get_search_console_service()
        today = date.today()
        start_date = today - timedelta(days=30)

        request = {
            "startDate": start_date.isoformat(),
            "endDate": today.isoformat(),
            "dimensions": ["query"],
            "rowLimit": 25000
        }

        response = service.searchanalytics().query(siteUrl=site_url, body=request).execute()
        rows = response.get("rows", [])

        clicks = sum(row.get("clicks", 0) for row in rows)
        impressions = sum(row.get("impressions", 0) for row in rows)

        site.clicks = clicks
        site.impressions = impressions
        db.session.commit()

        logging.info(f"[GSC] âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°å®Œäº† - site: {site_url} | Clicks: {clicks}, Impressions: {impressions}")

    except Exception as e:
        logging.error(f"[GSC] ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—å¤±æ•— - {site.url} - {e}")

# â”€â”€â”€â”€â”€â”€ ğŸ” å…¨æ¥ç¶šã‚µã‚¤ãƒˆã‚’ä¸€æ‹¬æ›´æ–°ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ç”¨ï¼‰â”€â”€â”€â”€â”€â”€
def update_all_gsc_sites():
    sites = Site.query.filter_by(gsc_connected=True).all()
    for site in sites:
        update_gsc_metrics(site)
