import os
import requests
import logging
from datetime import datetime, date, timedelta, timezone

from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from flask import current_app
from app.models import Site, GSCMetric, GSCDailyTotal, GSCConfig  # âœ… è¿½åŠ 
from app import db
from typing import List, Dict, Tuple, Optional

# â”€â”€â”€â”€â”€â”€ Service Account èªè¨¼æƒ…å ±ã®èª­ã¿è¾¼ã¿ â”€â”€â”€â”€â”€â”€
BASE_DIR = os.path.dirname(os.path.dirname(__file__))
KEY_PATH = os.path.join(BASE_DIR, "credentials", "service_account.json")
SCOPES = ["https://www.googleapis.com/auth/webmasters.readonly"]

def get_search_console_service():
    credentials = service_account.Credentials.from_service_account_file(
        KEY_PATH, scopes=SCOPES
    )
    service = build("searchconsole", "v1", credentials=credentials)
    return service


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç’°å¢ƒå¤‰æ•°ãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼ˆå€¤ãŒç„¡ã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
def _get_env_int(key: str, default: int) -> int:
    try:
        return int(os.getenv(key, str(default)))
    except Exception:
        return default

def _jst_today() -> date:
    """JSTãƒ™ãƒ¼ã‚¹ã®ä»Šæ—¥ï¼ˆæ—¥ä»˜ï¼‰ã‚’è¿”ã™ã€‚"""
    JST = timezone(timedelta(hours=9))
    return datetime.utcnow().replace(tzinfo=timezone.utc).astimezone(JST).date()

def _jst_28d_window_end_start() -> Tuple[date, date]:
    """
    GSC UI ã¨åˆã‚ã›ã¦ã€æ˜¨æ—¥ã¾ã§ã®28æ—¥ã€ã‚’åŸºæœ¬çª“ã¨ã™ã‚‹ã€‚
    end = æ˜¨æ—¥(JST), start = end - 27 æ—¥
    """
    end_d = _jst_today() - timedelta(days=1)
    start_d = end_d - timedelta(days=27)
    return end_d, start_d

# â”€â”€â”€â”€â”€â”€ âœ…âœ…âœ… è¿½åŠ : GSCMetricã¨ã—ã¦ä¿å­˜ã™ã‚‹å‡¦ç† â”€â”€â”€â”€â”€â”€
def store_metrics_from_gsc_rows(rows, site, metric_date: date):
    for row in rows:
        query = row["keys"][0]
        impressions = row.get("impressions", 0)
        clicks = row.get("clicks", 0)
        ctr = row.get("ctr", 0.0)
        position = row.get("position", 0.0)

        metric = GSCMetric(
            site_id=site.id,
            user_id=site.user_id,
            date=metric_date,
            query=query,
            impressions=impressions,
            clicks=clicks,
            ctr=ctr,
            position=position,
        )
        db.session.add(metric)
    db.session.commit()
    logging.info(f"[GSCMetric] âœ… ä¿å­˜å®Œäº†: {site.name} ({len(rows)} ä»¶)")

# â”€â”€â”€â”€â”€â”€ ğŸ” Search Console ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾— â”€â”€â”€â”€â”€â”€
def fetch_search_queries_for_site(site: Site, days: int = 28, row_limit: int = 25000) -> list[str]:
    try:
        service = get_search_console_service()
        # JSTæº–æ‹ ã§â€œæ˜¨æ—¥ã¾ã§â€
        JST = timezone(timedelta(hours=9))
        today_jst = datetime.utcnow().replace(tzinfo=timezone.utc).astimezone(JST).date()
        end_date = today_jst - timedelta(days=1)  # æ˜¨æ—¥ã¾ã§
        start_date = end_date - timedelta(days=days)
        property_uri = _resolve_property_uri(site)

        logging.info(f"[GSC] ã‚¯ã‚¨ãƒªå–å¾—é–‹å§‹: {property_uri}")

        rows = []
        start_row = 0
        while True:
            body = {
                "startDate": start_date.isoformat(),
                "endDate": end_date.isoformat(),
                "dimensions": ["query"],
                "rowLimit": row_limit,
                "startRow": start_row,
                "searchType": "web",
                "dataState": "FINAL",
            }
            resp = service.searchanalytics().query(siteUrl=property_uri, body=body).execute()
            chunk = resp.get("rows", []) or []
            rows.extend(chunk)
            logging.info(f"[GSC] pagination: fetched={len(chunk)} total={len(rows)} startRow={start_row}")
            if len(chunk) < row_limit:
                break
            start_row += row_limit

        # âœ… è¿½åŠ : ã‚¯ã‚¨ãƒªå–å¾—çµæœã®ãƒ­ã‚°
        logging.info(f"[GSC] {len(rows)} ä»¶ã®ã‚¯ã‚¨ãƒªã‚’å–å¾—: {property_uri}")
        if not rows:
            logging.warning(f"[GSC] ã‚¯ã‚¨ãƒªãŒ0ä»¶ï¼ˆç©ºï¼‰ã§è¿”å´ã•ã‚Œã¾ã—ãŸ: {property_uri}")

        # âœ…âœ…âœ… GSCMetricã«ä¿å­˜ï¼ˆä»Šå›ã®æ–°æ©Ÿèƒ½ï¼‰
        # â€»æ³¨æ„ï¼šã“ã®ä¿å­˜ã¯â€œæœŸé–“åˆç®—â€ã‚’1æ—¥ä»˜ã«æŠ¼ã—è¾¼ã‚€è¨­è¨ˆã€‚
        # UIåˆè¨ˆã¨ã®ä¸€è‡´æ€§ã‚’é‡è¦–ã™ã‚‹ãªã‚‰ã€dimensions=['query','date']ã§æ—¥æ¬¡ä¿å­˜ã«æ”¹ä¿®æ¨å¥¨ã€‚
        store_metrics_from_gsc_rows(rows, site, end_date)

        # âœ… æ—¢å­˜æ©Ÿèƒ½: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆè¨˜äº‹ç”Ÿæˆç”¨ï¼‰
        return [row["keys"][0] for row in rows]

    except Exception as e:
        logging.error(f"[GSCå–å¾—å¤±æ•—] site: {site.url} â†’ {e}")
        return []

# â”€â”€â”€â”€â”€â”€ ğŸ”„ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ï¼ˆã‚¯ãƒªãƒƒã‚¯æ•°ãƒ»è¡¨ç¤ºå›æ•°ï¼‰ â”€â”€â”€â”€â”€â”€
def update_gsc_metrics(site: Site):
    """
    äº’æ›ç”¨ï¼šå†…éƒ¨çš„ã«ã¯ â€œæ—¥æ¬¡åˆè¨ˆã®ä¿å­˜â€ ã«ç½®ãæ›ãˆã€‚
    ï¼ˆæ—¢å­˜å‘¼ã³å‡ºã—å…ƒãŒã‚ã£ã¦ã‚‚å£Šã•ãªã„ãŸã‚ã«æ®‹ã—ã¦ãŠãï¼‰
    """
    try:
        update_site_daily_totals(site, days=35)
    except Exception as e:
        logging.error(f"[GSC] update_gsc_metrics (compat) failed - {site.url} - {e}")

# â”€â”€â”€â”€â”€â”€ ğŸ” å…¨æ¥ç¶šã‚µã‚¤ãƒˆã‚’ä¸€æ‹¬æ›´æ–°ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ç”¨ï¼‰â”€â”€â”€â”€â”€â”€
def update_all_gsc_sites():
    sites = Site.query.filter_by(gsc_connected=True).all()
    total_upsert = 0
    for site in sites:
        try:
            total_upsert += update_site_daily_totals(site, days=35)
        except Exception as e:
            logging.error(f"[GSC] site batch failed: {site.url} - {e}")
    logging.info(f"[GSC] batch done: upsert={total_upsert} rows")


# è¿½åŠ ï¼šå†…éƒ¨ãƒ˜ãƒ«ãƒ‘ãƒ¼
def _site_url_norm(site: Site) -> str:
    return site.url if site.url.endswith("/") else site.url + "/"

# è¿½åŠ ï¼šGSCã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£URIã‚’æ±ºå®šï¼ˆã‚ã‚Œã° GSCConfig å„ªå…ˆï¼‰
def _resolve_property_uri(site: Site) -> str:
    """
    æ—¢ã« GSCConfig ãŒã‚ã‚Œã°ãã‚Œã‚’è¿”ã™ã€‚ãªã‘ã‚Œã° URL ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿”ã™ã€‚
    ï¼ˆå®Ÿéš›ã®å‘¼ã³å‡ºã—å´ã§å€™è£œã‚’è©¦ã—ã€æˆåŠŸã—ãŸ URI ã‚’ GSCConfig ã«ä¿å­˜ã™ã‚‹ï¼‰
    """
    cfg = GSCConfig.query.filter_by(site_id=site.id).order_by(GSCConfig.id.desc()).first()
    if cfg and cfg.property_uri:
        return cfg.property_uri.strip()
    return _site_url_norm(site)

def _run_query_date_matrix(property_uri: str, start_d: date, end_d: date, row_limit: int = 25000) -> List[Dict]:
    """
    dimensions=['query','date'] ã®ãƒãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ã€‚
    28æ—¥åˆè¨ˆãªã©ã®ã‚¢ãƒ—ãƒªå´é›†è¨ˆã«ä½¿ã†ã€‚
    """
    service = get_search_console_service()
    body = {
        "startDate": start_d.isoformat(),
        "endDate": end_d.isoformat(),
        "dimensions": ["query", "date"],
        "rowLimit": row_limit,
        "searchType": "web",
        "dataState": "FINAL",
    }
    logging.info(f"[GSC] query-date matrix: {property_uri} {start_d}..{end_d}")
    resp = service.searchanalytics().query(siteUrl=property_uri, body=body).execute()
    rows = resp.get("rows", []) or []
    logging.info(f"[GSC] query-date rows={len(rows)} {property_uri}")
    return rows

def _aggregate_query_stats(rows: List[Dict]) -> Dict[str, Dict]:
    """
    rowsï¼ˆkeys=[query, 'YYYY-MM-DD']ï¼‰ã‚’ã‚¯ã‚¨ãƒªå˜ä½ã«é›†è¨ˆã€‚
    - impressions_28d: åˆè¨ˆè¡¨ç¤ºå›æ•°
    - first_seen_date: æœ€åˆã«è¦³æ¸¬ã•ã‚ŒãŸæ—¥ä»˜ï¼ˆæœ€å°æ—¥ä»˜ï¼‰
    """
    from datetime import date as _date
    stats: Dict[str, Dict] = {}
    for r in rows:
        if not r.get("keys"): 
            continue
        q, ds = r["keys"][0], r["keys"][1]
        try:
            d = _date.fromisoformat(ds)
        except Exception:
            continue
        imp = int(r.get("impressions", 0) or 0)
        s = stats.get(q)
        if not s:
            stats[q] = {"impressions_28d": imp, "first_seen_date": d}
        else:
            s["impressions_28d"] += imp
            if d < s["first_seen_date"]:
                s["first_seen_date"] = d
    return stats

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… æ–°ç€ã‚¯ã‚¨ãƒªæŠ½å‡ºï¼ˆcutoffä»¥é™ã«åˆè¦³æ¸¬ ï¼† 28æ—¥åˆè¨ˆimprãŒENVä»¥ä¸Šï¼‰
def fetch_new_queries_since(
    site: Site,
    cutoff_date: Optional[date] = None,
    min_impressions: Optional[int] = None,
    row_limit: int = 25000,
) -> List[Dict]:
    """
    è¿”ã‚Šå€¤: [{"query": str, "impressions_28d": int, "first_seen_date": date}, ...]
    - first_seen_date >= cutoff_date
    - impressions_28d >= min_impressions
    çª“ã¯ JSTåŸºæº–ã§ã€æ˜¨æ—¥ã¾ã§ã®28æ—¥ã€ã«å›ºå®šï¼ˆUIæ•´åˆæ€§ã®ãŸã‚ï¼‰ã€‚
    """
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if cutoff_date is None:
        cutoff_date = getattr(site, "gsc_autogen_since", None) or _jst_today()
    if min_impressions is None:
        min_impressions = _get_env_int("GSC_MIN_IMPRESSIONS", 20)

    # é›†è¨ˆçª“ï¼ˆ28æ—¥å›ºå®šï¼‰
    end_d, start_d = _jst_28d_window_end_start()

    # siteUrl ã¯æ¤œè¨¼æ¸ˆã¿ã® property ã‚’å„ªå…ˆ
    property_uri = _resolve_property_uri(site)

    try:
        rows = _run_query_date_matrix(property_uri, start_d, end_d, row_limit=row_limit)
        stats = _aggregate_query_stats(rows)

        # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
        picked: List[Dict] = []
        for q, s in stats.items():
            if s["first_seen_date"] >= cutoff_date and s["impressions_28d"] >= min_impressions:
                picked.append({
                    "query": q,
                    "impressions_28d": int(s["impressions_28d"]),
                    "first_seen_date": s["first_seen_date"],
                })

        logging.info(
            "[GSC-AUTOGEN] pick_new_queries site_id=%s prop=%s start=%s end=%s cutoff=%s picked=%s (rows=%s, min_impr=%s)",
            site.id, property_uri, start_d, end_d, cutoff_date, len(picked), len(rows), min_impressions
        )
        return picked
    except HttpError as e:
        status = getattr(e, "status_code", None) or getattr(e.resp, "status", None)
        logging.warning(f"[GSC-AUTOGEN] query-date failed ({status}) site={site.id} prop={property_uri}")
        return []
    except Exception as e:
        logging.exception(f"[GSC-AUTOGEN] unexpected error site={site.id} prop={property_uri}: {e}")
        return []

def fetch_daily_totals_for_property(property_uri: str, start_d: date, end_d: date):
    """
    GSCã‹ã‚‰ dimensions=['date'] ã§ â€œæ—¥åˆ¥åˆè¨ˆâ€ ã‚’ãã®ã¾ã¾å–å¾—ã™ã‚‹ã€‚
    è¿”ã‚Šå€¤ã¯ Search Console API ã® rowsï¼ˆkeys[0]ãŒ'YYYY-MM-DD'ï¼‰ã€‚
    """
    service = get_search_console_service()
    body = {
        "startDate": start_d.isoformat(),
        "endDate": end_d.isoformat(),
        "dimensions": ["date"],
        "rowLimit": 25000,
        "searchType": "web",
        "dataState": "FINAL",
    }
    logging.info(f"[GSC] daily totals: {property_uri} {start_d}..{end_d}")
    try:
        resp = service.searchanalytics().query(siteUrl=property_uri, body=body).execute()
        rows = resp.get("rows", [])
    except HttpError as e:
        # å‘¼ã³å‡ºã—å…ƒã§ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã—ãŸã„ã®ã§ãã®ã¾ã¾æŠ•ã’ç›´ã™
        raise
    logging.info(f"[GSC] daily totals rows={len(rows)} {property_uri}")
    return rows

def upsert_gsc_daily_totals(site: Site, property_uri: str, rows: list[dict]) -> int:
    """
    rowsï¼ˆdateæ¬¡å…ƒã®åˆè¨ˆï¼‰ã‚’ GSCDailyTotal ã« upsertã€‚æˆ»ã‚Šå€¤ã¯ upsert ä»¶æ•°ã€‚
    """
    if not rows:
        return 0

    # ã™ã§ã«ã‚ã‚‹æ—¥ä»˜ã‚’ä¸€æ‹¬ã§å–å¾—ã—ã¦ãŠãï¼ˆå°ã•ãªæœŸé–“ãªã®ã§ã“ã‚Œã§ååˆ†é«˜é€Ÿï¼‰
    dates = [date.fromisoformat(r["keys"][0]) for r in rows if r.get("keys")]
    existing = {
        (x.date): x
        for x in GSCDailyTotal.query
            .filter(GSCDailyTotal.site_id == site.id,
                    GSCDailyTotal.property_uri == property_uri,
                    GSCDailyTotal.date.in_(dates))
            .all()
    }

    upsert_cnt = 0
    for r in rows:
        if not r.get("keys"):
            continue
        d = date.fromisoformat(r["keys"][0])
        clicks = int(r.get("clicks", 0) or 0)
        impressions = int(r.get("impressions", 0) or 0)

        row = existing.get(d)
        if row:
            # å€¤ãŒé•ã†ã¨ãã®ã¿æ›´æ–°
            if row.clicks != clicks or row.impressions != impressions:
                row.clicks = clicks
                row.impressions = impressions
                upsert_cnt += 1
        else:
            db.session.add(GSCDailyTotal(
                site_id=site.id,
                user_id=site.user_id,
                property_uri=property_uri,
                date=d,
                clicks=clicks,
                impressions=impressions
            ))
            upsert_cnt += 1

    if upsert_cnt:
        db.session.commit()
    return upsert_cnt

def update_site_daily_totals(site: Site, days: int = 35) -> int:
    """
    ã‚µã‚¤ãƒˆã®ç›´è¿‘Næ—¥ï¼ˆãƒ‡ãƒ•ã‚©35æ—¥ï¼‰ã® â€œæ—¥æ¬¡åˆè¨ˆâ€ ã‚’å–å¾—ã—ã¦ DB ã«ä¿å­˜ã€‚
    GSC UIã¨ä¸€è‡´ã•ã›ã‚‹ãŸã‚ã€JSTã®ã€Œä»Šæ—¥ã€åŸºæº–ã§ç¯„å›²ã‚’çµ„ã‚€ã€‚
    æˆ»ã‚Šå€¤ã¯ upsertä»¶æ•°ã€‚
    """
    if not site.gsc_connected:
        return 0

    JST = timezone(timedelta(hours=9))
    today_jst = datetime.utcnow().replace(tzinfo=timezone.utc).astimezone(JST).date()
    # ä¾‹ï¼š28æ—¥è¡¨ç¤ºã«å®Œå…¨ä¸€è‡´ã•ã›ã‚‹ã«ã¯ã€start = today_jst - timedelta(days=27)
    span = max(1, int(days))
    # âœ… GSC UI ã«åˆã‚ã›ã¦ã€Œæ˜¨æ—¥ã¾ã§ã€
    end_d = today_jst - timedelta(days=1)
    start_d = end_d - timedelta(days=span - 1)

    # ã¾ãšæ—¢çŸ¥ã® URIï¼ˆè¨­å®šæ¸ˆã¿ãŒã‚ã‚Œã°ãã‚Œï¼‰ã‹ã‚‰è©¦è¡Œ
    first = _resolve_property_uri(site)
    candidates = [first]
    # ãƒãƒªã‚¢ãƒ³ãƒˆã‚’è¿½åŠ ï¼ˆhttp/https, www æœ‰ç„¡, sc-domainï¼‰
    try:
        from urllib.parse import urlparse
        p = urlparse(_site_url_norm(site))
        host = p.hostname or ""
        bare = host.replace("www.", "")
        candidates += [
            f"https://{host}/",
            f"http://{host}/",
            f"https://www.{bare}/",
            f"http://www.{bare}/",
            f"sc-domain:{bare}",
        ]
    except Exception:
        pass
    # é‡è¤‡é™¤å»
    seen = set()
    candidates = [x for x in candidates if not (x in seen or seen.add(x))]

    last_err = None
    for prop in candidates:
        try:
            rows = fetch_daily_totals_for_property(prop, start_d, end_d)
            upserted = upsert_gsc_daily_totals(site, prop, rows)
            logging.info(f"[GSC] upserted={upserted} site={site.name} prop={prop}")
            # æˆåŠŸã—ãŸ prop ã‚’å­¦ç¿’ä¿å­˜ï¼ˆæ¬¡å›ä»¥é™ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã«ä½¿ã†ï¼‰
            cfg = GSCConfig(site_id=site.id, user_id=site.user_id, property_uri=prop)
            db.session.add(cfg)
            db.session.commit()
            return upserted
        except HttpError as e:
            # 403 ã‚„ 404 ãªã© â†’ æ¬¡ã®å€™è£œã¸
            status = getattr(e, "status_code", None) or getattr(e.resp, "status", None)
            logging.warning(f"[GSC] try prop failed ({status}): {prop}")
            last_err = e
            continue
        except Exception as e:
            logging.exception(f"[GSC] unexpected error for prop={prop}: {e}")
            last_err = e
            continue
    # ã™ã¹ã¦å¤±æ•—ï¼š403ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ã€ãã®ä»–ã¯raise
    if last_err:
        if isinstance(last_err, HttpError):
            status = getattr(last_err, "status_code", None) or getattr(last_err.resp, "status", None)
            if status == 403:
                logging.warning(f"[GSC] skip site (403 forbidden): site_id=%s url=%s", site.id, site.url)
                return 0
        raise last_err


def _run_search_analytics(site: Site, days: int, dimensions: list[str], row_limit: int,
                          order_by_impressions: bool = False):
    service = get_search_console_service()
    # JSTæº–æ‹ ã§â€œæ˜¨æ—¥ã¾ã§â€ã®çª“ã«çµ±ä¸€
    JST = timezone(timedelta(hours=9))
    today_jst = datetime.utcnow().replace(tzinfo=timezone.utc).astimezone(JST).date()
    end_date = today_jst - timedelta(days=1)  # æ˜¨æ—¥ã¾ã§
    start_date = end_date - timedelta(days=days)

    body = {
        "startDate": start_date.isoformat(),
        "endDate": end_date.isoformat(),
        "dimensions": dimensions,
        "rowLimit": row_limit,
        "searchType": "web",
        "dataState": "FINAL",
    }
    if order_by_impressions:
        body["orderBy"] = [{"field": "impressions", "descending": True}]

    property_uri = _resolve_property_uri(site)
    logging.info(f"[GSC] query dims={dimensions} limit={row_limit} prop={property_uri} body={body}")
    resp = service.searchanalytics().query(siteUrl=property_uri, body=body).execute()
    rows = resp.get("rows", [])
    logging.info(f"[GSC] rows={len(rows)} dims={dimensions} prop={property_uri}")
    return rows

# è¿½åŠ ï¼šä¸Šä½ã‚¯ã‚¨ãƒª40ä»¶ï¼ˆè¡¨ç¤ºå›æ•°é™é †ï¼‰
def fetch_top_queries_for_site(site: Site, days: int = 28, limit: int = 40) -> list[dict]:
    try:
        rows = _run_search_analytics(site, days, ["query"], limit, order_by_impressions=True)
        out = []
        for r in rows:
            out.append({
                "query": r["keys"][0],
                "impressions": int(r.get("impressions", 0)),
                "clicks": int(r.get("clicks", 0)),
                "ctr": float(r.get("ctr", 0.0)),
                "position": float(r.get("position", 0.0)),
            })
        return out
    except Exception as e:
        logging.error(f"[GSC] fetch_top_queries_for_site failed: {e}")
        return []

# è¿½åŠ ï¼šä¸Šä½ãƒšãƒ¼ã‚¸3ä»¶ï¼ˆè¡¨ç¤ºå›æ•°é™é †ï¼‰
def fetch_top_pages_for_site(site: Site, days: int = 28, limit: int = 3) -> list[str]:
    try:
        rows = _run_search_analytics(site, days, ["page"], limit, order_by_impressions=True)
        return [r["keys"][0] for r in rows if r.get("keys")]
    except Exception as e:
        logging.error(f"[GSC] fetch_top_pages_for_site failed: {e}")
        return []

# è¿½åŠ ï¼šã‚µã‚¤ãƒˆå†…ã®å…¨ãƒšãƒ¼ã‚¸ï¼ˆSearch Consoleã«å‡ºã¦ãã‚‹ç¯„å›²ï¼‰
def fetch_all_pages_for_site(site: Site, days: int = 180, limit: int = 25000) -> list[str]:
    try:
        rows = _run_search_analytics(site, days, ["page"], limit, order_by_impressions=False)
        # é‡è¤‡ã‚„ç©ºã‚’é™¤å»
        pages = [r["keys"][0] for r in rows if r.get("keys") and r["keys"][0]]
        # çµ¶å¯¾URLã®ã¿ã€æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã¯ãã®ã¾ã¾ï¼ˆãã®ãƒšãƒ¼ã‚¸URLã‚’å°Šé‡ï¼‰
        return list(dict.fromkeys(pages))  # é †åºç¶­æŒã®é‡è¤‡æ’é™¤
    except Exception as e:
        logging.error(f"[GSC] fetch_all_pages_for_site failed: {e}")
        return []

# app/google_client.py ã«è¿½åŠ ï¼ˆå¿…è¦ãªã¨ãã ã‘å‘¼ã¶ï¼‰
def fetch_totals_direct(property_uri: str, start_d: date, end_d: date) -> dict:
    """
    GSC UIã®28æ—¥åˆè¨ˆã«å¯„ã›ã‚‹ãŸã‚ã€dimensionsãªã—ã§ç›´æ¥åˆè¨ˆã‚’å–å¾—ã€‚
    dataState='FINAL' ã‚’æŒ‡å®šã€‚
    """
    service = get_search_console_service()
    body = {
        "startDate": start_d.isoformat(),
        "endDate": end_d.isoformat(),
        # ç„¡æ¬¡å…ƒã¯é¿ã‘ãŸã„ãŒäº’æ›ã®ãŸã‚æ®‹ã™ã€‚APIãŒtotalsã‚’è¿”ã•ãªã„å ´åˆã‚ã‚Š
        "dimensions": [],
        "dataState": "FINAL",
        "searchType": "web",
        "rowLimit": 1,
    }
    resp = service.searchanalytics().query(siteUrl=property_uri, body=body).execute()
    # rowsãƒ™ãƒ¼ã‚¹ or top-level totals ãƒ™ãƒ¼ã‚¹ã®åŒæ–¹ã‚’æ¢ã‚‹
    if "rows" in resp and resp["rows"]:
        r0 = resp["rows"][0]
        return {
            "clicks": int(r0.get("clicks", 0) or 0),
            "impressions": int(r0.get("impressions", 0) or 0),
        }
    return {
        "clicks": int(resp.get("clicks", 0) or 0),
        "impressions": int(resp.get("impressions", 0) or 0),
    }
