from __future__ import annotations
import os, random, time, logging, requests
import re
from typing import List
from flask import current_app
from werkzeug.utils import secure_filename

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Ë®≠ÂÆö ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ROOT_URL            = os.getenv("APP_ROOT_URL", "https://your-domain.com")
PIXABAY_API_KEY     = os.getenv("PIXABAY_API_KEY", "")
PIXABAY_TIMEOUT     = 5
MAX_PER_PAGE        = 30
RECENTLY_USED_TTL   = int(os.getenv("IMAGE_CACHE_TTL", "86400"))  # 24h
DEFAULT_IMAGE_PATH  = os.getenv("DEFAULT_IMAGE_URL", "/static/default-thumb.jpg")
DEFAULT_IMAGE_URL   = (
    DEFAULT_IMAGE_PATH.startswith("http")
    and DEFAULT_IMAGE_PATH
    or f"{ROOT_URL}{DEFAULT_IMAGE_PATH}"
)

# ‚úÖ ‰øùÂ≠ò„Éá„Ç£„É¨„ÇØ„Éà„É™„Å®URL„Éë„Çπ„ÅÆÂÆöÁæ©
IMAGE_SAVE_DIR = os.path.join("app", "static", "images")
IMAGE_URL_PREFIX = "/static/images"

# „Éì„Ç∏„Éç„ÇπÁ≥ª„Çø„Ç∞
_BUSINESS_TAGS = {
    "money","business","office","finance","analysis",
    "marketing","startup","strategy","computer",
    "statistics","success"
}

_used_image_urls: dict[str, float] = {}

def _is_recently_used(url: str, ttl: int = RECENTLY_USED_TTL) -> bool:
    ts = _used_image_urls.get(url)
    return ts is not None and (time.time() - ts) < ttl

def _mark_used(url: str) -> None:
    _used_image_urls[url] = time.time()

# Â§ñÈÉ®URL„Åß„ÇÇÊúâÂäπ„Åã„Çí HEAD „É™„ÇØ„Ç®„Çπ„Éà„ÅßÊ§úÊüªÔºàtext/html „ÇíÈô§Â§ñÔºâ
def _is_image_url(url: str) -> bool:
    if not url or url.strip() in ["", "None"]:
        return False

    if url.startswith("/static/images/"):
        filename = os.path.basename(url)
        if not filename.lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):
            return False
        local_path = os.path.join("app", "static", "images", filename)
        return os.path.exists(local_path) and os.path.getsize(local_path) > 0

    if url.startswith("http"):
        try:
            r = requests.head(url, timeout=3, allow_redirects=True)
            content_type = r.headers.get("Content-Type", "").lower()
            return "image" in content_type
        except Exception as e:
            logging.warning(f"[ÁîªÂÉèURLÁ¢∫Ë™çÂ§±Êïó] {url}: {e}")
            return False

    return False




def _search_pixabay(query: str, per_page: int = MAX_PER_PAGE) -> List[dict]:
    if not PIXABAY_API_KEY or not query:
        return []
    query = query.replace("„ÄÄ"," ").strip()
    params = {
        "key": PIXABAY_API_KEY,
        "q": query,
        "image_type": "photo",
        "per_page": per_page,
        "safesearch": "true",
    }
    try:
        r = requests.get("https://pixabay.com/api/", params=params, timeout=PIXABAY_TIMEOUT)
        if r.status_code == 400:
            logging.warning("Pixabay 400 for %s ‚Äì fallthrough", query)
            return []
        r.raise_for_status()
        return r.json().get("hits", [])
    except Exception as e:
        logging.debug("Pixabay error (%s): %s", query, e)
        return []

def _score(hit: dict, kw_set: set[str]) -> int:
    tags  = {t.strip().lower() for t in hit.get("tags","").split(",")}
    base  = sum(1 for kw in kw_set if kw in tags)
    bonus = 2 if tags & _BUSINESS_TAGS else 0
    return base + bonus

def _valid_dim(hit: dict) -> bool:
    w, h = hit.get("imageWidth",0), hit.get("imageHeight",1)
    if w<=0 or h<=0:
        return False
    ratio = w/h
    return 0.5 <= ratio <= 3.0

def _pick_pixabay(hits: List[dict], keywords: List[str]) -> str:
    if not hits:
        return DEFAULT_IMAGE_URL
    kw_set = {k.lower() for k in keywords}
    top = sorted(hits, key=lambda h: _score(h, kw_set), reverse=True)[:10]
    random.shuffle(top)

    for h in top:
        url = h.get("largeImageURL") or h.get("webformatURL")
        if url and not _is_recently_used(url) and _valid_dim(h) and _is_image_url(url):
            _mark_used(url)
            return url

    for h in top:
        url = h.get("largeImageURL") or h.get("webformatURL")
        if url and not _is_recently_used(url) and _is_image_url(url):
            _mark_used(url)
            return url

    url = top[0].get("largeImageURL") or top[0].get("webformatURL") or DEFAULT_IMAGE_URL
    _mark_used(url)
    return url

def _unsplash_src(query: str) -> str:
    words = query.split()[:6]
    short = " ".join(words)[:120]
    q = requests.utils.quote(short)
    return f"https://source.unsplash.com/featured/1200x630/?{q}"

# ‚úÖ Êñ∞Ê©üËÉΩÔºöÁîªÂÉè„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Å¶„É≠„Éº„Ç´„É´‰øùÂ≠òÔºà„Éï„Ç°„Ç§„É´Âêç = Ë®ò‰∫ã„Çø„Ç§„Éà„É´Ôºâ
def _download_and_save_image(image_url: str, title: str) -> str:
    try:
        filename = secure_filename(title.strip()) + ".jpg"
        local_path = os.path.join(IMAGE_SAVE_DIR, filename)
        if not os.path.exists(IMAGE_SAVE_DIR):
            os.makedirs(IMAGE_SAVE_DIR, exist_ok=True)

        # Êó¢„Å´‰øùÂ≠òÊ∏à„Å™„ÇâÂÜçDL„Åó„Å™„ÅÑ
        if not os.path.exists(local_path):
            r = requests.get(image_url, timeout=10)
            r.raise_for_status()
            with open(local_path, "wb") as f:
                f.write(r.content)
        return f"{IMAGE_URL_PREFIX}/{filename}"
    except Exception as e:
        logging.error(f"[ÁîªÂÉè‰øùÂ≠òÂ§±Êïó] {image_url}: {e}")
        return DEFAULT_IMAGE_URL

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üîß Public API
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def fetch_featured_image(query: str, title: str = "") -> str:
    def is_safe_image(url: str) -> bool:
        return url.lower().endswith(('.jpg', '.jpeg', '.png'))

    def clean_and_translate(query: str) -> str:
        stopwords = {"„ÅÆ", "„Å´", "„Åß", "„Çí", "„Å®", "„Åå", "„ÅØ", "„Åã„Çâ", "„Åü„ÇÅ", "„Åì„Å®", "„Å´„Å§„ÅÑ„Å¶", "ÊñπÊ≥ï", "„Åä„Åô„Åô„ÇÅ", "ÂÆåÂÖ®„Ç¨„Ç§„Éâ"}
        jp_to_en = {
            "„Éî„É©„ÉÜ„Ç£„Çπ": "pilates", "ËÖ∞Áóõ": "back pain", "ÊîπÂñÑ": "improvement", "Á¶èÂ≤°": "fukuoka",
            "Ëã±Ë™û": "english", "ÂâØÊ•≠": "side job", "„Éñ„É≠„Ç∞": "blog", "„Éì„Ç∏„Éç„Çπ": "business",
            "ÊóÖË°å": "travel", "ËÑ±ÊØõ": "hair removal", "„É°„Ç§„ÇØ": "makeup", "Âç†„ÅÑ": "fortune telling"
        }
        keywords = [w for w in re.split(r"[\\s„ÄÄ]+", query) if w and w not in stopwords]
        translated = [jp_to_en.get(w, w) for w in keywords]
        return " ".join(translated)

    try:
        base_query = clean_and_translate(query)
        keywords = base_query.split()

        hits = _search_pixabay(base_query)
        url  = _pick_pixabay(hits, keywords)
        if url and is_safe_image(url):
            return _download_and_save_image(url, title or query)

        enhanced_query = f"{base_query} woman person lifestyle"
        hits = _search_pixabay(enhanced_query)
        url = _pick_pixabay(hits, keywords + ["woman", "person", "lifestyle"])
        if url and is_safe_image(url):
            return _download_and_save_image(url, title or query)

        fallback_url = _unsplash_src(base_query)
        return fallback_url

    except Exception as e:
        logging.error("fetch_featured_image fatal: %s", e)
        return DEFAULT_IMAGE_URL

# „Çø„Ç§„Éà„É´„ÉªÊú¨Êñá„Åã„Çâ„Ç¢„Ç§„Ç≠„É£„ÉÉ„ÉÅÁîªÂÉè„ÇíÊé®ÂÆö„Åó„Å¶ÂèñÂæóÔºàÂæ©ÂÖÉÊ©üËÉΩÁî®Ôºâ
def fetch_featured_image_from_body(body: str) -> str:
    import re
    from .image_utils import fetch_featured_image

    match = re.search(r"<h2\b[^>]*>(.*?)</h2>", body or "", re.IGNORECASE)
    first_h2 = match.group(1) if match else ""
    query = first_h2 or "Ë®ò‰∫ã „Ç¢„Ç§„Ç≠„É£„ÉÉ„ÉÅ"
    return fetch_featured_image(query)
