# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# app/image_utils.py   â€“ v8-fixed2 (2025-05-XX)  *always-thumb*
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
Pixabay â†’ Unsplash â†’ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç”»åƒã®é †ã§ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒã‚’å–å¾—ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

ã“ã®æ”¹è¨‚ç‰ˆã§ã¯ fetch_featured_image ãŒçµ¶å¯¾ã«æ–‡å­—åˆ—ã‚’è¿”ã—ã€
None ã‚’è¿”ã•ãªã„ã“ã¨ã§ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼WPæŠ•ç¨¿æ™‚ã« â€œthumbâ€ ã®ã¾ã¾
è¡¨ç¤ºã•ã‚Œãªããªã‚‹å•é¡Œã‚’è§£æ¶ˆã—ã¾ã™ã€‚
"""

from __future__ import annotations
import os, random, time, logging, requests
import re
from typing import List
from flask import current_app

# â”€â”€â”€â”€â”€ è¨­å®š â”€â”€â”€â”€â”€
ROOT_URL            = os.getenv("APP_ROOT_URL", "https://your-domain.com")
PIXABAY_API_KEY     = os.getenv("PIXABAY_API_KEY", "")
PIXABAY_TIMEOUT     = 5
MAX_PER_PAGE        = 30
RECENTLY_USED_TTL   = int(os.getenv("IMAGE_CACHE_TTL", "86400"))  # 24h
DEFAULT_IMAGE_PATH  = os.getenv("DEFAULT_IMAGE_URL", "/static/default-thumb.jpg")
# ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºï¼WP æŠ•ç¨¿æ™‚ã«ä½¿ã†çµ¶å¯¾ URL
DEFAULT_IMAGE_URL   = (
    DEFAULT_IMAGE_PATH.startswith("http")
    and DEFAULT_IMAGE_PATH
    or f"{ROOT_URL}{DEFAULT_IMAGE_PATH}"
)

# ãƒ“ã‚¸ãƒã‚¹ç³»ã‚¿ã‚°ï¼ˆé–¢é€£æ€§å‘ä¸Šï¼‰
_BUSINESS_TAGS = {
    "money","business","office","finance","analysis",
    "marketing","startup","strategy","computer",
    "statistics","success"
}

# ç”»åƒåˆ©ç”¨å±¥æ­´ (urlâ†’timestamp)
_used_image_urls: dict[str, float] = {}

def _is_recently_used(url: str, ttl: int = RECENTLY_USED_TTL) -> bool:
    ts = _used_image_urls.get(url)
    return ts is not None and (time.time() - ts) < ttl

def _mark_used(url: str) -> None:
    _used_image_urls[url] = time.time()

# âœ…ã€è¿½åŠ ã€‘URLãŒç”»åƒå½¢å¼ã‹ã©ã†ã‹ã‚’HEADã§ç¢ºèª
def _is_image_url(url: str) -> bool:
    try:
        r = requests.head(url, timeout=5)
        content_type = r.headers.get("Content-Type", "")
        return content_type.startswith("image/")
    except Exception as e:
        logging.warning(f"[ç”»åƒåˆ¤å®šå¤±æ•—] {url} â†’ {e}")
        return False    

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Pixabay æ¤œç´¢
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _search_pixabay(query: str, per_page: int = MAX_PER_PAGE) -> List[dict]:
    if not PIXABAY_API_KEY or not query:
        return []
    query = query.replace("ã€€"," ").strip()
    params = {
        "key": PIXABAY_API_KEY,
        "q": query,
        "image_type": "photo",
        "per_page": per_page,
        "safesearch": "true",
    }
    try:
        r = requests.get("https://pixabay.com/api/", params=params, timeout=PIXABAY_TIMEOUT)
        if r.status_code == 400:
            logging.warning("Pixabay 400 for %s â€“ fallthrough", query)
            return []
        r.raise_for_status()
        return r.json().get("hits", [])
    except Exception as e:
        logging.debug("Pixabay error (%s): %s", query, e)
        return []

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° & é¸æŠ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _score(hit: dict, kw_set: set[str]) -> int:
    tags  = {t.strip().lower() for t in hit.get("tags","").split(",")}
    base  = sum(1 for kw in kw_set if kw in tags)
    bonus = 2 if tags & _BUSINESS_TAGS else 0
    return base + bonus

def _valid_dim(hit: dict) -> bool:
    w, h = hit.get("imageWidth",0), hit.get("imageHeight",1)
    if w<=0 or h<=0:
        return False
    ratio = w/h
    return 0.5 <= ratio <= 3.0

# âœ…ã€ä¿®æ­£ã€‘ç”»åƒå½¢å¼ã‹ã©ã†ã‹ã®ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ 
def _pick_pixabay(hits: List[dict], keywords: List[str]) -> str:
    if not hits:
        return DEFAULT_IMAGE_URL
    kw_set = {k.lower() for k in keywords}
    top = sorted(hits, key=lambda h: _score(h, kw_set), reverse=True)[:10]
    random.shuffle(top)

    # ğŸ” æœ€å„ªå…ˆã§ç”»åƒå½¢å¼ãƒ»æœªä½¿ç”¨ãƒ»ã‚µã‚¤ã‚ºé©æ­£ãªURLã‚’è¿”ã™
    for h in top:
        url = h.get("largeImageURL") or h.get("webformatURL")
        if url and not _is_recently_used(url) and _valid_dim(h) and _is_image_url(url):
            _mark_used(url)
            return url

    # ğŸ” ã‚µã‚¤ã‚ºä¸å•ã§ã‚‚ç”»åƒå½¢å¼ã§ã‚ã‚Œã°OK
    for h in top:
        url = h.get("largeImageURL") or h.get("webformatURL")
        if url and not _is_recently_used(url) and _is_image_url(url):
            _mark_used(url)
            return url

    # ğŸ”š æœ€å¾Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç¢ºèªãªã—ï¼‰
    url = top[0].get("largeImageURL") or top[0].get("webformatURL") or DEFAULT_IMAGE_URL
    _mark_used(url)
    return url

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Unsplash Source
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _unsplash_src(query: str) -> str:
    words = query.split()[:6]
    short = " ".join(words)[:120]
    q = requests.utils.quote(short)
    return f"https://source.unsplash.com/featured/1200x630/?{q}"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Public API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def fetch_featured_image(query: str) -> str:
    """
    é«˜ç²¾åº¦ãªã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒã‚’å–å¾—ã™ã‚‹ã€‚
    ã‚¯ã‚¨ãƒªã‹ã‚‰ä¸è¦èªé™¤å»ãƒ»è‹±èªåŒ–ãƒ»è£œå¼·èªè¿½åŠ ãƒ»Pixabayâ†’Unsplashâ†’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é †ã§å¯¾å¿œã€‚
    """
    def is_safe_image(url: str) -> bool:
        return url.lower().endswith(('.jpg', '.jpeg', '.png'))

    def clean_and_translate(query: str) -> str:
        stopwords = {"ã®", "ã«", "ã§", "ã‚’", "ã¨", "ãŒ", "ã¯", "ã‹ã‚‰", "ãŸã‚", "ã“ã¨", "ã«ã¤ã„ã¦", "æ–¹æ³•", "ãŠã™ã™ã‚", "å®Œå…¨ã‚¬ã‚¤ãƒ‰"}
        jp_to_en = {
            "ãƒ”ãƒ©ãƒ†ã‚£ã‚¹": "pilates", "è…°ç—›": "back pain", "æ”¹å–„": "improvement", "ç¦å²¡": "fukuoka",
            "è‹±èª": "english", "å‰¯æ¥­": "side job", "ãƒ–ãƒ­ã‚°": "blog", "ãƒ“ã‚¸ãƒã‚¹": "business",
            "æ—…è¡Œ": "travel", "è„±æ¯›": "hair removal", "ãƒ¡ã‚¤ã‚¯": "makeup", "å ã„": "fortune telling"
        }
        keywords = [w for w in re.split(r"[\\sã€€]+", query) if w and w not in stopwords]
        translated = [jp_to_en.get(w, w) for w in keywords]
        return " ".join(translated)

    try:
        base_query = clean_and_translate(query)
        keywords = base_query.split()

        # 1. ç´ ã®ã‚¯ã‚¨ãƒª
        hits = _search_pixabay(base_query)
        url  = _pick_pixabay(hits, keywords)
        if url and is_safe_image(url):
            return url

        # 2. è£œå¼·æ¤œç´¢ï¼ˆè£œåŠ©èªè¿½åŠ ï¼‰
        enhanced_query = f"{base_query} woman person lifestyle"
        hits = _search_pixabay(enhanced_query)
        url = _pick_pixabay(hits, keywords + ["woman", "person", "lifestyle"])
        if url and is_safe_image(url):
            return url

        # 3. Unsplash fallback
        unsplash_url = _unsplash_src(base_query)
        if is_safe_image(unsplash_url):
            return unsplash_url

        return DEFAULT_IMAGE_URL

    except Exception as e:
        logging.error("fetch_featured_image fatal: %s", e)
        return DEFAULT_IMAGE_URL


def fetch_featured_image_from_body(body_html: str, keyword: str) -> str:
    from bs4 import BeautifulSoup

    def extract_top_topics(html: str) -> List[str]:
        soup = BeautifulSoup(html, "html.parser")
        # h2ã¨h3ã‚’ã™ã¹ã¦å–å¾—ã—ã€è¦‹å‡ºã—èªå¥ã®ä¸Šä½3ä»¶ã‚’é¸å®š
        headings = soup.find_all(["h2", "h3"])
        phrases = [h.get_text().strip() for h in headings]
        return phrases[:3] if phrases else [keyword]

    try:
        topics = extract_top_topics(body_html)
        queries = [f"{keyword} {t}" for t in topics]

        for query in queries:
            hits = _search_pixabay(query)
            url = _pick_pixabay(hits, query.split())
            if url and url.lower().endswith((".jpg", ".jpeg", ".png")):
                return url

        # æœ€å¾Œã« Unsplash fallback
        return _unsplash_src(f"{keyword} {topics[0]}")
    except Exception as e:
        logging.error(f"ç”»åƒé¸å®šå¤±æ•—: {e}")
        return DEFAULT_IMAGE_URL
